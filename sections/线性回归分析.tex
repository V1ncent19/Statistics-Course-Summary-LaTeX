\section{线性回归分析部分}\label{SecLinearRegressionAnalysis}
\begin{center}
    Instructor: Zaiying Zhou
\end{center}
\begin{point}
    Steps in Regression Analysis
\end{point}

\begin{enumerate}[topsep=2pt,itemsep=2pt]
    \item Statement of the problem;
    \item Selection of potentially relevant \textbf{variables};
    \item Data collection;
    \item Exploratory Data Analysis (\textbf{EDA} )\index{EDA (Exploratory Data Analysis)}
    \item \textbf{Model} specification;
    \item Choice of fitting method;
    \item Model fitting;
    \item Model validation and criticism;
    \item Using the chosen model(s) for the solution of the posed problem;
    \item \textbf{Explain} the result.
\end{enumerate}

    \lstinline|R.| Code for EDA
\begin{lstlisting}[language=R]
libaray('GGally')
head(df)
ggpairs(df)
str(df)
summary(df)
\end{lstlisting}



\begin{point}
    Used Packages in \lstinline|R.|
\begin{lstlisting}[language=R]
library('ggplot2')
libaray('GGally')
library('car')
library('moments')
library('lmtest')
library('nortest')
library('MASS')
library('tseries')

source('package.r')
\end{lstlisting}

\end{point}


\subsection{Regression Model}
% \begin{itemize}[topsep=6pt,itemsep=4pt]
%     \item Assume a Model
%     \begin{enumerate}[topsep=6pt,itemsep=4pt]
%         \item Parameter of the model
%         \item Basic Assumptions
%         \item Dsitribution of error
%     \end{enumerate}
%     \item Parametric Estimation
%     \begin{enumerate}[topsep=6pt,itemsep=4pt]
%         \item Ordinary Least Squares Estimation
%         \item Maximun Likelihood Estimation
%     \end{enumerate}
%     \item Statistics Inference
%     \begin{enumerate}[topsep=6pt,itemsep=4pt]
%         \item Hypotheses Testing
%         \item Interval Estimation
%     \end{enumerate}

% \end{itemize}

    In regression model, we will observe pairs of variables, called 'cases'(样本点). A sample is $ (X_1;Y_1),\ldots,(X_n;Y_n) $, where $ X_i $ can be multivariate $ X_i=\vec{X}_i=(X_{i1},X_{i2},\ldots,X_{ip}) $.

    If $ X $ is continuous \lstinline|numeric| variable, use Regression Model(s), else if $ X $ is discrete \lstinline|factor| variable, use Factor Model(s). 

\begin{rcode}
    Example data import:
\begin{lstlisting}[language=R]
df <- read.table('dataset/testdata.txt', header=FALSE, sep=',', col.names = c('y','x1','x2'))
\end{lstlisting}
\end{rcode}


\subsubsection{Linear Regression Model}
    Regression Model focuses on how $ Y $ changes with continuous variables $ X\in\mathbb{R} $. As a basic situation, we use \textbf{Linear Regression}, i.e. $ Y\sim X $ in linear relation.  


\begin{point}
    Sample Geometry Notation (Full Version)
\end{point}

    For most general case, in sample matrix notation:
    \begin{equation}
        Y=X\beta+\varepsilon \,\leftrightharpoons\, Y_j=X\beta _j+\varepsilon _j,\,\forall j=1,2,\ldots,q
    \end{equation}
    
    in Einstein Summation Convention:
    \begin{equation}
        Y_{ij}=X_{ij'}\beta _{j'j} +\varepsilon _{ij}
    \end{equation}

    Why we need $ \varepsilon $ as 'random error term'?
    \begin{itemize}[topsep=3pt,itemsep=1pt]
        \item It represents the intrinsic random property of the model.
        \item Based on $ \varepsilon  $, we can take r.v. into our statistic model.
    \end{itemize}
    
    where 
\begin{subequations}\label{EqaSampleNotationOfMultiLinear}
    \begin{align}
        \mathop{Y}\limits_{n\times q} =&\begin{bmatrix}
        y_{11}&y_{12}&\ldots&y_{1q}\\
        y_{21}&y_{22}&\ldots&y_{2q}\\
        \vdots&\vdots&\ddots&\vdots\\
        y_{n1}&y_{n2}&\ldots&y_{nq}\\
        \end{bmatrix}
        =\begin{bmatrix}
            y_1,y_2,\ldots,y_q
        \end{bmatrix}
        &
        y _j=&\begin{bmatrix}
                y _{1j}\\
                y _{2j}\\
                \vdots\\
                y _{nj}
            \end{bmatrix}\\
        \mathop{X}\limits_{n\times (p+1)}=&\begin{bmatrix}
        1&x_{11}&x_{12}&\ldots&x_{1p}\\
        1&x_{21}&x_{22}&\ldots&x_{2p}\\
        \vdots&\vdots&\ddots&\vdots\\
        1&x_{n1}&x_{n2}&\ldots&x_{np}\\
        \end{bmatrix}
        =
        \begin{bmatrix}
        x_1'\\x_2'\\\vdots\\x_n'   
        \end{bmatrix} 
        &
        x_i=&\begin{bmatrix}
            1\\
            x_{i1}\\
            \vdots\\
            x_{ip}
        \end{bmatrix}\\
        \mathop{\beta }\limits_{(p+1)\times q}=&\begin{bmatrix}
        \beta _{01}&\beta _{02}&\ldots&\beta _{0q}\\
        \beta _{11}&\beta _{12}&\ldots&\beta _{1q}\\
        \beta _{21}&\beta _{22}&\ldots&\beta _{2q}\\
        \vdots&\vdots&\ddots&\vdots\\
        \beta _{p1}&\beta _{p2}&\ldots&\beta _{pq}\\
        \end{bmatrix} =\begin{bmatrix}
            \beta _1,\beta _2,\ldots,\beta _q
        \end{bmatrix}
        & \beta _j=&\begin{bmatrix}
            \beta _{j0}\\
            \beta_{j1}\\
            \vdots\\
            \beta_{jp}
        \end{bmatrix}\\
        \mathop{\varepsilon }\limits_{n\times q} =&
        \begin{bmatrix}
        \varepsilon _{11}&\varepsilon _{12}&\ldots&\varepsilon _{1q}\\
        \varepsilon _{21}&\varepsilon _{22}&\ldots&\varepsilon _{2q}\\
        \vdots&\vdots&\ddots&\vdots\\
        \varepsilon _{n1}&\varepsilon _{n2}&\ldots&\varepsilon _{nq}\\
        \end{bmatrix}=
        \begin{bmatrix}
            \varepsilon _1,\varepsilon _2,\ldots,\varepsilon _q
        \end{bmatrix}
        &
        \varepsilon _j=&\begin{bmatrix}
                \varepsilon _{1j}\\
                \varepsilon _{2j}\\
                \vdots\\
                \varepsilon _{nj}
            \end{bmatrix}
    \end{align}
\end{subequations}

with Guass-Markov Assumption:\index{Gauss-Markov Assumption}\hypertarget{GaussMarkovAssumption}{}
\begin{equation}\label{EqaGaussMarkovAssumption}
    \begin{aligned}
        \text{Zero-Mean: }&\mathbb{E}(\epsilon_i|X_i)=0 \\
        \text{Homogeneity of Variance: }&var(\epsilon_i)=\sigma^2\\
        \text{Independent: }&\epsilon_i\text{ i.i.d. }\sim \varepsilon
    \end{aligned}
\end{equation}

and Normality Error Assumption: 
% Further in most cases, we consider $ \varepsilon \sim N(0,\sigma^2) $ ----because of its well-property distribution, 
% \footnote{i.e. $ Y_i $ are independent
\begin{equation}\label{EqaLRANormalityAssumption}
    \text{Normality: }\varepsilon _i \text{ i.i.d. }\sim N(0,\sigma ^2) 
\end{equation}

%     Expressed the above-mentioned 4 assumptions in condensed notation:
% \begin{equation}
%     Y_i\sim N(\beta _0+\beta _1X_i,\sigma^2)\quad i=1,2,\ldots ,n 
% \end{equation}
    Under matrix notation, model and assumptions \autoref{EqaGaussMarkovAssumption}(\autoref{EqaLRANormalityAssumption}) can be expressed in condensed notation:

    \begin{equation}
        Y_j=X\beta_j +\varepsilon_j  \sim N_n(X\beta_j ,\sigma_j^2I_n),\quad j=1,2,\ldots,q
    \end{equation}

\fbox{
    \begin{minipage}{0.9\linewidth}
        $ \Delta $ \textbf{Note: }In this section we only focus on $ q=1 $, i.e.
        \begin{equation}\label{EqaMultivariteLinearModel}
            \mathop{Y}\limits_{n\times 1} =\mathop{X}\limits_{n\times(p+1)} \mathop{\beta }\limits_{(p+1)\times 1}  +\mathop{\varepsilon }\limits_{n\times 1}   
        \end{equation}
    \end{minipage}
}\\


\subsubsection{Factor Analysis Model}
    Regression Model focuses on continuous variables $ X\in\mathbb{R} $ while factor model focus on discrete variable. More specifically, the `value' of $ X $ is just a label, not necessarily a `numeric value'.

    Here only introduce one-way factor analysis,(single factor analysis) i.e. $ Y $ with only one factor with $ r $ levels: $ \mathrm{fac} =1,2,\ldots,r $. Re-denote $ Y_{ij}= $ the observation outcome of the $ j^\mathrm{th}  $ item labelled the $ i^\mathrm{th}  $ level.

    Model:
    \begin{equation}
        Y_{ij}=\mu +\tau_i+\varepsilon _{ij} ,\, i=1,\ldots,r,\, j=1,\ldots,n_i
    \end{equation}
    
    where $ \mu  $ is the average effect of all $ r $ factor levels, $ \tau_i $ is the level effect of the $ i^\mathrm{th}  $ factor level, and $ \varepsilon  $ i.i.d. $ \sim N(0,\sigma ^2) $ is noise error.
    
    In matrix notation:

    \begin{subequations}\label{EqaFactorAnalysisModel}
        \begin{align}
            Y=&\begin{bmatrix}
        y_{11}&\ldots& y_{1n_1}&y_{21}&\ldots &y_{2n_2}&\ldots
        \end{bmatrix}^T\\
            X=&\begin{bmatrix}
            1&1&0&\ldots\\
            1&1&0&\ldots\\
            \vdots&\vdots&\vdots&\ddots\\
            1&0&1&\ldots\\
            1&0&1&\ldots\\
            \vdots&\vdots&\vdots&\ddots\\
            \vdots&\vdots&\vdots&\vdots
        \end{bmatrix}=
        \begin{bmatrix}
            \mathbf{1}_{n_1}&\mathbf{1}_{n_1}&0&\ldots &0\\
            \mathbf{1}_{n_2}&0&\mathbf{1}_{n_2}&\ldots &0\\
            \vdots&\vdots&\vdots&\ddots&\vdots\\
            \mathbf{1}_{n_r}&0&0&\ldots&\mathbf{1}_{n_r}
        \end{bmatrix}\\
        \tau=&\begin{bmatrix}
            \mu&\tau_1&\tau_2&\ldots
        \end{bmatrix}^T\\
        \varepsilon =&\begin{bmatrix}
            \varepsilon _{11}&\ldots&\varepsilon _{1n_1}&\varepsilon _{21}&\ldots&\varepsilon _{2n_2}&\ldots
                \end{bmatrix}^T
        \end{align}
    \end{subequations}

    For more factor model e.g. two-way factor analysis with $ k $ denoting item and $ i,j $ denoting factor:
    \begin{equation}
         Y_{ijk}=\mu +\alpha _i+\beta _j+\varepsilon  _{ij}
    \end{equation}

    cannot be simply expressed in matrix notation $ \longrightarrow $ use index notation.

    Assumption: Normal, Equal variance, independent
\begin{itemize}[topsep=2pt,itemsep=0pt]
    \item One-way: $ Y_{j|i} $ i.i.d. $ \sim N(\mu+\tau_i,\sigma ^2) $, $ \forall i $
    \item Two-way: $ Y_{k|ij} $ i.i.d. $ \sim N(\mu +\alpha _i+\beta _j,\sigma ^2) $, $ \forall i,j $
\end{itemize}

    % Note: estimator of $ \mu $ should be such that $ \sum_{i=1}^r\tau_i=0 $ or $ \sum_{i,j} (\alpha_i+\beta _j)=0  $.
    
    
    


























\subsection{Monovariate Linear Regression Model}
    First focus on the simpliest monovariate case $ \vec{X}_i=X_i $. Monovariate Linear Model\footnote{Here in linear regression, we consider $ X_i $ only as real number, \textbf{without} randomness. So here $ Y_i $ can be considered as an r.v. with $ X_i $ as parameter, i.e. $ Y_i|_{X_i=x_i} $}
    with \hyperlink{GaussMarkovAssumption}{Gauss-Markov assumption} \& Normal Error assumption:
    
%% 关于线性模型的X_i性质的假定
    \begin{equation}
        Y_i=\beta _0+\beta _1X_i+\varepsilon _i ,\, \varepsilon _i\text{ i.i.d.}\sim N(0,\sigma ^2) 
    \end{equation}


 
  


    What does Linear Regression do? Try to estimate 
    \begin{itemize}[topsep=0pt,itemsep=-2pt]
        \item $ \beta _0\text{ (intercept) }$;
        \item $\beta _1\text{ (slope) }$;
        \item $\sigma ^2\text{ (variance of error)} $.
    \end{itemize}
    
    
    (Thus Linear Regression is also a Statistics Inference process: deduce properties of model from data)
        
\subsubsection{The Ordinary Least Square Estimation}
    \index{OLS (Ordinary Least Squares)}
    Aim: use $ (x_i,y_i) $  to estimate $ \beta _0,\beta _1,\sigma^2 $. The idea is to define a 'loss function' to reflect the 'distance' from sample point to estimation point.

    Estimate Principle: \footnote{Detailed Definition and Derivation see \autoref{SubSectionMoM_MLE_LinearRegression} or \autoref{SubSectionMultivariateLinearRegressionModel}.}
    \begin{itemize}[topsep=2pt,itemsep=2pt]
        \item Ordinary Least Squares\index{OLS (Ordinary Least Squares)}:
        \begin{equation}
            (\hat{\beta  }_0,\hat{\beta _1})=\arg\min\sum_{i=1}^n (y-\beta _0-\beta _1x_i)^2
        \end{equation}
        \item MLE or MoM Estimation.
    \end{itemize}
    

    
    And get $ \hat{\beta _1},\hat{\beta _0}$ as well as $ \hat{\sigma^2} $(see \autoref{EqaOLSEstimatorOfSigma}:\footnote{A memory trick: use $ \dfrac{Y}{\sqrt{s_Y}}=r_{XY}\dfrac{X}{\sqrt{s_X}} $ to get formular of $ Y\sim X $:
    \begin{equation}
        \hat{\beta }_1=r_{XY}\dfrac{\sqrt{s_Y}}{\sqrt{s_X}}=\dfrac{{\displaystyle\sum (x_i-\bar{x})(y_i-\bar{y})}}{{\displaystyle\sum (x_i-\bar{x})^2}} 
    \end{equation}}

%LSE beta_0 beta_1
\begin{equation}\label{EqaOLSEstimatorOfBeta}
    \begin{aligned}
        \hat{\beta }_1=&\dfrac{\sum\limits_{i=1}^n (x_i-\bar{x})(y_i-\bar{y})}{\sum\limits_{i=1}^n (x_i-\bar{x})^2}\\
        \hat{\beta }_0=&\bar{y}-\hat{\beta _1}\bar{x}\\
        \hat{\sigma^2}=&\dfrac{1}{n-p-1}\sum_{i=1}^n(y_i-\hat{\beta }_0-\hat{\beta }_1x_i)^2
    \end{aligned}
\end{equation}


    
    Def. \index{Residual}\textbf{Residual}: distance from sample point to estimate point, to reflect how the sample points fit the model.
    \begin{equation}
        e_i=y_i-\hat{y}_i=\text{observed value of }\varepsilon _i 
    \end{equation}
    
    Note: under least square estimation, we have\footnote{Intuitively, they each means '$ E(\varepsilon )=0 $' and '$ X\parallel \varepsilon  $'.}
\begin{equation}\label{Limit_to_Residual}
        \sum e_i=0\qquad \sum x_ie_i=0 
\end{equation}
    

    Then use $ e_i $ to estimate $ \sigma ^2 $ (because it is $ \varepsilon _0 $ that are i.i.d., not $ Y_i $), where $ (n-p-1) $ is Degree of Freedom (df or dof)\footnote{Generally, MLE and OLSE are different.

    Comment from R.A.Fisher: $ \sum e_i^2 $ should be divided by 'number of $ e_i^2 $ that contribute to variance'. Here $ (n-p-1) $ corresponds to 'degree of freedom' $ =(n-2) $, $ p=1 $ corresponds to `one' variable (see \autoref{SubSectionMoM_MLE_LinearRegression}, \autoref{EqaEstimatorSigmaWithDoF}), and correponds to the two equations of $ e_i $, \autoref{Limit_to_Residual}}
\begin{equation}\label{EqaOLSEstimatorOfSigma}
    \begin{aligned}
        \hat{\sigma _n^2}&=\dfrac{1}{n}\sum e_i^2 \quad\text{(use MLE or MoM)}\\
        \hat{\sigma^2}&=\dfrac{1}{n-p-1}\sum e_i^2=\dfrac{1}{n-2}\sum e_i^2\quad\text{(use OLS, unbiased)}
\end{aligned}
\end{equation}

\textbf{Degree of Freedom}\index{dof/df (Degree of Freedom)@$ dof $/$ df $ (Degree of Freedom)} of a Quadric Form:\index{Degree of Freedom}
\begin{itemize}[topsep=2pt,itemsep=2pt]
    \item Intuitively: the number of independent variable;
    \item Rigorously: for quadric $ \mathrm{SS}=x'Ax $:
    \begin{equation}\label{EqaDefinitionOfDegreeOfFreedom}
        dof_{SS}=\mathrm{rank}(A)
    \end{equation}

    Which comes from Cochran's Theorem. A proof can be found here: \url{https://v1ncent19.github.io//texts/Cochran/}\index{Cochran's Theorem}
    
    
    
\end{itemize}

     

\begin{rcode}
\begin{lstlisting}[language=R]
lmfit <- lm(formula,df)
summary(lmfit,cor=TRUE)
ggcoef(lmfit)
\end{lstlisting}

    \lstinline|lmfit| includes parameters \lstinline|lmfit$coefficient| and \lstinline|lmfit$residuals|

    Example \lstinline|lm()| output:
\begin{lstlisting}[language=R]
    Call:
    lm(formula = y ~ x, data = df)
    
    Residuals:
         Min       1Q   Median       3Q      Max 
    -16.1368  -6.1968  -0.5969   6.7607  23.4731 
    
    Coefficients:
                Estimate Std. Error t value Pr(>|t|)    
    (Intercept) 156.3466     5.5123   28.36   <2e-16 ***
    x            -1.1900     0.0902  -13.19   <2e-16 ***
    ---
    Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
    
    Residual standard error: 8.173 on 58 degrees of freedom
    Multiple R-squared:  0.7501,    Adjusted R-squared:  0.7458 
    F-statistic: 174.1 on 1 and 58 DF,  p-value: < 2.2e-16
\end{lstlisting}

\end{rcode}

    % MSE SSE dof






    % Review: Statistical Inference
    % \begin{itemize}[topsep=6pt,itemsep=4pt]
    %     \item Basic concepts: HT CI;
    %     \item Inference about $ \beta _1$;
    %     \item Inference about $ \beta _0 $.
    % \end{itemize}

    % Note: the distribution of $ \hat{\beta }_0,\hat{\beta }_1 $ is sampling distribution（抽样分布）: distribution of statistics.



    % Power function of testing
    % \begin{itemize}[topsep=6pt,itemsep=4pt]
    %     \item Definition;
    %     \item Calculation;
    %     \item Sample<->Power (Calculation of sampling).
    % \end{itemize}


\subsubsection{Statistical Inference to $ \beta _0 $, $ \beta _1 $, $ \sigma ^2 $, $ e_i $}

\begin{point}
    Sampling Distribution of $ \hat{\beta} _1,\hat{\beta} _0  $
\end{point}

    Consider $ \hat{\beta} _1,\hat{\beta} _0 $ as statistics of sample, then we can examine the sampling distribution of $  \hat{\beta} _1,\hat{\beta} _0 $. Their randomness comes from
    \begin{equation}
        Y_i=\beta_0+\beta_1X_i+\varepsilon _i 
    \end{equation}
    
    

    (The following part treats $\hat{\beta} _1,\hat{\beta} _0 $ as r.v., and note that $ X_i $ are \textbf{not }r.v.. And  for convenience and conciseness, denote $ S_{XX}={\displaystyle\sum_{i=1}^n(X_i-\bar{X})^2} $)

   
\begin{align*}
        \hat{\beta }_1&=\beta _1+\sum_{i=1}^n\dfrac{X_i-\bar{X}}{S_{XX}}\varepsilon _i\\
        \hat{\beta }_0&=\beta _0+\sum_{i=1}^n\left(\dfrac{1}{n}-\dfrac{(X_i-\bar{X})\bar{X}}{S_{XX}}\right)\varepsilon _i
\end{align*}
 
    Denote corresponding variance as $ \sigma^2_{\hat{\beta}_1} $ and $ \sigma^2_{\hat{\beta}_0} $, using \autoref{EqaDistributionOfSumOfiidNormal} to get:
    \begin{equation}
        \sigma^2_{\hat{\beta}_1}= \dfrac{\sigma^2}{S_{XX}}\qquad \sigma^2_{\hat{\beta}_0}=\sigma^2(\dfrac{1}{n}+\dfrac{\bar{X}^2}{S_{XX}})
    \end{equation} 
    
     And under normal error assumption, distribution of $ \hat{\beta} _1,\hat{\beta} _0  $ are
    \begin{align*}
        \hat{\beta }_1&\sim N(\beta _1,\sigma^2_{\hat{\beta}_1}) =N(\beta_1,\dfrac{\sigma^2}{S_{XX}})\\
        \hat{\beta}_0&\sim N(\beta_0,\sigma^2_{\hat{\beta }_0}) =N(\beta_0,\sigma^2(\dfrac{1}{n}+\dfrac{\bar{X}^2}{S_{XX}}))
    \end{align*}
    
    Based on sampling distribution of $ \hat{\beta} _1,\hat{\beta} _0  $, we can conduct statistical inference, including CI and HT.\footnote{Detail see \autoref{SectionHypothesisTesting}, estimating/testing $ \hat{\beta} _1,\hat{\beta} _0  $ usually corresponds to 'estimate $ \mu $, with $ \sigma^2 $ unknown'.}
    
    % \begin{itemize}[topsep=2pt,itemsep=2pt]
    %     \item LSE of $ \beta _1 $ gives 
    %     \begin{equation}
    %         \hat{\beta _1}=\dfrac{\sum (x_i-\bar{x})(y_i-\bar{y})}{\sum (x_i-\bar{x})^2}
    %     \end{equation}
        
    %     and satisfies $ E(\hat{\beta}_1)=\beta_1 $. Can prove that $ \hat{\beta }_1\sim N(\beta _1,\dfrac{\sigma ^2}{\sum (x_i-\bar{x})^2})=N(\beta_1,\sigma^2(\hat{\beta}_1))$
       
    % \end{itemize}
    
    Note: In linear regression model, we usually focus more on $ \beta_1 $. And note that when $ 0 $ is \textbf{not} within the fitting range,$ \beta_0 $ is not so important.\footnote{Two reason:\begin{itemize}[topsep=2pt,itemsep=2pt]
        \item The etimation error of $ Y $ from $ \hat{\beta}_1 $ increases with $ X_h-\bar{X} $;
        \item $ \beta_1==0  $ is important: decides whether linear model can be used. 
    \end{itemize}}


\begin{point}
    Sampling Distribution of $ e_i $ 
\end{point}
    Consider $ e_i $ as r.v. satisfies
    \begin{equation}
        e_i= Y_i-\hat{Y}_i=Y_i-\hat{\beta }_0-\hat{\beta }_1X_i
    \end{equation}

    and get the expression of $ \hat{e}_i $
    \begin{equation}
        \begin{aligned}
            \hat{e}_i=\varepsilon _i-\sum_{k=1}^n\left( \dfrac{1}{n}+\dfrac{(X_i-\bar{X})^2}{S_{XX}} \right)\varepsilon _k
        \end{aligned}
    \end{equation}
    
    
    \begin{equation}
        \mathbb{E}(e_i)=0\qquad \sigma ^2_{e_i}=\sigma ^2 \left( 1-\dfrac{1}{n}-\dfrac{(X_i-\bar{X})^2}{S_{XX}} \right)
    \end{equation}

    Under normal assumption:
    \begin{equation}\label{EqaSamplingDistributionOfResiduals}
        \varepsilon _i\sim N(0,\sigma ^2\left( 1-\dfrac{1}{n}-\dfrac{(X_i-\bar{X})^2}{S_{XX}} \right) ) 
    \end{equation}
    

    Further we can get $ \hat{\sigma }^2=\mathbb{E}(\dfrac{1}{n-2}\sum_{i=1}^ne_i^2) $ where $ e_i^2\sim \sigma ^2\left( 1-\dfrac{1}{n}-\dfrac{(X_i-\bar{X})^2}{S_{XX}} \right)\chi^2 $
    \begin{equation}
        \hat{\sigma }^2=\dfrac{1}{n-2}\sigma ^2\sum_{i=1}^n(1-\dfrac{1}{n}-\dfrac{(X_i-\bar{X})^2}{S_{XX}})=\sigma ^2
    \end{equation}
    
    More definition of refined residuals see \autoref{SubSecDiagnostics} in page~\ref{SubSecDiagnostics}.
    


\begin{point}
    Why we choose OLS to get regression coefficients?\index{Gauss-Markov Thm.}
\end{point}

    Gauss–Markov Thm.: the OLS estimator has the lowest sampling variance within the class of linear unbiased estimators, i.e. OLS is the \textbf{Best Linear Unbiased Estimator(BLUE)}.\footnote{This Thm. does \textbf{not }require normal error assumption.}\index{BLUE (Best Linear Unbiased Estimator)}



\subsubsection{Prediction to $ Y_h $}
    For a new $ X_h $ at which we wish to \textbf{predict }the corresponding $ Y_h $ (based on other known point $ (X_i,Y_i) $), denote the estimator as $ \hat{\mu}_h $:
    \begin{equation}
        \hat{\mu}_h=\hat{\beta}_1X_h+\hat{\beta}_0 =\beta_1X_h+\beta _0+\sum_{i=1}^n\left( \dfrac{1}{n}+\dfrac{(X_i-\bar{X})(X_h-\bar{X})}{S_{XX}} \right)\varepsilon _i
    \end{equation}
    
    Thus we can get\footnote{So $ \sigma ^2(\hat{\mu }_h) $ increases with $ X_h-\bar{X} $. Intuitively it make sense, because $ (\bar{X},\bar{Y})$ must falls on regression line.}
    \begin{equation}
        \mathbb{E}(\hat{\mu}_h)= \beta _1X_h+\beta _0\qquad \sigma ^2_{\hat{\mu}_h}=\left( \dfrac{1}{n}+\dfrac{(X_h-\bar{X})^2}{S_{XX}} \right)\sigma^2
    \end{equation}
    
    Under Normal assumption:
    \begin{equation}
        \hat{\mu}_h\sim N(\beta _1X_h+\beta _0,\left( \dfrac{1}{n}+\dfrac{(X_h-\bar{X})^2}{S_{XX}} \right)\sigma^2) 
    \end{equation}
    
    Base on distribution we can give CI and HT.

\begin{point}
    We can either consider :
\end{point}

    \begin{itemize}[topsep=2pt,itemsep=2pt]
        \item $ \hat{\mu }_h $ as a function of all data points: 
        We can just use $  \sigma ^2_{\hat{\mu}_h} $ to construct Confidence Interval of $ Y_h $;

        \begin{rcode}
\begin{lstlisting}[language=R]
predict(lmfit,newdata = 40),
    interval="confidence",level=0.95)
\end{lstlisting}
        \end{rcode}
        \item $ \hat{\mu }_h $ as a function of all data points, and $ Y_h $ in generated from the model, which also has randomness: Prediction Interval of $ Y_h $ need to  have both the randomness of $ Y_h,\,\hat{\mu }_h $ considered.

        Def. Prediction Error: $ Y_h $ itself is an $ Y $ of the linear model, i.e. $ Y_i=\beta_0+\beta_1X_h+\varepsilon _h $, we can define \textbf{Prediction Error}: 
        \begin{equation}
            d_h=Y_h-\hat{\mu}_h 
        \end{equation}
    
        with
        \begin{equation}
            \mathbb{E}(d_h)=0\qquad \sigma^2_{d_h}=var(Y_h-\hat{\mu }_h)=\left[ 1+\dfrac{1}{n}+\dfrac{(X_h-\bar{X})}{S_{XX}} \right]\sigma^2 > \sigma ^2_{\hat{\mu}_h}
        \end{equation}
\begin{rcode}
\begin{lstlisting}[language=R]
predict(lmfit,newdata = 40),
    interval="prediction",level=0.95)
\end{lstlisting}
\end{rcode}
    
    \end{itemize}

    Comment: in prediction error, we considered more random component, thus the CI is also larger.
    
    % Remember that when we consider the estimator $ \hat{\mu } $, we \textbf{must } have the randomness of $ \hat{\beta }_0,\hat{\beta }_1 $ considered(if they are unknown).
    


    \begin{point}
       Simultaneous Confidence Band (SCB)\index{SCB (Simultaneous Confidence Band)}\index{CB (Confidence Band)}\index{Confidence Region!Confidence Band}
    \end{point}

    Confidence Band is \textbf{not} the CI at each point, but really a \textbf{band} for the \textbf{entire} regression line.\footnote{Why they are different? We require the confidence band have a \textbf{simultaneous} converage probability. For the same band $ (L(x),U(x)) $, $ P(\text{the whole line})< P(\text{each point})$, so Confidence Band is wider than $ \bigcup $CIs to hold the same $ 1-\alpha $.
    
    Also, we will see that for linear model, the boundary of SCB forms hyperbola, which make sense considering its asymptotic line.}
    
    
    Aim: Find lower and upper function $ L(x) $ and $ U(x) $ such that
    \begin{equation}
        \mathbb{P}[L(x)<(\beta _0+\beta _1x)<U(x),\,\forall x\in I_x]=1-\alpha  
    \end{equation}
    
    and get \textbf{Confidence Band}:\index{Confidence Band}
    \begin{equation}
        \{(x,y)|L(x)<y<U(x)|\forall x\in I_x\} 
    \end{equation}
    
    % Note: \textbf{Cannot} use CI at each point to form Confidence Band. Band is wider. And we are actually conduce CI \textbf{simoutanesly} to all $ x $.

    Where $ (L(x),U(x)) $ can be derived as
    \begin{equation}
        (L(x),U(x))=\hat{\mu}_x\pm s_{\hat{\mu}_x}W_{2,n-2,1-\alpha}
    \end{equation}

    Where $ W $ correponds to $ W $ distribution: $ W_{m,n}=\sqrt{2F_{m,n}} $
    
    
    
    Small sample case: Bonferroni correction.
    
\begin{rcode}
\begin{lstlisting}[language=R]
library(ggplot2)
ggplot(df,aes(x,y))+geom_point()+geom_smooth(method='lm',formula=y~x)
\end{lstlisting}
\end{rcode}


\subsubsection{Analysis of Variance: Monovariate}
    \index{ANOVA (Analysis of Variance)}\textbf{AN}alysis \textbf{O}f \textbf{VA}riance (ANOVA): \hyperlink{OneSampletTest}{One-sample $ t $ test} $\rightsquigarrow $ \hyperlink{TwoSampletTest}{Two sample $ t $ test} $\rightsquigarrow $ More-sample: ANOVA

\begin{point}
    \textbf{Key Idea Of ANOVA}: Test whether the mean of some groups are the same, i.e. $ \mu_1=\mu _2=\ldots=\mu _r $ 
\end{point}



    In linear regression model, modified as testing $ \beta _1==0 $. Conduction: Take Partition of Total Sum of Square To Examine \textbf{Variation}. Because $ Y_i $ are not i.i.d. (different mean value $ X\beta  $), so has different parts of variation from Regression Model/Error Term.


% \begin{itemize}[topsep=2pt,itemsep=2pt]
%     \item Partition of Totla Sum of Squares;
%     \item Partition of Degree of Freedom;
%     \item MSS$ \rightsquigarrow $ F-test;
%     \item ANOVA table;
%     \item General linear test. --to be examined further in later sections.
%     \item (Pearson) Correlation Coefficient $ \leftrightarrow \, R^2$
% \end{itemize}


    Measure of Variation: Sum of Square (SS) \& Mean Sum of Square (MS).

    MS: Divide each SS by corresponding $ dof $. Definition of $ dof $ see \autoref{EqaDefinitionOfDegreeOfFreedom}.
    \begin{equation}
        \mathrm{MS}=\dfrac{\mathrm{SS}}{dof} 
    \end{equation}

\begin{itemize}[topsep=2pt,itemsep=2pt]
    \item SST: Total Sum of Squares\index{SST (Total Sum of Squares)}
    \begin{equation}
        \mathrm{SST}=\sum_{i=1}^n(Y_i-\bar{Y})^2 \qquad dof_{\mathrm{SST}}=n-1
    \end{equation}
    \item SSRegression: Variation due to Regression Model \index{SSR (Regression Sum of Squares)} (which is explained by regression line);\footnote{$ \mathrm{SSR}=\hat{\beta }_1^2\sum_{i=1}^n(X_i-\bar{X})^2$, so $ dof_R=1 $}
    \begin{equation}
        \mathrm{SSR}= \sum_{i=1}^n(\hat{Y}_i-\bar{Y})^2 \qquad dof_{\mathrm{SSR}}=1
    \end{equation}
    
    \item SSError: Variation attribtes to $ \varepsilon  $ \index{SSE (Error Sum of Squares)} (which is reflected by residuals).
    \begin{equation}
        \mathrm{SSE}= \sum_{i=1}^n(Y_i-\hat{Y_i}) \qquad dof_{\mathrm{SSE}}=n-2
    \end{equation}
\end{itemize}

\fbox{
    \begin{minipage}{0.9\linewidth}
        $ \Delta $ \textbf{IMPORTANT: }In some books \begin{itemize}[topsep=2pt,itemsep=2pt]
        \item SSRegression $ \to $ SSExplained or SSModel;
        \item SSError $ \to $ SSResidual.
    \end{itemize}

    And Cause \textbf{Confusion}! In this summary we take the former.
    \end{minipage}
}\\



    Idea: take partition of SST. i.e.
    \begin{equation}
        Y_i-\bar{Y}=(Y_i-\hat{Y})+(\hat{Y}-\bar{Y})=e_i 
    \end{equation}
    
    And we can prove that
    \begin{equation}
        \mathrm{SST}=\sum_{i=1}^n(Y_i-\bar{Y})^2=\sum_{i=1}^n(\hat{Y}_i-\bar{Y})^2+\sum_{i=1}^n(Y_i-\hat{Y_i})^2=\mathrm{SSR+SSE} 
    \end{equation}

    That is: we \textbf{partition} SST into two parts, so that we can examine them seperately.
    

    \begin{point}
        ANOVA Table
    \end{point}
    
        \begin{table}[H]
            \centering
            \renewcommand\arraystretch{1}
            \begin{tabular}{c|cccc}
                \hline
                Source&$ dof $&SS&MS&$ F $-Statistic\\\hline
                SSRegression&$ 1 $&$ \sum_{i=1}^n(\hat{Y}_i-\bar{Y})^2  $&SSR/$ dof_R $& $ \mathrm{MSR}/\mathrm{MSE} $\\
                SSError&$ n-2 $&$ \sum_{i=1}^n(Y_i-\hat{Y}_i)^2  $&SSE/$ dof_E $& \\
                SSTotal&$ n-1 $&$ \sum_{i=1}^n(Y_i-\bar{Y})^2  $&SST/$ dof_T $& \\
                \hline
            \end{tabular}
        \end{table}
    \begin{rcode}
\begin{lstlisting}[language=R]
anova(lmfit)
\end{lstlisting}
    \end{rcode}    





Properties:
\begin{equation}
    \mathbb{E}(\mathrm{MSE})=\sigma ^2\qquad \mathbb{E}(\mathrm{MSR})=\sigma ^2+\beta _1^2S_{XX} 
\end{equation}



\subsection{Multivariate Linear Regression Model}\label{SubSectionMultivariateLinearRegressionModel}
    As a more general case of $ \vec{X}_i=(X_{i1},X_{i2},\ldots,X_{ip})  $, Multivariate Linear Model is expressed as in \autoref{EqaMultivariteLinearModel}:
    \begin{equation}
        Y=X\beta +\varepsilon ,\,\varepsilon \sim N_p(0,\sigma ^2I) 
    \end{equation}


\subsubsection{The Ordinary Least Estimation}
    To conduct OLS\index{OLS (Ordinary Least Squares)}
    \begin{equation}
        \hat{\beta }=\mathop{ \arg\min }\limits_{\beta \in \mathbb{R}^{p+1} } (Y-X\beta )^T(Y-X\beta )
    \end{equation}
    
    Here we introduce two approaches:
\begin{itemize}[topsep=2pt,itemsep=2pt]
    \item Analytical: Take matrix differciation (See \autoref{SubSubSectionMatrixNotationAndLemma} \autoref{EqaMatrixDifferential})
    
   

\begin{align*}
    0&=\dfrac{\partial^{} (Y-X\beta )^T(Y-X\beta ) }{\partial \beta ^{}} =\dfrac{\partial^{} }{\partial\beta  ^{}}(Y^TY- Y^TX\beta -\beta ^TX^TY+\beta ^TX^TX\beta )\\ 
    &=-X^TY-X^TY+(X^TX+XX^T)\beta 
    =-2X^T(Y-X\beta )
\end{align*}
    
    Thus we get OLS:
    \begin{equation}
        \hat{\beta }=(X'X)^{-1}X'Y 
    \end{equation}
    
    
    \item Geometric/Algebraical: Use hyper-projection.
    \begin{equation}
        \hat{\beta }=\mathop{ \arg\min }\limits_{\beta \in \mathbb{R}^{p+1} } d(Y,X\beta )
    \end{equation}

    i.e. $ \hat{\beta } $ is the (hyper-)projection of $ Y $ onto $ X $ (within Euclidean Space), naturally we have
    \begin{equation}
        (X\beta )^T(Y-X\beta )=0\Rightarrow \hat{\beta }=(X'X)^{-1}X'Y 
    \end{equation}

\end{itemize}

\begin{point}
    Matrix Notation of OLS Estimator:
    \begin{equation}
        \hat{\beta }=(X'X)^{-1}X'Y 
    \end{equation}
\end{point}

    % (For simplification, the following part consider multivariate $ \mathop{X}\limits_{n\times (p+1)}  $ with one $ \mathop{Y}\limits_{n\times 1}  $)

\subsubsection{Statistical Inference to $ \beta  $, $ \sigma ^2 $, $ e $}\label{SubSubSectionStatisticalInferenceInMultiLRA}
    Properties \& Extrapolation
\begin{itemize}[topsep=2pt,itemsep=2pt]
    \item Sampling Distribution of $ \hat{\beta } $: (Here consider normal case $ Y\sim N(X\beta ,\sigma^2I_n) $, and use \autoref{EqaTransformOfMultiNormal}) 
    \begin{equation}\label{EqaDistributionOfMultiVariateBeta}
        \hat{\beta }=(X'X)^{-1}X'Y \sim N_p(\beta,\sigma^2(X'X)^{-1})
    \end{equation}

    Comment: $ cov(\beta_i,\beta_j ) $ are generally not 0, $ \Rightarrow $ $ \beta _i,\beta _j $ dependent.
    \item Predicted Response \& Hat Matrix $ H $:
    \begin{equation}
        \hat{Y}=X\hat{\beta }=X(X'X)^{-1}X'Y\equiv  HY=P_XY
    \end{equation}

    where \textbf{Hat Matrix}/Influence matrix/Projection matrix $ H=P_X=X(X'X)^{-1}X' $, with properties
    \begin{itemize}[topsep=2pt,itemsep=2pt]
        \item Symmertric: $ H^T=H $;
        \item Idempotence: $ H^2=H $
        \item Rank: $ \mathrm{rk}(H)=tr(H)=\mathrm{rk}(X)   $
        \item $ H $ and self-influene factor $ h_{ii} $: Note the linearity of $ \hat{Y} $ on $ Y $
        \begin{equation}
            \hat{Y}=HY \Rightarrow H=\dfrac{\partial^{} \hat{Y}}{\partial Y^{}}
        \end{equation} 
    
        The diagonal elements of $ H $ is 
        \begin{equation}
            h_{ii}=\dfrac{\partial^{}\hat{y}_i}{\partial y_i^{}}=X_i(X'X)^{-1}X'_i
        \end{equation}

        Comment on $ h_{ii} $: $ var(e_i) =\sigma ^2(1-h_{ii})$, for $ h_{ii}\to 1 $, i.e. the regression line always pass $ y_i $, thus it's `influential'.
    \end{itemize}

    \item Residual:
    \begin{equation}\label{EqaMatrixNotationOfResidual}
        e=Y-\hat{Y}=(I-H)Y\sim N_n\left(0 , \sigma ^2(I-H) \right)
    \end{equation}

    where $ I-H $ is the complementary projection of $ X $

    Covariance Matrix of Residual:
    \begin{equation}
        cov(e)=\sigma ^2(I-H)=
        \sigma ^2\begin{bmatrix}
        1-h_{11}&-h_{12}&\ldots&-h_{1n}\\
        -h_{21}&1-h_{22}&\ldots&-h_{2n}\\
        \vdots&\vdots&\ddots&\vdots\\
        -h_{n1}&-h_{n2}&\ldots&1-h_{nn}\\
        \end{bmatrix}
    \end{equation}
    
    
    \item Estimator and Distribution of  $ \sigma ^2 $:
    
    First use \autoref{EqaExpectationOfQuadric} to get \footnote{Also we need the property of idmpotnet matrix
    \begin{equation}
        \lambda_i=0\text{ or }1\Rightarrow tr(H)=\mathrm{rank}(H)=\sum_{i=1}^n\lambda _i=\# (\lambda =1) 
    \end{equation}
    }
    \begin{equation}\label{EqaExpectationOfSSE}
        \mathbb{E}(\mathrm{SSE})=\mathbb{E}(e'e)=\mathbb{E}(Y'(I-H)Y)=(X\beta )'(I-H)X\beta +tr((I-H)\sigma ^2I_n) =\sigma ^2(n-p-1)
    \end{equation}

    $ dof $ of Residual $ e $ (use definition \autoref{EqaDefinitionOfDegreeOfFreedom}):
    \begin{equation}
        dof_e=dof_{(I-H)Y}=\mathrm{rank}(I-H)=n-p-1 
    \end{equation}
    
    
    
    Thus the unbiased estimator of $ \sigma ^2 $ is 
    \begin{equation}
        \hat{\sigma }^2=\mathrm{MSE}=\dfrac{e'e}{n-p-1}=\dfrac{Y'(I-H)Y}{n-p-1}
    \end{equation}

    Distribution (under normal assumption):
    
    \begin{equation}\label{EqaDistributionOfMultiVariateSigma}
        \dfrac{(n-p-1)\hat{\sigma }^2}{\sigma ^2}\sim \chi^2_{n-p-1}
    \end{equation}
    
    \item Gauss–Markov Thm.: OLS Estimator of $ \beta  $ is the BLUE Estimator.
\end{itemize}

\noindent More hypethesis testing to $ \beta  $ see \autoref{SubSectionMultivariateHypothesisTesting}.



\subsubsection{Prediction to $ Y_h $}
    For a new $ \vec{X}_h $ at which we wish to \textbf{predict }the corresponding $ Y_h $ (based on other known point $ (X_i,Y_i) $), denote the estimator as $ \hat{\mu}_h $:
    \begin{equation}
        \hat{\mu }_h=X_h'\hat{\beta }=X_h'(X'X)^{-1}X'Y
    \end{equation}
    
    thus we get 
    \begin{equation}
        \mathbb{E}(\hat{\mu }_h)= X_h'\beta  \qquad \sigma ^2_{\hat{\mu}_h}=\sigma ^2(1+X_h'(X'X)^{-1}X_h)
    \end{equation}

    under normal assumption:
    \begin{equation}
        \hat{\mu}_h\sim N(X'\beta,\sigma ^2(1+X_h'(X'X)^{-1}X_h))
    \end{equation}
    
    
    
    
    

    

\subsubsection{Analysis of Variance: Multivariate}
    Sampling Notation see \autoref{EqaSampleNotationOfMultiLinear}, still consider $ (p+1) $ -dim $ (\mathbf{1}_n,X_i) $ v.s. $ 1 $-dim $ Y $, and $ \beta=(\beta _0,\beta _1,\beta _2,\ldots,\beta _p) $\index{ANOVA (Analysis of Variance)}

\begin{itemize}[topsep=2pt,itemsep=2pt]
    \item SST:
    \begin{equation}
        \mathrm{SST}=(Y-\bar{Y}\mathbf{1}_n)'(Y-\bar{Y}\mathbf{1}_n)\qquad dof_{\mathrm{SST}}=n-1
    \end{equation}
    \item SSR:
    \begin{equation}
         \mathrm{SSR}=(\hat{Y}-\bar{Y}\mathbf{1}_n)'(\hat{Y}-\bar{Y}\mathbf{1}_n)\qquad dof_{\mathrm{SSR}}=p
    \end{equation}

    Denoted in hat matrix $ H $ and $ \mathcal{J} $ in \autoref{EqaAllOneMatrix}
    
    \begin{equation}\label{EqaSSMInMatrixNotation}
        \mathrm{SSR}=Y'(H-\dfrac{1}{n}\mathcal{J})Y 
    \end{equation}
    
    
    \item SSE:
    \begin{equation}
         \mathrm{SSE}=(Y-\hat{Y})'(Y-\hat{Y})\qquad dof_\mathrm{SSE}=n-p-1
    \end{equation}

    Denoted in residual $ e $ and hat matrix $ H $:
    \begin{equation}
        \mathrm{SSE}=e'e=Y'(I-H)Y 
    \end{equation}
    
    
    
\end{itemize}

More knowledge about multivariate ANOVA see \autoref{SubSubSectionExtraSumOfSquare}.






\begin{point}
    ANOVA Table
\end{point}

    \begin{table}[H]
        \label{TableMultivariateANOVA}
        \centering
        \renewcommand\arraystretch{1}
        \begin{tabular}{c|cccc}
            \hline
            Source&$ dof $&SS&MS&$ F $-Statistic\\\hline
            SSRegression&$ p $&$ \sum_{i=1}^n(\hat{Y}_i-\bar{Y})^2  $&SSR/$ dof_R $& $ \mathrm{MSR}/\mathrm{MSE} $\\
            SSError&$ n-p-1 $&$ \sum_{i=1}^n(Y_i-\hat{Y}_i)^2  $&SSE/$ dof_E $& \\
            SSTotal&$ n-1 $&$ \sum_{i=1}^n(Y_i-\bar{Y})^2  $&SST/$ dof_T $& \\
            \hline
        \end{tabular}
    \end{table}
\begin{rcode}
    \begin{lstlisting}[language=R]
anova(lmfit)
    \end{lstlisting}
\end{rcode}    






     
    % Basic Diagnostics including to $ X $, to leverage, etc. see \autoref{SubSectionDiagnosticsForMonoModel}.




\subsection{Diagnostics}\label{SubSectionDiagnosticsForMonoModel}


\colorlet{lcfree}{Green3}
\colorlet{lcnorm}{Blue3}
\colorlet{lccong}{Red3}
% -------------------------------------------------
% Set up a new layer for the debugging marks, and make sure it is on
% top
\pgfdeclarelayer{marx}
\pgfsetlayers{main,marx}
% A macro for marking coordinates (specific to the coordinate naming
% scheme used here). Swap the following 2 definitions to deactivate
% marks.
\providecommand{\cmark}[2][]{%
  \begin{pgfonlayer}{marx}
    \node [nmark] at (c#2#1) {#2};
  \end{pgfonlayer}{marx}
  } 
\providecommand{\cmark}[2][]{\relax} 

\begin{figure}[htbp]



\begin{center}
    \begin{tikzpicture}[%
        >=triangle 60,              % Nice arrows; your taste may be different
        start chain=going below,    % General flow is top-to-bottom
        node distance=6mm and 60mm, % Global setup of box spacing
        every join/.style={norm},   % Default linetype for connecting boxes
        ]
    % ------------------------------------------------- 
    % A few box styles 
    % <on chain> *and* <on grid> reduce the need for manual relative
    % positioning of nodes
    \tikzset{
      base/.style={draw, on chain, on grid, align=center, minimum height=4ex},
      proc/.style={base, rectangle, text width=8em},
      test/.style={base, diamond, aspect=2, text width=5em},
      term/.style={proc, rounded corners},
      % coord node style is used for placing corners of connecting lines
      coord/.style={coordinate, on chain, on grid, node distance=6mm and 25mm},
      % nmark node style is used for coordinate debugging marks
      nmark/.style={draw, cyan, circle, font={\sffamily\bfseries}},
      % -------------------------------------------------
      % Connector line styles for different parts of the diagram
      norm/.style={->, draw, lcnorm},
      free/.style={->, draw, lcfree},
      cong/.style={->, draw, lccong},
      it/.style={font={\small\itshape}}
    }
    \node [term] (t0) {Collect Data};
    \node [proc,join] (p1) {Preliminary checks on data quality};
    \node [proc,join] (p2) {Diagnostics for relationships and  strong interactions};
    \node [test,join] (t3) {Remedies needed?};
    \node [proc,join] (p5) {Determine several potentially useful subsets of explanatory variables; include known essential variables};
    \node [proc,right=of t0] (p6) {Investigate curvature and interaction effects more fully};
    \node [proc,join] (p7) {Study residuals and other diagnostics};
    \node [test,join] (t8) {Remedies needed?};
    \node [proc,join] (p10) {Select tentative model};
    \node [test,join] (t11) {Validity checks?};
    \node [term,join] (t12) {Final regression model};
    %lines
    \node [coord,left=of t3] (c1) {};
    \path (t3.west) to node [yshift=-1em] {Yes} (c1); 
    \draw [->,lcnorm] (t3.west) -- (c1) |- (p2);
    \node [coord,above=18mm of p6] (c3) {};
    \draw [->,lcnorm] (p5.south) --++(0,-7mm) --++(27mm,0) |- (c3) --(p6);
    \node [coord,below=of t3] (c2) {};
    \path (t3.south) to node [xshift=1.2em,yshift=-1.5em] {No} (c2);
    \node [coord,right=of t8] (c4) {};
    \path (t8.east) to node [yshift=-1em] {Yes} (c4); 
    \draw [->,lcnorm] (t8.east) -- (c4) |- (p6);
    % \path (t8.south) to node [xshift=1.2em,yshift=-2em] {No} (c2);
    \node [coord,left=of t11] (c6) {};
    \draw [->,lcnorm] (t11.west) -- (c6) |- (t0);
    \node [coord,below=of t8] (c7) {};
    \path (t8.south) to node [xshift=1.2em,yshift=-1.4em] {No} (c7);
    \path (t11.west) to node [yshift=-1em] {No} (c6); 
    \node [coord,below=of t11] (c5) {};
    \path (t11.south) to node [xshift=1.2em,yshift=-1.4em] {Yes} (c5);
    \end{tikzpicture}
    \caption{Diagnostics and Remedies for Regression Model}
    
\end{center}
\end{figure}






    To apply OLS, we need the basic Gauss–Markov Assumption \autoref{EqaGaussMarkovAssumption}; or we further need better properties of the model, e.g. take Normal Assumption.
    
    Assumptions:
    \begin{equation}
        \begin{aligned}
            \text{Zero-Mean: }&\mathbb{E}(\epsilon_i|X_i)=0 \\
            \text{Homogeneity of Variance: }&var(\epsilon_i)=\sigma^2\\
            \text{Independent: }&\epsilon_i\text{ i.i.d. }\sim \varepsilon\\
            \text{Normal: }&\varepsilon \sim N(0,\sigma^2)
        \end{aligned}
    \end{equation}
    
    Or sum up as 
    \begin{equation}
        Y\sim N_n(X\beta ,\sigma^2I_n) 
    \end{equation}
    
    
    
    Thus we need to conduct Diagnostics and Remedies to 
    \begin{itemize}[topsep=2pt,itemsep=2pt]
        \item examine whether these assumptions are satisfies;
        \item perform correction to regression method.
    \end{itemize}

    \noindent Preliminary Diagnostics:

\begin{rcode}
\begin{lstlisting}[language=R]
lmfit <- lm(y~x,lmfit)
par(mfrow = c(2, 2))
plot(lmfit)
\end{lstlisting}
\end{rcode}
    

\subsubsection{Useful Diagnostics Plots}

        \begin{itemize}[topsep=2pt,itemsep=2pt]
            \item BoxPlot: to examine the similarity of  distribution.
            
            Notation:
            \begin{enumerate}[topsep=2pt,itemsep=2pt]
                \item min point above 25\% quartile-1.5IQR;
                \item 25\% quartile;
                \item median;
                \item 75\% quartile;
                \item max point below 75\% quartile+1.5IQR.
            \end{enumerate}
            
                
            \begin{center}
                \begin{tikzpicture}
                    \draw(-2,-0.6)rectangle(2,0.6);
                    \draw (-5.5,0)--(-2,0);
                    \draw (2,0)--(5.5,0);
                    \draw(-5.5,-0.7)--(-5.5,0.7);
                    \draw(5.5,-0.7)--(5.5,0.7);
                    \draw(0,-0.6)--(0,0.6);
                    \node at (-2,1){$ 2 $};
                    \node at (2,1){$ 4 $};
                    \node at (-5.5,1.1){$ 1 $};
                    \node at (5.5,1.1){$ 5 $};
                    \node at (0,1){$ 3 $};
                \end{tikzpicture}
            \end{center}


    
            

        \item Histogram Plots: Frequency distribution (can deal with many-peak)
            
        \item Quartile-Quartile Plots\index{QQ-Plot (Quartile-Quartile Plots)}\hypertarget{QQplot}{}: Examine the similarity  between distribution.
            
        For two CDF $ q=F(x) $ and $ q=G(x) $(where $ q $ for quartile), with $ x=F^{-1}(q) $, $ x=G^{-1}(q) $. And Plot $ F^{-1}(q) $-$ G^{-1}(q) $.

        Usually test normality, take $ G=\Phi  $



        \item \hypertarget{AVPlot}{Partial Regression Plot}:\index{Partial Regression Plot} Test non-linearity/heterogeneous-variance.
    
        For each $ X_i $ variable: 
        \begin{itemize}[topsep=2pt,itemsep=0pt]
            \item Use other $ X_{(\wedge i)} $ to predict $ Y $, get residual $ e_Y|X_{(\wedge i)} $;
            \item Use other $ X_{(\wedge i)} $ to predict $ X_i $, get residual $ e_{X_i}|X_{(\wedge i)} $
        \end{itemize}
        
        Plot $ (e_Y|X_{(\wedge i)}) $-$ (e_{X_i}|X_{(\wedge i)}) $ as Added Variable Plot (AV Plot)\index{AV Plot (Added Variable Plot)}. Used for testing non-linearity/heterogeneous-variance.
    \end{itemize}

     
\begin{rcode}
\begin{lstlisting}[language=R]
boxplot(df$x)

hist(df$x)

hist(df$x,freq=FALSE)
lines(density(df$x))

stem(df$x)

qqnorm(df$x)
qqline(df$x,col='red')

library(car)
avPlots(lmfit)
\end{lstlisting}

\end{rcode}




    
\subsubsection{Diagnostics to $ X $ Distribution}\label{SubSecXDiagnostics}





    Considering the dependence of $ Y_i $ on $ X_i $, to get a more reliable $ \hat{\beta }_1 $, we cannot just focus on the (marginal) distribution of $ Y_i $, we would also need a better 'distribution' of $ X_i $
    \begin{itemize}[topsep=2pt,itemsep=2pt]
        \item Plots: BoxPlot/QQPlot
        \item 4 statistics(parameters);\footnote{See \autoref{SubSectionStatistics}}
        \begin{itemize}[topsep=2pt,itemsep=2pt]
            \item Mean: Location;
            \begin{equation}
                \bar{X}=\dfrac{1}{n}\sum_{i=1}^nX_i 
            \end{equation}
            \item Standard Deviation: Variability;
            \begin{equation}
                S^2=\dfrac{1}{n-1}\sum_{i=1}^n(X_i-\bar{X}) ^2
            \end{equation}
            
            
            \item Skewness: Lack of Symmertry;\index{Skewness}
            \begin{equation}
                \hat{g}_1=\dfrac{m_{n,3}}{m_{n,2}^{3/2}}=\dfrac{\frac{1}{n}\sum\limits_{i=1}^n(X_i-\bar{X})^3}{\left( \frac{1}{n}\sum\limits_{i=1}^n(X_i-\bar{X}) \right)^{3/2}} 
            \end{equation}

            Adjusted Skewness (Least MSE):\index{Skewness!Adjusted Skewness}
            \begin{equation}
                \dfrac{\sqrt{n(n-1)}}{n-2}\hat{g}_1 
            \end{equation}
            
            \begin{itemize}[topsep=2pt,itemsep=2pt]
                \item $ \hat{g}_1>0 $: Right skewness, longer right tail;
                \item $ \hat{g}_1<0 $: Left skewness, longer left tail.
            \end{itemize}
            
                
            Fisher-Pearson coefficient of skewness: $ \dfrac{3(\mathrm{mean}-\mathrm{median})}{\sigma } $.


            \item Kurtosis: Heavy/Light Tailed.\index{Kurtosis}
            \begin{equation}
                \hat{g}_2=\dfrac{m_{n,4}}{m_{n,2}^2}-3= \dfrac{\frac{1}{n}\sum\limits_{i=1}^n(X_i-\bar{X})^4}{\left( \frac{1}{n}\sum\limits_{i=1}^n(X_i-\bar{X})^2 \right)^{2}} -3
            \end{equation}

            $ \hat{g}_2=0 \Rightarrow $ similar to normal.
            \begin{itemize}[topsep=2pt,itemsep=2pt]
                \item $ \hat{g}_2>0 $: Leptokurtic, heavy tail, slender;\index{Kurtosis!Leptokurtic}
                \item $ \hat{g}_2<0 $: Platykurtic, light tail, broad.\index{Kurtosis!Platykurtic}
            \end{itemize}
            
            Note: In expression of $ \hat{g}_1 $ and $ \hat{g}_2 $, we already divide the variance. So Skewness and Kurtosis only reflect the difference from normal, but \textbf{not}  related to variance.
                
            Best tool to determine Kurtosis: \hyperlink{QQplot}{QQ-Plot}.
            
        \end{itemize}

\begin{rcode}
\begin{lstlisting}[language=R]
summary(df$x)
\end{lstlisting}

    Other moments use package \lstinline|moments|
\end{rcode}
        
    
        \item Bias: Inspect the design methodology
        \begin{itemize}[topsep=2pt,itemsep=2pt]
            \item Selection Bias: Not completely random sampling;
            \item Information Bias: Difference between 'designed' and 'get', e.g. no response;
            \item Confounding: Exist another important variable, while the model actually focuses on a less important variable, or even reverse the causality.
        \end{itemize}
        
            
\end{itemize}
    
\subsubsection{Diagnostics to Residual}\label{SubSecDiagnostics}

\begin{point}
    Residual Reflects the properties of $ \varepsilon  $
\end{point}


\begin{itemize}[topsep=2pt,itemsep=2pt]
    \item \textbf{Linearity} : use Residual Plot/AV Plot to Reflect the linearity and variance assumption.
    
\begin{rcode}
\begin{lstlisting}[language=R]
lmfit <- lm(y~x,df)
scatter(df$x,lmfit$residuals)
abline(h=0)

library(car)
avPlots(lmfit)
\end{lstlisting}

\end{rcode}
    \item The Assumption of \textbf{Equal Variances} / Homoskedasticity (齐方差性)\index{Homoskedasticity}: 
    \begin{itemize}[topsep=2pt,itemsep=2pt]
        \item \hyperlink{AVPlot}{AV Plot} , e.g. test the $ R^2 $ of $ (e_Y|X_{(\wedge i)} )$-$( e_{X_i}|X_{(\wedge i)}) $ relation.
        \item Bartlett's test:\index{Bartlett's test}
        
        Idea: divide the sample into groups $ g $, and get each MSE
        \begin{equation}
             \mathrm{MSE}_g=\dfrac{1}{n_g}\sum_{i=1}^{n_g}(Y_{gi}-\hat{Y}_g)^2
        \end{equation}
        
        and take statistic
        \begin{equation}
            S=-\dfrac{(N-g)\ln\left[ \sum\limits_g \dfrac{n_g}{N-n_g}\mathrm{MSE}_g \right]-\sum\limits_{g}(n_g-1)\ln \dfrac{n_g}{N-n_g}\mathrm{MSE}_g }{1+\dfrac{1}{3(G-1)}\sum\limits_g\left( \dfrac{1}{n_g-1}-\dfrac{1}{N-G} \right)} \sim \chi^2
        \end{equation}

        to conduct test. 

        Note: \textbf{sensitive}  to normal assumption, not robust. Used when normal assumption is satisfied.
        \item Levene's test: \index{Levene's Test}Divide the sample into $ G $ groups. Denote \textbf{mean}  of residual within each group as $ \tilde{e}_g $, and in each group compute
        \begin{equation}
            d_{ig}=|e_{ig}-\tilde{e}_g| \Rightarrow \bar{d}_{g}=\dfrac{1}{n_g}\sum_{j=1}^{n_g}d_{ig}
        \end{equation}

        Then conduct ANOVA to $ d_{ig} $.

        If $ G=2 $: 2-sample $ t $-test,
        \begin{equation}
            T=\dfrac{\bar{d}_1-\bar{d}_2}{s\sqrt{\frac{1}{n_1}+\frac{1}{n_2}}}\xrightarrow[]{\mathrm{d}} t_{n-2}\qquad s^2=\dfrac{\sum(d_{i1}-\bar{d}_1)^2+\sum(d_{i2}-\bar{d}_2 )^2}{n-2}
        \end{equation}
        

        
        
        \item Brown-Forsythe's Test\index{Brown-Forsythe's Test} (Modified Levene's test): For skewed sample, take the \textbf{mean} as \textbf{median}, more robust. 


        \item[$ {\color{red}\star} $] Breusch-Pagan Test:\index{B-P Test (Breusch-Pagan Test)}
        
        Assume variance of $ \varepsilon _i $ dependent on $ X_i $ as $ m^{\mathrm{th}} $ polynomial:
        \begin{equation}
            \sigma_i^2=\alpha _0+\sum_{k=1}^m\alpha _kX_i^k
        \end{equation}
        
        and test 
        \begin{equation}
            H_0:\alpha _k=0\,\forall k=1,2,\ldots,m\longleftrightarrow H_1 
        \end{equation}

        Method: First conduct OLS to get regression line $ \hat{l}_1 $ and residuals $ e_i $ and SSE, and conduct regression of $ e_i^2 $ over $ X_i $ to get another regression line $ \hat{l}_2 $ and corresponding SSR$ ^* $.

        Then statistic
        \begin{equation}
            S=\dfrac{\mathrm{SSR^*}/2}{(\mathrm{SSE}/n)^2}\xrightarrow[]{\mathrm{d}} \chi^2_m
        \end{equation}
        
    \end{itemize}
\begin{rcode}
    Example for $ G=2 $:
\begin{lstlisting}[language=R]
group <-factor(rep(c(1,2),length.out=length(df$x),
    each=(ceiling(length(df$x)/2))))

bartlett.test(lmfit$residuals~group,group)

library(car)
leveneTest(lmfit$residuals~group,group,center=mean)
leveneTest(lmfit$residuals~group,group,center=median)

library(lmtest)
bptest(lmfit)
\end{lstlisting}

\end{rcode}

    \item The Assumption of \textbf{Normality} :
    
    In most case we use S-W Test($ n<2000 $) and K-S Test($ n>2000 $):
    \begin{itemize}[topsep=2pt,itemsep=2pt]
        \item QQ-plot of ordered residuals.
        
        \item[$ {\color{red}\star}  $] Shapiro-Wilk Test\index{S-W Test (Shapiro-Wilk Test)} (Most Powerful)\footnote{Detail of S-W Test and K-S Test see \hyperlink{testofnormality}{Test of Normality} in \autoref{SubSectionIntroToNonParametricHypothesisTesting}}: To test $ H_0: \,\exists \sigma ^2, \,s.t. \,\varepsilon \sim N_n(0,\sigma ^2I_n)  $, denote 
        \begin{equation}
            m_i=\mathbb{E}(\dfrac{\varepsilon _{(i)} }{\sigma }) 
        \end{equation}

        then under $H_0 $, $ \varepsilon _{(i)}\sim m_i \to $ linear, thus test correlation 
        
        \begin{equation}
            R^2=\dfrac{\left(\sum_{i=1}^n(e_{(i)}-\bar{e})(m_i-\bar{m})\right)^2}{\sum_{i=1}^n(e_{i}-\bar{e})^2\sum_{i=1}^n(m_i-\bar{m})^2}=corr(e_{(i)},m_i) 
        \end{equation}
        
        \item Kolmogorov-Smirnov Test\index{K-S Test (Kolmogorov-Smirnov Test)}: 
        \begin{equation}
            D_n=\sum_{e}|F_n(e)-\Phi(e)|
        \end{equation}
        
        
        \item Cramér-von Mises Test\index{CvM Test (Cramér-von Mises Test)}:
        \begin{equation}
            T=n\int_{-\infty}^\infty (F_n(e)-\Phi (e)) ^2\,\mathrm{d}\Phi(e)
        \end{equation}
        \item Anderson-Darling Test:\index{A-D Test (Anderson-Darling Test)}
        \begin{equation}
            A^2-n\int _{-\infty}^\infty (F_n(e)-\Phi(e))^2\dfrac{1}{\Phi(e)(1-\Phi(e))} \,\mathrm{d}\Phi(e)
        \end{equation}
        \item Jarque-Bera Test \index{JB-test (Jarque-Bera test)}, using skewness $ \hat{g}_1 $ and kurtosis $ \hat{g}_2 $ of $ \vec{e} $
        \begin{equation}
            \mathrm{JB}=\dfrac{n}{6}(\hat{g}_1^2+\dfrac{1}{4}\hat{g}_2^2) \xrightarrow[]{\mathrm{d}} \chi^2_2
        \end{equation}
    \end{itemize}
\begin{rcode}
\begin{lstlisting}[language=R]
qqnorm(lmfit$residuals)
qqline(lmfit$residuals)

qqp <- qqnorm(lmfit$residuals)
cor(qqp$x,qqp$y)

shapiro.test(lmfit$residuals)

ks.test(jitter(lmfit$residuals), pnorm,mean(lmfit$residuals), sd(lmfit$residuals))

library(nortest)
cvm.test(lmfit$residuals)

ad.test(lmfit$residuals)

library(tseries)
jarque.bera.test(lmfit$residuals)
\end{lstlisting}

\end{rcode}


    \item The Assumption of \textbf{Independence} :
    \begin{itemize}
        \item Durbin-Watson Test:  \index{DW Test (Durbin-Watson Test)}
        \begin{equation}
            d=\dfrac{\sum_{j=2}^n(e_j-e_{j-1})^2}{\sum_{j=1}^ne_j^2} 
        \end{equation}
        
        $ d\in (1.5,2.5) $ is fine.
        \item Ljung-Box Test:\index{Ljung-Box Test}
        
        \begin{equation}
            Q=n(n+2)\sum_{k=1}^n\dfrac{\hat{\rho}_k^2}{n-k} 
        \end{equation}
        
        
    \end{itemize}

\begin{rcode}
\begin{lstlisting}[language=R]
dwtest(lmfit)
\end{lstlisting}

\end{rcode}   
        
\end{itemize}














    
        
        


\subsubsection{Diagnostics to Influentials}\label{SubSubSectionDiagnosticsToInfluentials}
    An intuitive explanation to extreme values:
    \begin{itemize}[topsep=2pt,itemsep=2pt]
        \item Outliers: Extreme case for $ Y $;
        \item High Leverage: Extreme case for $ X $;
        \item Influentials: Cases that influence the regression line.
    \end{itemize}

    \begin{point}
        Influentials = Outliers $ \cap $ High Leverage
    \end{point}
    

    In \autoref{SubSectionMultivariateLinearRegressionModel}, we got the $ \hat{\beta}  $ as $\hat{ \beta} = (X'X)^{-1}X'Y=HY $ and got $ \hat{Y} $ as 
    \begin{equation}
        \hat{Y}= X\hat{\beta }=X(X'X)^{-1}X'Y=\hat{H}Y
    \end{equation}
    where hat matrix $ H\equiv  X(X'X)^{-1}X'=\dfrac{\partial^{} \hat{Y}}{\partial Y^{}}$

    Also we got statistical inference to $ \beta ,\sigma ^2, e $
    \begin{align}
        &\hat{\beta }=(X'X)^{-1}X'Y\sim N(\beta ,\sigma ^2(X'X)^{-1})\\
        &e=Y-\hat{Y}=(I-H)Y \sim N(0,\sigma ^2(I-H))\\
        &\hat{\sigma }^2=\mathrm{MSE}=\dfrac{e'e}{n-p-1}=\dfrac{Y'(I-H)Y}{n-p-1}\\
        &\dfrac{(n-p-1)\hat{\sigma ^2}}{\sigma ^2}\sim  \chi^2_{n-p-1} 
    \end{align}
 
    The diagonal elements  of $ \hat{H} $ is self-sensitivity $ h_{ii} $
    \begin{equation}
        h_{ii}=X_i'(X'X)^{-1}X_i
    \end{equation}
    
    



\begin{point}
    Some refined residuals to help conduct Diagnostics:
\end{point}
    
\begin{itemize}[topsep=2pt,itemsep=2pt]
    \item Standardized Residual:\index{Standardized Residual}
    \begin{equation}
         e_{\mathrm{sd}i}=\dfrac{e_i}{\sigma _{e_i}}=\dfrac{e_i}{\sigma \sqrt{1-h_{ii}}}
    \end{equation}
    \item (Internally) Studentized Residual: replace $ \sigma  $ with $ s=\hat{\sigma } $\index{Studentized Residual}\index{Internally Studentized Residual}
    \begin{equation}\label{EqaInternalStudentizedResidual}
         r_i=\dfrac{e_i}{\hat{\sigma }\sqrt{1-h_{ii}}}=\dfrac{e_i}{\sqrt{\mathrm{MSE} }\sqrt{1-h_{ii}}}\sim t_{n-p-1}
    \end{equation}
    
    \item Deleted Residual\index{Deleted Residual}:\footnote{
        \textit{Proof:}

        Lemma: $ (A+B)^{-1}=A^{-1}-\dfrac{1}{1+tr(BA^{-1})}A^{-1}BA^{-1} $, where $ \mathrm{rk}(B)=1  $.

        

        \begin{equation}
            \hat{\beta }_{(\wedge i)}=(X_{(\wedge i)}'X_{(\wedge i)})^{-1}X_{(\wedge i)}'Y_{(\wedge i)} 
        \end{equation}
        
        Using the above lemma: (here for aesthetic purpose, treat $ X_i $ as row vector)
        \begin{align*}
            (X_{(\wedge i)}'X_{(\wedge i)})^{-1}=&(X'X-X_i'X_i)^{-1}\\
            =&(X'X)^{-1}+\dfrac{1}{1-tr[X_i'X_i(X'X)^{-1}]}(X'X)^{-1}X_i'X_i(X'X)^{-1}\\
            =&(X'X)^{-1}+\dfrac{1}{1-h_{ii}}(X'X)^{-1}X_i'X_i(X'X)^{-1}\\
            X_{(\wedge i)}Y_{(\wedge i)}=&X'Y-X_i'Y_i
        \end{align*}

        then calaulate $ \hat{\beta }_{(\wedge i)} $:
   

        \begin{align}\label{EqaProofOfDeleteedResidual}
            \hat{\beta }_{(\wedge i)}=&(X_{(\wedge i)}'X_{(\wedge i)})^{-1}X_{(\wedge i)}'Y_{(\wedge i)}\notag \\
            =&\left[(X'X)^{-1}+\dfrac{(X'X)^{-1}X_i'X_i(X'X)^{-1}}{1-h_{ii}}\right]\left(X'Y-X_i'Y_i\right)\notag\\
            =&\hat{\beta }+\dfrac{(X'X)^{-1}X_i'{\color{red}X_i(X'X)^{-1}X'Y}}{1-h_{ii}}-(X'X)^{-1}X_i'Y_i-\dfrac{(X'X)^{-1}X_i'{\color{red}X_i(X'X)^{-1}X_i'}Y_i}{1-h_{ii}}\notag\\
            =&\hat{\beta }+\dfrac{(X'X)^{-1}X_i'{\color{red}\hat{Y}_i}}{1-h_{ii}}-\dfrac{(X'X)^{-1}X_i'Y_i(1-h_{ii})}{1-h_{ii}}-\dfrac{(X'X)^{-1}X_i'Y_i}{1-h_{ii}}{\color{red}h_{ii}}\notag\\
            =&\hat{\beta }+\dfrac{(X'X)^{-1}X_i'}{1-h_{ii}}(\hat{Y}_i-Y_i)\notag\\
            \Rightarrow &\hat{\beta }-\hat{\beta }_{(\wedge i)}=(X'X)^{-1}X_i'\dfrac{e_i}{1-h_{ii}}
        \end{align}
                    

        Then 
            \begin{align*}
            Y_i-\hat{Y}_{i(\wedge i)}=&Y_i-\hat{Y}_i+\hat{Y}_i-\hat{Y}_{i(\wedge i)}\\
            =&e_i+X_i(\hat{\beta }-\hat{\beta }_{(\wedge i)})\\
            =&e_i+X_i(X'X)^{-1}X_i'\dfrac{e_i}{1-h_{ii}}\\
            =&\dfrac{e_i}{1-h_{ii}} 
        \end{align*}

    }


    \begin{equation}\label{EqaEstimatorWithWedgeX}
        d_i=Y_i-\hat{Y}_{i(\wedge i)} =\dfrac{e_i}{1-h_{ii}}
    \end{equation}

    where $ \hat{Y}_{i(\wedge i)} $ is predicted $ Y $ value at $ X_i $ obtained from the regression of dataset with the $ i^\mathrm{} $ case $ (X_i,Y_i) $ removed:
    \begin{equation}
         \hat{\beta} _{(\wedge i)}=(X_{(\wedge i)}'X_{(\wedge i)})^{-1}X_{(\wedge i)}'Y_{(\wedge i)}\qquad \hat{Y}_{i(\wedge i)}=X_i'\hat{\beta} _{(\wedge i)}
    \end{equation}
        
    \item (Externally) Studentized Residual\index{Externally Studentized Residual}: To avoid self-influence, take deleted residual in \autoref{EqaInternalStudentizedResidual}
    
    % Delete the $ i^{\mathrm{th}} $ case and conduct regression to the $ n-1 $ sample cases, denote the regression parameter as 
    % \begin{equation}
    %     \hat{\beta }_{(\wedge i)}=(\hat{\beta }_{1(\wedge i)},\hat{\beta }_{0(\wedge i)} )
    % \end{equation}
    \begin{equation}
         t_i=\dfrac{d_i}{s^2(d_i)}=\dfrac{e_i}{\hat{\sigma} _{(\wedge i)}\sqrt{1-h_{ii}}}= \dfrac{e_i}{\sqrt{\mathrm{MSE}_{(\wedge i)} }\sqrt{1-h_{ii}}}\sim t_{n-p-2}
    \end{equation}

    Relation between $ \mathrm{MSE}  $ and $ \mathrm{MSE}_{(\wedge i)}  $:
    \begin{equation}
        (n-p-1)\mathrm{MSE}=(n-p-2)\mathrm{MSE}_{(\wedge i)}+\dfrac{e_i^2}{1-h_{ii}}   
    \end{equation}

    which also gives the relation between $ t_i $ and $ r_i $:
    \begin{align*}
        t_i=r_i\left(\dfrac{n-p-2}{n-p-1-r_i^2}\right)^{1/2}\Leftrightarrow r_i=t_i\left(\dfrac{n-p-1}{n-p-2+t_i^2}\right)^{1/2} 
    \end{align*}
    
    
    
    
    
\end{itemize}


\begin{itemize}[topsep=2pt,itemsep=0pt]
    \item Diagnostics to \textbf{Outlier}: use external studentized residual for $ t $-test with Bonferroni adjustment. Declare the $ i^\mathrm{th} $ case an outlier if: 
    \begin{equation}
         |t_i|>t_{\alpha/2n,n-p-2}
    \end{equation}
    \item Diagnostics to \textbf{Leverage}: use hat matrix $ H $/self-sensitivity $ h_{ii} $.
    \begin{equation}
        \sum_{i=1}^n h_{ii}=tr(H)=p+1 \Rightarrow \bar{h}=\dfrac{p+1}{n} 
    \end{equation}
    
    Declare the $ i^\mathrm{th}  $ case a {leverage} if:
    
    \begin{equation}
        h_{ii}>\kappa \bar{h}=\kappa \dfrac{p+1}{n} 
    \end{equation}
    
    where usually take $ \kappa =2 $ or $ 3 $.
    \item Diagnostics to \textbf{Influential}: Studentized DIFference caused to FITted values (DIFFITS)\index{DIFFITS (Studentized Difference caused to Fitted values)}
    
    DIFFIT:
    \begin{equation}
        \mathrm{DIFFIT}_i=\hat{Y}_i-\hat{Y}_{i(\wedge i)}=e_i\dfrac{h_{ii}}{1-h_{ii}} 
    \end{equation}

    DIFFITS:
    
    \begin{equation}
        \mathrm{DIFFITS}_i=\dfrac{\mathrm{DIFFIT}_i}{s(\hat{Y}_i)}=t_i\sqrt{\dfrac{h_{ii}}{1-h_{ii}}}
    \end{equation}
    
    Declare the $ i^\mathrm{th}  $ case an {influential} if:
    
    \begin{equation}
        \begin{cases}
            \mathrm{DIFFITS}_i>1&\text{small/medium data} \\
            \mathrm{DIFFITS}_i>2\sqrt{\dfrac{p+1}{n}}&\text{large data}
        \end{cases}   
    \end{equation}
    
    \item Diagnostics to \textbf{Influential}: Cook's Distance\index{Cook's Distance}, by quantifying the `influence' to $ \hat{\beta } $.
    
    Using \autoref{EqaDistributionOfMultiVariateBeta}(\autoref{EqaDistributionOfMultiVariateSigma}) we could construct the following Cook's Distance\footnote{Proof uses {EqaProofOfDeleteedResidual}.}
    \begin{equation}
         D_i=\dfrac{\left\Vert X(\hat{\beta} -\hat{\beta }_{(\wedge 1)})\right\Vert ^2}{(p+1)\hat{\sigma }^2}=\dfrac{e_i^2}{(p+1)\hat{\sigma }^2}\dfrac{h_{ii}}{(1-h_{ii})^2}\qquad \dfrac{1-h_{ii}}{h_{ii}}D_i\sim F_{p+1,n-p-1}
    \end{equation}
    
    Comment:
    \begin{equation}
        D_i=\dfrac{e_i^2}{(p+1)\hat{\sigma }^2}\left[ \dfrac{h_{ii}}{(1-h_{ii})^2} \right]=\dfrac{1}{p+1}\dfrac{h_{ii}}{1-h_{ii}}\times r_i^2
    \end{equation}

   where $ \dfrac{1}{p+1}\dfrac{h_{ii}}{1-h_{ii}} $ correponds to hige leverage, and $ r_{i}^2 $ correponds to outliers, multiply to get influentials.

   Declare the $ i^\mathrm{th}  $ case an {influential} if 
   \begin{equation}
       D_i>\dfrac{4}{n}
   \end{equation}
   
   Or conduct $ F $-test using the distribution of $ D_i $, with $ \alpha \sim 20\% $. 

   \item Diagnostics to \textbf{Influential}: Studentized DiFference in BETA estimates (DFBETAS)\index{DFBETAS (Studentized Difference in Beta Estimates)}. Use \autoref{EqaDistributionOfMultiVariateBeta}, define
   \begin{equation}
       var(\hat{\beta }_k)=\sigma ^2(X'X)^{-1}_{kk}:=\sigma ^2c_{kk} 
   \end{equation}
   
   And studentize difference in $ \hat{\beta } $ with $ i^\mathrm{th} $ case removed: $ \hat{\beta }_k-\hat{\beta }_{k(\wedge i)} $
   \begin{equation}
       \mathrm{DFBETAS}_{k(\wedge i)}=\dfrac{\hat{\beta }_k-\hat{\beta }_{k(\wedge i)}}{\sqrt{\mathrm{MSE}_{(\wedge i)}c_{kk} }},\,k=1,2,\ldots, p
   \end{equation}
   
   Declare the $ i^\mathrm{th}  $ case an {influential} if 
   \begin{equation}
    \begin{cases}
        \mathrm{DFBETAS}_i>1&\text{small/medium data} \\
        \mathrm{DFBETAS}_i>\dfrac{2}{\sqrt{n}}&\text{large data}
    \end{cases}   
   \end{equation}
   
   
   
   

\end{itemize}

    
 


\begin{rcode}
\begin{lstlisting}[language=R]
rstudent(lmfit)
library(car)
outlierTest(lmfit)

hatvalues(lmfit)

cooks.distance(lmfit)
plot(lmfit,which=4)

dfbetas(lmfit)
\end{lstlisting}
\end{rcode}

















Leverage and Mahalanobis Distance:
    
Mahalanobis Distance between $ X $ and $ Y $ as defined in \autoref{MahalanobisDistance}
\begin{equation}
     d_M(\vec{x})=\sqrt{(\vec{x}-\vec{\mu})^TS ^{-1}(\vec{x}-\vec{\mu})} 
\end{equation}

And we can proof $ d_M $ of a case item $ X_{ i\cdot}=(1,X_{i1},X_{i2},\ldots,X_{ip}) $ is\footnote{Proof hint: use lemma 
\begin{align*}
    (A+B)^{-1}=A^{-1}-\dfrac{A^{-1}BA^{-1}}{1+tr(B^{-1}A)},\quad \mathrm{rank}(B)=1 
\end{align*}
and note that $ X_{:,1}=\mathbf{1}_n $
}
\begin{equation}
    d_{M}^2(X_{i\cdot})=(n-1)(h_{ii}-\dfrac{1}{n}) 
\end{equation}
here $ S=\mathop{S}\limits_{(p+1)\times (p+1)}  $. Note that $  \mathrm{L.H.S.}\geq 0$, thus it's also an evidence that $ h_{ii}\geq \dfrac{1}{n} $

\subsubsection{Extra Sum Of Square}\label{SubSubSectionExtraSumOfSquare}
    Def. Extra SS: the part of SSE explained by a new $ X_2 $ when adding to model $ Y\sim X_1 $:
    \begin{equation}
        \mathrm{SSR}(X_2|X_1)=\mathrm{SSE}(X_1)-\mathrm{SSE}(X_1,X_2)=\mathrm{SSR}(X_1,X_2)-\mathrm{SSR}(X_1)  
    \end{equation}

    where $ \mathrm{SS}(\cdot)  $ represents the $ \mathrm{SS} $ when the model contains variable $ \cdot $.\footnote{$ \mathrm{SSE}(1)=\mathrm{SST} $, where $ 1 $ correponds to intercept.}
    
    
    (The following part use model $ (Y,X_1,X_2) $ as example.)


    We could use extra SS to examine the proper regression model: examine the \lstinline|F value| and \lstinline|Pr(>F)| in the output.
\begin{rcode}
\begin{lstlisting}[language=R]
lm(y~x1+x2+x1:x2) %>% anova
\end{lstlisting}
\end{rcode}

    \textbf{Note}: Three types of SS 
\begin{table}[H]
    \centering
    \renewcommand\arraystretch{1.15}
    \begin{tabular}{c|ccc}
        \hline
        Term&Type I SS\footnote{Type I SS is order specified.}&Type II SS&Type III SS\\\hline
        $ X_1 $&$ \mathrm{SSR}(X_1) $&$ \mathrm{SSR}(X_1|X_2)  $&$ \mathrm{SSR}(X_1|X_2,X_1X_2)  $\\
        $ X_2 $&$ \mathrm{SSR}(X_2|X_1) $&$ \mathrm{SSR}(X_2|X_1)  $&$ \mathrm{SSR}(X_2|X_1,X_1X_2)  $\\
        $ X_1X_2 $&$ \mathrm{SSR}(X_1X_2|X_1,X_2) $&Assume no interaction term &$ \mathrm{SSR}(X_1X_2|X_1,X_2)  $\\\hline
        Language.Function&\lstinline|R.anova|&\lstinline|python.|&\lstinline|SPSS,SAS,R.lm|\\
        \hline
    \end{tabular}
\end{table}

    To get Type II and III anova, use \lstinline|Anova(lmfit,type='III')| in \lstinline|'car'| package.

    \textbf{Hierarchical Principle}: the interaction term $ X_1X_2 $ should always come in \textbf{after}  marginal term $ X_1 $ and $ X_2 $.  \index{Hierarchical Principle}

\begin{rcode}
\begin{lstlisting}[language=R]
libaray('car')
Anova(lmfit,type='II')
Anova(lmfit,type='III')
\end{lstlisting}
\end{rcode}






\subsubsection{Hypotheses Testing to Slope}
    Main focus: whether the linear relation exist:
\begin{equation}
    H_0:\beta _1=\beta _2=\ldots=\beta _p=0\longleftrightarrow H_1:\exists \beta _i\neq 0,\, i=1,2,\ldots,p
\end{equation}

    As for general case $ H_0:\mathop{C}\limits_{q\times (p+1)} \beta -\mathop{t}\limits_{(p+1)\times 1}  = 0$, use \hyperlink{HyperlinkGLT}{General Linear Test $ F $}.

\begin{itemize}[topsep=2pt,itemsep=2pt]
\item ANOVA $ F $-Test:\index{ANOVA $ F $-test}
    
    We can examine  
    \begin{equation}
        F=\dfrac{\mathrm{MSR}}{\mathrm{MSE}}\sim F_{p,n-p-1} 
    \end{equation}
    
\item General Linear Test (GLT)\index{GLT (General Linear Test)}
    
    First we introduce the examine models:
    \begin{itemize}[topsep=2pt,itemsep=2pt]
        \item Full model: Include all variable/parameters to be examined, with $ p $ variables.
        \begin{equation}
            Y=X\beta +\varepsilon
        \end{equation}

        And define $ \mathrm{SSE}_\mathrm{F} $ with $ dof_\mathrm{F}=n-p-1 $ under Full Model.
        \item Reduced model: Apply the Null Hypothesis to Full Model, with $ \tilde{p} $ variables
        \begin{equation}
         Y_i=\tilde{X}\tilde{\beta }+\varepsilon 
        \end{equation}
        
        And define $ \mathrm{SSE}_\mathrm{R} $ with $ dof_\mathrm{R}=n-\tilde{p}-1 $ under Reduced Model.
    \end{itemize}

    Then conduct test to the difference between Full model and Reduced model through $ \mathrm{SSE}_\mathrm{F}  $ and $ \mathrm{SSE}_\mathrm{R}  $.

    \begin{itemize}[topsep=2pt,itemsep=2pt]
        \item One dimensional case: $ H_0:\beta _1=0 $
        
        Examine
        \begin{equation}
            F=\dfrac{(\mathrm{SSE_R-SSE_F})/(dof_\mathrm{R}-dof_\mathrm{F} )}{\mathrm{SSE_F}/dof_F} \sim F_{1,n-2}
        \end{equation}
\begin{rcode}
\begin{lstlisting}[language=R]
fullmodel <- lmfit
nullmodel <- lm(y ~ 1,df)
anova(nullmodel,fullmodel)
\end{lstlisting}
\end{rcode}

        \item \hypertarget{HyperlinkGLT}{General case:} Test $ H_0: \mathop{C}\limits_{q\times (p+1)} \beta -\mathop{t}\limits_{(p+1)\times 1} =0 $, construct $ F $ statistics as
        \begin{equation}
            F=\dfrac{ (C\hat{\beta }-t)'\left[C(X'X)^{-1}C'\right]^{-1}(C\hat{\beta }-t)  }{q\hat{\sigma }^2 }\sim F_{q,n-q-1  } 
        \end{equation}
        
        
        % take $ Y\sim X_1+X_2+X_3+X_4+X_5 $ as example
        % \begin{equation}
        %     H_0:\beta _4=\beta _5=0  \longleftrightarrow H_1: \beta _4\neq 0 \text{ or }\beta _5\neq 0
        % \end{equation}
        
        % Examine
        % \begin{equation}
        %     F=\dfrac{(\mathrm{SSE_R-SSE_F})/(dof_\mathrm{R}-dof_\mathrm{F} )}{\mathrm{SSE_F}/dof_F} \sim F_{p-\tilde{p},n-p-1}
        % \end{equation}

        % Note: here $ \mathrm{SSE_R-SSE_F}=\mathrm{SSR}(X_4,X_4|X_1,X_2,X_3)$.

    





    \end{itemize}
    
        






\item $ r $ and Different $ R^2 $:
\begin{itemize}[topsep=2pt,itemsep=0pt]
    \item Pearson's $ r $:\index{Pearson's Correlation Coefficient}\index{Correlation Coefficient!Pearson's Correlation Coefficient}
    \begin{equation}
        r_{Y,\hat{Y}}=\hat{cov}(Y,\hat{Y})=\dfrac{\sum\limits_{i=1}^n(Y_i-\bar{Y})(\hat{Y}_i-\bar{Y})}{\sqrt{\sum\limits_{i=1}^n(Y_i-\bar{Y})^2}\sqrt{\sum\limits_{i=1}^n(\hat{Y}_i-\bar{Y})^2}}=\sqrt{\dfrac{\sum\limits_{i=1}^n(\hat{Y}-\bar{Y})^2}{\sum\limits_{i=1}^n(Y_i-\bar{Y})^2}}
    \end{equation}
    \item Coefficient of Multiple Determination $ R^2 $:\index{Correlation Coefficient!Coefficient of Multiple Determination $ R^2 $}\index{CMD $ R^2 $ (Coefficient of Multiple Determination)}

    \begin{equation}
        R^2=\dfrac{\mathrm{SSR}}{\mathrm{SST}}=1-\dfrac{\mathrm{SSE}}{\mathrm{SST}}
    \end{equation}
    \item Adjusted $ R^2 $:\index{Correlation Coefficient!Adjusted $ R^2 $}
    \begin{equation}
        R^2_\mathrm{a}=1-\dfrac{\mathrm{MSE}}{\mathrm{MST}} =1-\dfrac{n-1}{n-p-1}\dfrac{\mathrm{SSE}}{\mathrm{SST}}
    \end{equation}
\end{itemize}

    Relation between $ r $ and $ R^2 $: Under Simple Linear Model, we have 
        \begin{equation}
            R^2=r^2 
        \end{equation}
    
    Relation between $ R^2 $ and $ F $-Statistic:
        \begin{equation}
            F=\dfrac{R^2}{1-R^2}\dfrac{n-p-1}{n-1} \sim F_{n-1,n-p-1}
        \end{equation}

    Hypothesis testing for $ r $: 
    \begin{equation}
        t=\dfrac{r}{\sqrt{1-r^2}}\sqrt{n-2}\sim t_{n-p-1} 
    \end{equation}

\begin{rcode}
\begin{lstlisting}[language=R]
cor.text(df$x,df$y)
\end{lstlisting}
\end{rcode}
    


\item Coefficient of Partial Determination $ R^2_{Yk|\wedge X_k} $ and Coefficient of Multiple Determination $ R^2 $:
\index{Correlation Coefficient!Coefficient of Partial Determination $ R^2_{Yk|\wedge X_k} $}
    CMD reflects the interpretability of the model, to examine the interpretability of each variable, use coef. partial determination

\begin{equation}
    R^2_{YX_k|X_1,\ldots,X_{k-1},X_{k+1},\ldots,X_p}=R^2_{YX_k|\wedge X_k}=\dfrac{\mathrm{SSR}(X_k|X_1,\ldots,X_{k-1},X_{k+1},\ldots,X_p) }{\mathrm{SSE}(X_1,\ldots,X_{k-1},X_{k+1},\ldots,X_p) }=\dfrac{\mathrm{SSR}(X_k|\wedge X_k) }{\mathrm{SSE}(\wedge X_k) }
\end{equation}

    Note: Coef. Partial determination can also be used for $ X_i,X_j $: $ R^2_{X_iX_j|\wedge X_i,X_j} $

    Sometimes we use $ \eta^2_k=R^2_{YX_k|\wedge k}=R^2_{Yk.\wedge k} $

    \item Coefficient of Partial Correlation $ \eta_k $: Measures the strength of linear relation, $ \pm $ sign depend on posi./nega. correction.\index{Correlation Coefficient!Coefficient of Partial Correlation $ \eta_k $}
    \begin{equation}
        \eta_k=\pm \sqrt{\eta^2_k} 
    \end{equation}
    
    
\begin{rcode}
\begin{lstlisting}[language=R]
library('heplots')
etasq(lmfit)
\end{lstlisting}

\end{rcode}

\end{itemize}











\subsubsection{Diagnostics to Multi-collinearity}\label{SubSubSectionDiagnosticsToMultiCollinearity}
    
\begin{itemize}[topsep=2pt,itemsep=2pt]
    \item Venn Diagram for Multi-Linear Regression: Used to show the interpretability of variables.\index{Venn Diagram}
    
    \begin{center}
            \begin{tikzpicture}
        \begin{scope}[blend group = soft light]
          \fill[red!30!white]   ( 90:1.2) circle (2);
          \fill[green!30!white] (210:1.2) circle (2);
          \fill[blue!30!white]  (330:1.2) circle (2);
        \end{scope}
        \node at ( 90:3.5)    {$ Y $};
        \node at ( 210:3.5)   {$ X_1 $};
        \node at ( 330:3.5)   {$ X_2 $};
        \node at ( 30:1.5) {1};
        \node at ( 150:1.5) {3};
        \node at ( 90:2) {2};
        \node at ( 210:2) {4};
        \node   {5};
      \end{tikzpicture}
    \end{center}

    Explanation of each region:
    \begin{itemize}[topsep=2pt,itemsep=2pt]
        \item 1/3: Variation in $ Y $ uniquely attributes to $ X_2 $/$ X_1 $;
        \item 2: Variation in $ Y $ that cannot be explained by regression to $ X_1,X_2 $, corresponds to $ \varepsilon  $;
        \item 5: Cross term of $ X_1,X_2 $, \textbf{cannot} verify the orientation, corresponds to \textbf{Multi-colinearity}.
    \end{itemize}

    In the presence of multi-colinearity, i.e. $ X $ is column singular ($ \dfrac{S_5}{S_1\text{ or }S_3} $ large), the regression parameter
    \begin{equation}
        \hat{\beta }=(X'X)^{-1}X'Y
    \end{equation}

    
    
   
\end{itemize}

     Issue of multi-collinearity: 
    \begin{itemize}[topsep=2pt,itemsep=2pt]
        \item Statistically:`better' prediction, worse interpretability;
        \item Numerically: Calculation of $ (X'X)^{-1} $ becomes unstable/ill-posed/NAN.
    \end{itemize}

\begin{point}
    Use Variance Inflation Factor (VIF)\index{VIF (Variance Inflation Factor)} to detect multi-collinearity.
\end{point}    

    First construct $ R^2_k $, $ k=1,2,\ldots p $: Regress $ X_k $ against other $ p-1 $ variable $ X_i $s and get corresponding $ R^2_k $, and 

    \begin{align}
        \mathrm{VIF}_k=&(1-R^2_k)^{-1}\\
        \overline{\mathrm{VIF}} =&\dfrac{1}{p}\sum_{k=1}^p\mathrm{VIF}_k 
    \end{align}

    
    If $ \Vert \mathrm{VIF}_i\Vert _{\infty}>10 $ or $ \overline{\mathrm{VIF}}> 1 $, then we identify an excessive multi-colinearity.\footnote{Why $ \mathrm{VIF}_k=\dfrac{1}{R_k^2}  $ is called `variance inflation factor'? We can prove that 
\begin{equation}
    var(\hat{\beta }_k)=\dfrac{\sigma ^2}{(n-1)S_{x_k}^2}\cdot \dfrac{1}{R_k^2}= \dfrac{\sigma ^2}{(n-1)S_{x_k}^2}\cdot \mathrm{VIF}_k
\end{equation}

}

\begin{rcode}
\begin{lstlisting}[language=R]
library('car')
vif(lmfit)
\end{lstlisting}
\end{rcode}
    
    

    
\subsubsection{Diagnostics to Model Variable Selection}

        In Multi-variate regression, proper explanatory variables form a subset of all available variables.

        Aim: Avoid over-fitting, get a simple explanatory model.

        Comment: If we consider the model with all $ p_\mathrm{max} $ variables as full, unbiased model, then model selection is a kind of \textbf{Bias-Variance Trade-Off}. 

\begin{point}
    Model Validation: $ k $-Fold Cross Validation(CV):\index{CV ($ k $-Fold Cross Validation)} 
    \begin{enumerate}[topsep=2pt,itemsep=2pt]
        \item Separate the dataset size $ n $ into $ k $ parts;
        \item pick the $ i^\mathrm{th} $ part as test set $ y_i $, and the other $ k-1 $ part as train set $ y_{\wedge i} $(to conduct regression, etc); then conduct prediction of model $ y_{\wedge i} $ to part $ y_i $ ane get $ \mathrm{MSE}_i $;
        \item Take average of $ \mathrm{MSE}_i $ as the measure of validity.
    \end{enumerate}
\end{point}




\begin{point}
    \hypertarget{ModelValidationCriteria}{Evaluation Criteria}
\end{point}

     Useful model validation approach: To check a model with $ p-1 $ variable (this part $ p $ for 1+\# variable)
        \begin{itemize}[topsep=2pt,itemsep=2pt]
        \item Traditional way: Test $ r $, $ R^2 $, $ R_a^2 $, $ p $-value, etc.
% Mallow's Cp, AIC,SBC/BIC,PRESS,
        \item Mallow's $ C_p $: For a model with $ p $ variable: \index{Mallow's $ C_p $}
        \begin{equation}
            \hat{Y}^p=X_p(X_p'X_p)^{-1}X'_pY=H_pY 
        \end{equation}
        
        Denote:
        \begin{equation}
            \mathbb{E}(\hat{Y}^p)=H_pE(Y)\equiv H_p\mu \qquad var(\hat{Y}^p)=H_p\sigma ^2I_nH_p'=\sigma ^2H_p
        \end{equation}

        Recall the MSE expansion of bias-variance trade-off in \autoref{EqaMSEExpansion}\footnote{Derivtion:
        \begin{itemize}[topsep=2pt,itemsep=0pt]
            \item Bias part: (Here use \autoref{EqaExpectationOfQuadric} in $ 3^\mathrm{rd} $ line; use \autoref{EqaExpectationOfSSE} in $ 4^\mathrm{th} $ line.)
            \begin{align*}
                \sum_{i=1}^n[e(\hat{Y}_i^p-\mu _i)]^2=&\mu'(H_p-I)'(H_p-I)\mu\\
                =&\mu (I-H_p)\mu '\\
                =&E(Y'(I-H_p)Y)-tr[(I-H_p)\sigma ^2]\\
                =&E(\mathrm{SSE}(p))-(n-p)\sigma ^2
            \end{align*}
            \item Variance part:
            \begin{align*}
                \sum_{i=1}^nvar(\hat{Y}_i^p)=& tr(var(\hat{Y}^p))=\sigma ^2tr(H_p)=p\sigma ^2
            \end{align*}
        \end{itemize}

        Then
        \begin{equation}
             \dfrac{\sum_{i=1}^n \mathbb{E}[(\hat{Y}_i^p-\mu_i)^2]}{\sigma ^2}=\dfrac{\mathbb{E}(\mathrm{SSE}(p) )}{\sigma ^2}-(n-2p)
        \end{equation}
            }

        \begin{align*}
            \sum_{i=1}^n \mathbb{E}[(\hat{Y}_i^p-\mu_i)^2]=&\sum_{i=1}^n\mathbb{E}(\hat{Y}^p_i)-\mu_i]^2+\sum_{i=1}^nvar(\hat{Y}_i^p)\\
            \Rightarrow =&\mathbb{E}(\mathrm{SSE}(p) )-(n-2p)\sigma ^2
        \end{align*}
            
        Sum Squared Prediction Error (SSPE)\index{SSPE (Sum Squared Prediction Error)}:
        \begin{equation}
            \Gamma _0\equiv \dfrac{\sum_{i=1}^n \mathbb{E}[(\hat{Y}_i^p-\mu_i)^2]}{\sigma ^2}=\dfrac{\mathbb{E}(\mathrm{SSE}(p) )}{\sigma ^2}-(n-2p)
        \end{equation}
        
        And construct Mallow's $ C-p $: Estimation of $ \Gamma _p $
        \begin{equation}\label{EqaMallowsCp}
            C_p=\hat{\Gamma }_p=\dfrac{\mathbb{E}(\mathrm{SSE}(p))}{\hat{\sigma }^2}-(n-2p)
        \end{equation}

        where $ \mathrm{SSE(p)}=Y'(I-H_p)Y  $.
        
        When the model is unbiased, then $ \mathbb{E}(\mathrm{SSE} (p))\to n-p $, use $ C_p$-$p $ plot to pick proper $ p $: 
        \begin{itemize}[topsep=2pt,itemsep=0pt]
            \item $ C_p\approx p $: Model unbiased, then choose model with smaller $ C_p $;
            \item $ C_p\gg p $: Significant biased, miss some important predictors;
            \item $ C_p\ll p $: Overfitting.
        \end{itemize}
        

        \item Akaike Information Criterion (AIC):\index{AIC (Akaike Information Criterion)} Euivalent to Mallow's $ C_p $ for gaussion regression model.
        \begin{equation}
            \mathrm{AIC}(p)=-2\log(\hat{L})+2p 
        \end{equation}

        where $ \hat{L} $ is the maximum likelihood, for linear regression case 
        
        \begin{equation}
            \mathrm{AIC}(p)=n\log\left(\dfrac{\mathrm{SSE}(p) }{n}\right)+2p 
        \end{equation}

        Select the model that minimizes $ \mathrm{AIC}(p) $.
        \item Bayesian Information Criterion (BIC)/Schwarz's Bayesian Criterion (SBC):\index{BIC (Bayesian Information Criterion)}\index{SBC (Schwarz's Bayesian Criterion)}
        
        \begin{equation}
            \mathrm{BIC}(p)=-2\log(\hat{L})+p\log n 
        \end{equation}
        
        where $ \hat{L} $ is the maximum likelihood, for linear regression case 
        
        \begin{equation}
            \mathrm{BIC}(p)=n\log\left(\dfrac{\mathrm{SSE}(p) }{n}\right)+p\log n
        \end{equation}
        Select the model that minimizes $ \mathrm{BIC}(p) $.
        
        \item PRESS Creterion (Predictive Residual Error Sum of Squares)\index{PRESS (Predictive Residual Error Sum of Squares)}: A kind of within-model cross validation
        \begin{equation}
            \mathrm{PRESS}(p)=\sum_{i=1}^n(Y_i-\hat{Y}_{i(\wedge i)})^2 
        \end{equation}
        
        where 
        \begin{align*}
            \hat{Y}_{i(\wedge i)}=&(1,X_{i1},\ldots,X_{ip})\hat{\beta }_{(\wedge i)}\\
            \hat{\beta }_{(\wedge i)}=&(X_{(\wedge i)}'X_{(\wedge i)})^{-1}X_{(\wedge i)}'Y_{(\wedge i)}
        \end{align*}
        
        where $ \hat{\beta }_{(\wedge i)} $ as in {EqaEstimatorWithWedgeX}, is the estimated $ \beta  $ with $ (X_i,Y_i) $ removed from $ X $.\footnote{A useful thm.: Deleted Residual
        \begin{equation}
            d_i:=Y_i-\hat{Y}_{i(\wedge i)}=\dfrac{e_i}{1-h_{ii}} 
        \end{equation}
        
        }
            
        Select the model that minimizes $ \mathrm{PRESS}(p) $.

        % \item Prediction $ R^2 $: $ R_p^2 $\index{Pre $ R^2 $}
        
        % \begin{equation}
        %     R_p^2=1-\dfrac{\mathrm{PRESS} }{\mathrm{SST} } 
        % \end{equation}
        
        
        \end{itemize}
\begin{rcode}
\begin{lstlisting}[language=R]
library('leaps')
predictor <- df[,c('...','...',...)]
response <- df[,...]
leapSet <- leaps(x=predictor, y=response, nbest = ...)
# method=c('Cp','adjr2','r2')
leapSet$which[which.min(leapSet$Cp),]
\end{lstlisting}
    
    \lstinline|nbest| for \lstinline|NUMBER_OF_BEST_MODELS|
\end{rcode}





















\subsection{Remedies}

\subsubsection{Variable Transformation}\label{SubSubSectionVarianceStablizeTransformation}
    The goal of Transformation:
        
        \begin{itemize}[topsep=2pt,itemsep=2pt]
            \item Stablize Variance;
            \item Improve Normality;
            \item Simplify the Model.
        \end{itemize}
    
    \begin{point}
        Transformation Methods:
    \end{point}
    
        \begin{itemize}[topsep=2pt,itemsep=2pt]
            \item Variance Stabilizing Transformations:\index{Variance Stabilizing Transformation}
                For $ E(Y_X)=\mu_X$, $ var(Y_X)=h(\mu_X) $, take transformation $ f(Y) $ such that $ var(f(Y))=\mathrm{const} $, satisfies
                \begin{equation}
                    f(\mu)=\int\dfrac{c\,\mathrm{d}\mu}{\sqrt{h(\mu)}} 
                \end{equation}
    
                Examples:
                \begin{align*}
                    h(\mu)=&\mu^2\Rightarrow f(\mu )=\ln\mu\\
                    h(\mu)=&\mu^{2\nu}\Rightarrow f(\mu )=\mu ^{1-\nu}
                \end{align*}
            
            \item Box-Cox Transformation: Take \index{Box-Cox Transformation}
        \begin{equation}
            Y^*=\dfrac{Y^\lambda -1}{\lambda }
        \end{equation}
    
                Examples:
            \begin{align*}
                \lambda =1&\Rightarrow Y^*\sim Y\\
                \lambda =0.5&\Rightarrow Y^*\sim \sqrt{Y}\\
                \lambda =0&\Rightarrow Y^*\sim \ln Y\\
                \lambda =-1&\Rightarrow Y^*\sim 1/Y
            \end{align*}
        
            And conduct regression to model
            \begin{equation}
                Y^* =\beta _0+\beta _1X+\varepsilon_i 
            \end{equation}
            
            Likelihood Function
            \begin{equation}
                L(\beta ,\sigma ^2;\lambda )=\dfrac{1}{(2\pi\sigma ^2)^{n/2}}\exp\left( -\dfrac{1}{2\sigma ^2}\sum_{i=1}^n (Y_i^*-\beta _0-\beta _1X_i)^2 \right) J(\dfrac{\partial^{} Y^*}{\partial Y^{}})
            \end{equation}
    
            where the Jacobi Matrix denoted in Geometric Mean $ \mathrm{GM}(Y)=\prod_{i=1}^n Y_i^{1/n}$
            \begin{equation}
                J(\dfrac{\partial^{} Y^*}{\partial Y^{}})=\prod_{i=1}^nY_i^{\lambda -1}=\mathrm{GM}(Y)^{n(\lambda -1)}
            \end{equation}
            
            
    
            MLE Estiamtor:
            \begin{align*}
                \hat{\beta }^*&= (X'X)^{-1}X'Y^*\\
                \hat{\sigma }^2_n&=\dfrac{1}{n}\mathrm{SSE}^*\\
                \mathrm{SSE}^*&=\sum_{i=1}^n(Y_i^*-\hat{Y}^*)^2
            \end{align*}
    
            And when $ \beta  $, $ \sigma ^2 $ take MLE estimator, $ L(\beta ,\sigma ^2;\lambda ) $ can be regarded a function of $ \lambda  $:
            \begin{equation}
                \ln L(\beta ,\sigma ^2;\lambda )=l(\lambda )=-\dfrac{n}{2}\ln \dfrac{\hat{\sigma}^2_n}{\mathrm{GM}(Y)^{2(\lambda -1)}}+\mathrm{const}
            \end{equation}
    
            For simplification, denote $ Z=Y^*/J^{1/n} $ and get
            \begin{equation}
                \ell (\lambda )=-n\ln \sigma^2_{n_Z}+\mathrm{const} 
            \end{equation}
            
            where 
    
            \begin{equation}
                Z_i^* =\begin{cases}
                    \dfrac{Y_i^\lambda-1 }{\lambda }\dfrac{1}{\prod\limits_{k=1}^n Y_k^{\frac{\lambda -1}{n}}},&\lambda \neq 0\\
                    \ln Y_i\prod\limits_{k=1}^n Y_k^{\frac{1}{n}},&\lambda =0
                \end{cases}
            \end{equation}
    
            Plot $ l(\lambda ) $-$ \lambda  $ to determine a proper $ \lambda  $ and transform $ Y^*=\dfrac{Y^\lambda -1}{\lambda } $:
            \begin{itemize}[topsep=2pt,itemsep=2pt]
                \item Selected $ \lambda $ should be closed to $ \lambda_{\arg\max l} $, at least within CI\footnote{Here CI can be derived using Wilk's Thm.}
                \begin{equation}
                    \{\lambda |l(\lambda )\geq l(\lambda_{\arg\max l})-\dfrac{1}{2}\chi^2_{1,1-\alpha }\}
                \end{equation}
                \item Should pick a $ \lambda  $ which is \textbf{Interpretable}. e.g. If $ \lambda =1 $ is within range $ [0.94,1.08] $, then take $ \lambda =1 $ (does not transform).
                
                
            \end{itemize}
            
                
            
\begin{rcode}
\begin{lstlisting}[language=R]
library(MASS)
bctrans <- boxcox(y~x,df,lambda = seq(-1.5, 1.5, length = 15))
bctrans$x[which.max(bctrans$y)]
\end{lstlisting}

    \end{rcode}
            
            
        Note: we can transform on $ X $ or $ Y $ or simultaneously to get better regression model.
        
    \end{itemize}

\subsubsection{Weighted Least Squares Regression}
    To deal with heterogeneous variance, use Weighted Least Squares (WLS)\index{WLS (Weighted Least Squares)} instead of OLS: Minimizing
    \begin{equation}
        \sum_{i=1}^ne_i^2\longrightarrow \sum_{i=1}^nw_ie^2_i         
    \end{equation}
    
    And e.g. take weight for each case as 
    \begin{equation}
        w_i=\dfrac{1}{\sigma _i^2} 
    \end{equation}

    Solution:
    \begin{equation}
        \hat{\beta }_W=(X'WX)^{-1}X'WY 
    \end{equation}
    
    
\begin{rcode}
\begin{lstlisting}[language=R]
Wlmfit <- lm(y~x,weights=WEIGHT_VECTOR,data=df)
\end{lstlisting}
\end{rcode}
    
    
    







\subsubsection{Remedies for Model Variable Selection \& More Regression Model}
    Several Algorithm to search for best variable set:
\begin{itemize}[topsep=2pt,itemsep=0pt]
\item Exhaustive Search and \hyperlink{ModelValidationCriteria}{Test}: Used for $ p\leq \sim 30 $
\item Greedy Search: Get a locally optimal solution.\index{Greedy Algorithm}
\begin{itemize}[topsep=2pt,itemsep=2pt]
    \item Forward Selection: Start with $ p=0 $, add one variable each times and conduct $ t/F/p $-value test until a presupposed certain limit.
    \item Backward Elimination: Start with $ p_\mathrm{max} $, eliminate one variable each times and conduct $ t/F/p $-value test until a presupposed certain limit.
    \item Stepwise Regression: Alternate forward selection \& backward elimination until no add/elimination.
\end{itemize}

\item Penalized Optimization: 

Recall: OLS regression model: Minimize $ \mathrm{SSE}  $\footnote{Here expressed in $ \ell_p  $ norm, definition see sec.\ref{SubSubSectionMatrixNotationAndLemma}, \hyperlink{NormDefinition}{Norm}}
\begin{equation}
    \hat{\beta }=\arg\min \left\Vert Y-X\beta \right\Vert _2 ^2
\end{equation}

Idea: Add a penalty term in $ \mathrm{SSE} $, such that $ \mathrm{SSE} $ increases with \# of variables/value of variables.



\begin{itemize}[topsep=2pt,itemsep=2pt]  
\item LASSO (Least Absolute Shrinkage and Selection Operator)\index{LASSO (Least Absolute Shrinkage and Selection Operator)}
     
Penalty term: $ \lambda\Vert \beta \Vert _1) $, where $ \lambda  $ is a proper penalty parameter.
\begin{equation}
    \hat{\beta }=\arg\min (\left\Vert Y-X\beta \right\Vert _2 ^2+\lambda\left\Vert \beta \right\Vert _1)
\end{equation}

    or equivlantly expressed as 

\begin{equation}
    \hat{\beta }=\arg\min \left\Vert Y-X\beta \right\Vert _2 ^2, \text{ with }\Vert \beta \Vert _1\leq s
\end{equation}

where $ s $ is a parameter correponding to $ \lambda  $. Select a proper value of $ \lambda  $(or equivlantly $ s $) for expected model: Some $ \hat{\beta }_i $ would be exactly $ 0 $.



\item Ridge Regression/Tikhonov Regularization:\index{Ridge Regression}\index{Tikhonov Regularization}

Penalty term: $ \lambda\Vert \beta \Vert _2^2) $, where $ \lambda  $ is penalty parameter.
\begin{equation}
    \hat{\beta }=\arg\min (\Vert Y-X\beta \Vert _2 ^2+\lambda\Vert \beta \Vert _2^2)
\end{equation}

    or equivlantly expressed as 

\begin{equation}
    \hat{\beta }=\arg\min \Vert Y-X\beta \Vert _2 ^2, \text{ s.t. }\Vert \beta \Vert ^2_2\leq s
\end{equation}
Select a proper value of $ \lambda  $ (or equivlantly $ s $) for expected model. Generally Ridge regression \textbf{cannot} conduct variable selection, but usually used to avoid non-invertible $ X'X $, or used to retain important but collinear variable.

Solution of Ridge regression:\footnote{Why Ridge regression can also fix the problem of colinearity, i.e. non-full rank $ XX' $:

Assume the SVD decomposition of $ X $: $ X=U\Sigma V' $, then 
\begin{align*}
    X'X+\lambda I=&V\Sigma U'U\Sigma V'+\lambda I\\
    =& V \begin{bmatrix}
    \sigma^2 _{1}+\lambda &0&\ldots&0\\
    0&\sigma^2 _{2}+\lambda &\ldots&0\\
    \vdots&\vdots&\ddots&\vdots\\
    0&0&\ldots&\sigma^2 _{p+1}+\lambda 
    \end{bmatrix}V'
\end{align*}

then for $ \lambda >0 $, we can get a positive-definite matrix $ X'X+\lambda I $
}
\begin{equation}
    \hat{\beta }^\mathrm{Ridge}_\lambda  =(X'X+\lambda I)^{-1}X'Y
\end{equation}



\item Mixed Model: Elastic Net\index{Elastic Net}
\begin{equation}
    \hat{\beta }=\arg\min (\Vert Y-X\beta \Vert _2 ^2+\lambda _1\Vert \beta  \Vert _1+\lambda_2\Vert \beta \Vert _2^2)
\end{equation}

or equivalent form:
\begin{align*}
    \hat{\beta }=&\mathop{\arg\min}\limits_{\beta}\Vert Y-X\beta  \Vert ^2\\
    &s.t. \, \dfrac{\lambda _1}{\lambda _1+\lambda _2}\Vert \beta  \Vert_1+\dfrac{\lambda _2}{\lambda _1+\lambda _2}\Vert \beta  \Vert _2^2\leq s 
\end{align*}

picking proper hyper-parameter $ (s,\lambda =\dfrac{\lambda _2}{\lambda _1+\lambda _2}) $

\end{itemize}

\begin{rcode}
    \begin{lstlisting}[language=R]
    library('MASS')
    Rfit <- lm.ridge(y~x,lambda=seq(0,0.1,0.001),data=df)
    summary(Rfit)
    whichLambda <- which.min(Rfit$GCV)
    coef(fits)[whichLambda,]
    
    library('lars')
    Lfit <- lars(x,y,type='lasso')
    summary(Lfit)
    whichCp <- which.min(Lfit$Cp)
    Lfit$Cp[whichCp]
    Lfit$beta[whichCp,]
    \end{lstlisting}
    \end{rcode}
    \item Non-parametric Regression Model: Add smooth/penalty function.
    
    Example: \lstinline|loess| (Locally Regression), \lstinline|lowess| (Locally Weighted ScatterPlot Smoother), Regression Tree.

%%%%%%%%%% Example Code NEEDED

    \item Other Regression Model:
   、\begin{itemize}[topsep=2pt,itemsep=0pt]
       \item Standardized Regression Model
For regression model $Y_i=\beta _0+\sum_{j=1}^pX_{ij}\beta _j +\varepsilon_i,\, i=1,2,\ldots,n  $, conduct Standardization (with an extra const $ 1/\sqrt{n-1} $) to $ Y $ and $ X $.
\begin{equation}
    Y^*_i=\dfrac{1}{\sqrt{n-1}}\dfrac{Y_i-\bar{Y}}{s_Y}\qquad X_{ij}^*=\dfrac{1}{\sqrt{n-1}}\dfrac{X_{ij}-\bar{X}_i}{s_{X_i}}\qquad \varepsilon _i^*=\dfrac{1}{\sqrt{n-1}}\dfrac{\varepsilon _i-\bar{\varepsilon }}{s_Y} 
\end{equation}

And the regreeesion model for standardized data:
\begin{equation}
    Y^*_i=0+\sum_{j=1}^nX_{ij}^*\beta _j^*+\varepsilon _i^* 
\end{equation}

with
\begin{equation}
    \beta _j^*=\dfrac{\beta _js_{X_j}}{s_Y} 
\end{equation}

Note: set the const as $ \sqrt{n-1} $ so that 
\begin{equation}
    r_{X^*X^*}=X^{*T}X^* \qquad r_{Y^*X^*}=X^{*T}Y^*
\end{equation}
\begin{rcode}
\begin{lstlisting}[language=R]
scaledf <- data.frame(scale(df))
scalelmfit <- lm(~,scaledf)
summary(scalelmfit)
\end{lstlisting}
\end{rcode}
    \item Polynomial Regression Model

\begin{rcode}
\begin{lstlisting}[language=R]
polfit <- lm(y~x+I(x^2),df)
polfit <- lm(y~polym(x1,x2,degree=),df)
\end{lstlisting}
\end{rcode}
    
    \item Interaction Model
    Example: 
    \begin{equation}
        Y=\beta _0+\beta _1X_1+\beta _2X_2+\beta _{3}X_1X_2+\varepsilon  
    \end{equation}
    
    Re-write as
    \begin{align*}
        Y=&\beta _0+(\beta _1+\beta _3X_2)X_1+\beta _2X_2+\varepsilon \\
        Y=&\beta _0+\beta _1X_2+(\beta _2+\beta _3X_1)X_2+\varepsilon 
    \end{align*}

    test the regression coefficient dependence on another variable.

\end{itemize}
   
      
\end{itemize}





\subsection{Factor Analysis of Variance}
\subsubsection{Single Factor Model}
    Single factor, or one-way analysis of variance focuses on continuous $ Y \sim $ categorical $ X $ (numeric-factor). Regression goal is the mean response of each category $ \pi_i $: whether \& how much they are different.

    Basic assumptions: Normal within each categories, Equal variance, independent

    Model: See \autoref{EqaFactorAnalysisModel} expression for single factor model
    \begin{equation}
        Y_{ij}=\mu+\tau_i+\varepsilon _{ij},\quad \varepsilon _{ij}\sim N(0,\sigma ^2)
    \end{equation}

    where $ \tau_i $ for group effect, $ \mu_i=\mu +\tau_i $ for factor effect. Originally only $ \mu_i $ are estimatable.

\begin{rcode}
    \lstinline|lm()| in \lstinline|R.| uses cell means model, returns $ \mu_i=\mu +\tau_i $ for each categories.
\begin{lstlisting}[language=R]
facfit <- lm(y~x,df) # where x should be as.factor() type
\end{lstlisting}
\end{rcode}    

\begin{point}
    Statistical Inference to Individual $ \mu ,\tau_i $
\end{point}

    Note: $ \mathrm{rk}(X)=r<\# \mathrm{variable}=r+2 \Rightarrow $ estimator not unique. Usually use constraint
\begin{align*}
    \sum_{i=1}^rc_i\tau_i=&0
    % \sum_{i=1}^rn_i\tau_i=&0
\end{align*}
    usually take $ c_i=1 $ or $ c_i=n_i $

\begin{itemize}[topsep=2pt,itemsep=0pt]
    \item Factor effect solution for $ c_i=1 $, i.e. $ \sum_{i=1}^r \tau _i=0 $
\begin{align*}
    \hat{\mu }=&\dfrac{1}{r}\sum_{i=1}^r\bar{Y}_i=\dfrac{1}{r}\sum_{i=1}^r\sum_{j=1}^{n_i}\dfrac{Y_{ij}}{n_i}\\
    \hat{\tau}_i=&\bar{Y}_i-\hat{\mu }
\end{align*}
    \item Factor effect solution for $ c_i=n_i $, i.e. $ \sum_{i=1}^r n_i\tau _i=0 $
\begin{align*}
    \hat{\mu }=&\bar{Y}=\dfrac{1}{n_T}\sum_{i,j}Y_{ij}\\
    \hat{\tau}_i=&\bar{Y}_i-\bar{Y}=\dfrac{1}{n_i}\sum_{j=1}^{n_i}Y_{ij}-\hat{\mu}\\
\end{align*}

\end{itemize}

\begin{point}
    One-Way ANOVA
\end{point}

    ANOVA table in the form of $ r=p+1 $ multivariate ANOVA in page~\pageref{TableMultivariateANOVA} 

    \begin{table}[H]
        \centering
        \renewcommand\arraystretch{1}
        \begin{tabular}{c|cccc}
            \hline
            Source&$ dof $&SS&MS&$ F $-Statistic\\\hline
            SSRegression&$ r-1 $&$ \sum_{i=1}^r(\hat{Y}_i-\bar{Y})^2  $&SSR/$ dof_R $& $ \mathrm{MSR}/\mathrm{MSE} $\\
            SSError&$ n_T-r $&$ \sum_{i=1}^r\sum_{j=1}^{n_i}(Y_{ij}-\hat{Y}_i)^2  $&SSE/$ dof_E $& \\
            SSTotal&$ n_T-1 $&$ \sum_{i=1}^r\sum_{j=1}^{n_i}(Y_{ij}-\bar{Y})^2  $&SST/$ dof_T $& \\
            \hline
        \end{tabular}
    \end{table}

    Use $ \mathrm{MSE}  $ as estimator of $ \sigma ^2 $:

\begin{equation}
    \hat{\sigma ^2}=\dfrac{1}{n_T-r} \sum_{i=1}^r\sum_{j=1}^{n_i}(Y_{ij}-\hat{Y}_i)^2 =\dfrac{1}{n_T-r}\left[ \sum_{i=1}^r\sum_{j=1}^{n_i}Y_{ij}^2-\sum_{i=1}^r\dfrac{\bar{Y}_i^2}{n_i} \right]
\end{equation}

    Also $ F $-statistics for $ H_0 :\tau_1=\tau_2=\ldots=\tau_r=0$
    \begin{equation}
        F=\mathrm{MSR}/\mathrm{MSE}=\dfrac{\mathrm{SSR}/(r-1) }{\mathrm{SSE}/(n_T-r) }\sim F_{r-1,n_T-r},\text{ under }H_0
    \end{equation}
    
\begin{point}
    Statistical Inference to Difference
\end{point}

    We usually focus on `difference' between factor effects, general form 
    \begin{equation}\phi=\sum_{i=1}^r\xi _i\tau_i ,\qquad  \sum_{i=1}^r\xi _i=0 \end{equation}
    where $ \phi $ with $ \sum_{i=1}^r\xi _i=0 $ is called a contrast. Assume there are $ m $ estimator $ \phi _k $, $ k=1,2,\ldots,m $, $ m\leq \dfrac{r(r-1)}{2} $

\begin{equation}
    \mathop{\phi }\limits_{m\times 1} =\begin{bmatrix}
        \phi _1\\\phi _2\\\vdots\\\phi _m
    \end{bmatrix} 
    =\mathop{\xi }\limits_{m\times r} \mathop{\tau}\limits_{r\times 1}  =
    \begin{bmatrix}
    \xi _{11}&\xi _{12}&\ldots&\xi _{1r}\\
    \xi _{21}&\xi _{22}&\ldots&\xi _{2r}\\
    \vdots&\vdots&\ddots&\vdots\\
    \xi _{m1}&\xi _{m2}&\ldots&\xi _{mr}
    \end{bmatrix}\begin{bmatrix}
        \tau_1\\\tau_2\\\vdots\\\tau_r
    \end{bmatrix}
\end{equation}


\begin{itemize}[topsep=2pt,itemsep=0pt]
    \item Distribution of $ \phi_k=\sum\limits_{i=1}^r\xi _{ki}\tau_i $, with $ \sum\limits_{i=1}^r\xi _i=0 $:
    \begin{equation}
        \phi_k=\sum_{i=1}^r\xi _{ki}\tau_i\sim N(\sum_{i=1}^r\xi _{ki}\bar{Y}_i,\sigma ^2\sum_{i=1}^r\dfrac{\xi _{ik}^2}{n_i})
    \end{equation}
    
    Or use transform of multivariate normal in \autoref{EqaTransformOfMultiNormal}
    \begin{equation}
         \phi \sim N_m(\xi \tau,\sigma ^2\xi \xi ')
    \end{equation}
    
    \item Sampling Distribution of $ \hat{\phi}_k  $:
    \begin{equation}
        \hat{\phi}_k\sim N(\sum_{i=1}^r\xi _{ki}\bar{Y}_i,\sigma ^2\sum_{i=1}^r\dfrac{\xi _{ki}^2}{n_i}) 
    \end{equation}
    
    \item Bonferroni's Confidence Region for $ \mathop{\phi}\limits_{m\times 1}  $, using result in \autoref{EqaConfidenceRegionUsingBonferroni}
    \begin{equation}
        R(\phi )= \bigotimes\limits_{k=1}^m\left(\sum_{i=1}^r\xi _{ki}\bar{Y}_i \pm \hat{\sigma }t_{n_T-r,\frac{\alpha }{2m}} \sqrt{\sum_{i=1}^r\dfrac{\xi _{ki}^2}{n_i}} \right)
    \end{equation}
    
    \item Scheff\`{e}'s Confidence Region for $ \mathop{\phi }\limits_{1\times 1} $:
    \begin{equation}
         R(\phi)=\sum_{i=1}^r\xi_i\bar{Y}_i\pm \hat{\sigma }\sqrt{(r-1)F_{r-1,n_T-r,\alpha }}\sqrt{\sum_{i=1}^r\dfrac{\xi _{ki}^2}{n_i}}
    \end{equation}
    
    \item Tukey's Confidence Region for $ \mathop{\phi }\limits_{1\times 1} $, under condition $ n_1=\ldots=n_r=n $: focus on estimating $ \tau_i-\tau_j $
    
    \begin{itemize}[topsep=2pt,itemsep=0pt]
        \item Def.: studentized range distribution\index{Studentized Range Distribution}: for $ Z_1,\ldots,Z_n $ i.i.d. $ \sim N(0 ,1) $, $ mW^2\sim \xi _m^2 $, then 
        \begin{equation}
            q=\dfrac{\max Z_i-\min Z_i}{W}\sim q_{n,m} 
        \end{equation}
    \end{itemize}
    
    Then confidence interval for $ \phi =\tau_i-\tau_j $ 
    \begin{equation}
        R(\phi )=\bar{Y}_i-\bar{Y}_j\pm q_{r,n_T-r,\alpha }\dfrac{\hat{\sigma }}{\sqrt{n}}
    \end{equation}
    
    General case: $ \phi =\sum_{i=1}^r\xi _i\tau_i $
    \begin{equation}
        R(\phi )=\sum_{i=1}^r\xi _i\bar{Y}_i\pm q_{r,n_T-r,\alpha }\dfrac{\hat{\sigma }}{2\sqrt{n}}\sum_{i=1}^r|\xi _i|
    \end{equation}
    
     
\end{itemize}

    Comment: Scheff\`{e} is more conservative, i.e. shorter. If confidence interval does not include $ 0 $, we can say they are significantly different.

\begin{rcode}
\begin{lstlisting}[language=R]
library('agricolae')
facaov <- aov(y~0+x,df)

LSD.test(facaov,trt='design',group=FALSE,console=TRUE)

scheffe.test(facaov,trt='design',group=FALSE,console=TRUE)

TukeyHSD(facaov,conf.level=0.95)
\end{lstlisting}
use \lstinline|plot()| to view interval estimation
\end{rcode}    
    
    


\subsubsection{Double Factor Model}
    Double factor, or two-way analysis of variance, categories $ \pi_{ij} $:
    
    \begin{equation}
        Y_{ijk}= \mu +\alpha _i+\beta _j+e_{ijk}
    \end{equation}
    
    LS estimator with $ \sum_{i=1}^a \alpha _i=0 $, $ \sum_{j=1}^b\beta _j=0 $:
    \begin{align*}
        \hat{\mu }=&\dfrac{1}{ab}\sum_{i=1}^a\sum_{j=1}^b\sum_{k=1}^{n_{ij}}\dfrac{Y_{ijk}}{n_{ij}}\\
        \hat{\alpha }_i=&\dfrac{1}{b}\sum_{j=1}^b\sum_{k=1}^{n_{ij}}\dfrac{Y_{ijk}}{n_{ij}}-\hat{\mu }\\
        \hat{\beta }_j&\dfrac{1}{a}\sum_{i=1}^a\sum_{k=1}^{n_{ij}}\dfrac{Y_{ijk}}{n_{ij}}-\hat{\mu }
    \end{align*}
    
    $ \mathrm{MSE}  $ estimator of $ \sigma ^2 $:
    \begin{equation}
        \hat{\sigma ^2}= \dfrac{1}{n_T-ab}\sum_{i=1}^a\sum_{j=1}^b\sum_{k=1}^{n_{ij}}(Y_{ijk}-\bar{Y}_{ij})^2=\dfrac{1}{n_T-ab}\left[ \sum_{i=1}^a\sum_{j=1}^b\sum_{k=1}^{n_{ij}}Y_{ijk}^2-\sum_{i=1}^a\sum_{j=1}^b\dfrac{\bar{Y}_{ij}^2}{n_{ij}} \right]
    \end{equation}
    

    
% \begin{itemize}[topsep=2pt,itemsep=0pt]
%     \item Cell Means Model
%     \begin{equation}
%         Y_{ij}=\mu_i+\varepsilon _{ij} ,\quad \varepsilon _{ij}\text{ i.i.d. }\sim N(0,\sigma ^2) 
%     \end{equation}
    
%     Target: Estimate $ \mu _i $, $ \sigma ^2 $/Hypotheses testing $ H_0:\mu _1=\ldots=\mu _r=\mu $

%     Estimator:
% \begin{align*}
%     \hat{\mu }_i=&\bar{Y}_i=\dfrac{1}{n_i}\sum_{j=1}^{n_i}Y_{ij}\\
%     \hat{\sigma^2 }_i=&\dfrac{1}{n_i-1}\sum_{j=1}^{n_i}(Y_{ij}-\bar{Y}_i)^2\\
%     \hat{\sigma ^2}=\dfrac{1}{n_T-r}\sum_{i=1}^r\left[(n_i-1)\hat{\sigma ^2}_i\right]=\dfrac{1}{n_T-r}\sum_{i=1}^r\sum_{j=1}^{n_i}(Y_{ij}-\bar{Y}_i)^2
% \end{align*}
% \begin{rcode}
% \begin{lstlisting}[language=R]
% facfit <- lm(y~x,df) # where x should be as.factor() type
% \end{lstlisting}
% \end{rcode}
        
% \end{itemize}

    


\subsection{Generalized Linear Model}\label{SubSectionGeneralizedLinearModel}
    Recall: Linear model with normal assumption can be expressed as :
    \begin{equation}
        Y_i\sim N(\mu _i,\sigma_i ^2)=N(x_i'\beta ,\sigma_i ^2) 
    \end{equation}
    
    Question: How to generalize the simple linear model?
\begin{itemize}[topsep=2pt,itemsep=0pt]
    \item Generalize the distribution
    \item Generalize the dependent mode
\end{itemize}

\begin{point}
    Distribution Generalize: Scaled Exponential Family
\end{point}

    % simple linear model focuses on continuous $ Y $, thus use $ N(\mu _i,\sigma ^2) $ ranging in $ \mathbb{R} $, f
    For different range and feature of $ Y $ we can use different distribution for regression. We usually use Exponential Family distribution $ f(y;\vec{\theta },\vec{\phi }) $ as in \autoref{EqaExponentialDistributionFamily}, with some constraint on subfunctions for better distribution properties, written as linear scaled exponential family:
\begin{equation}\label{EqaScaledExponentialFamily}
    f(y;\vec{\theta },\vec{\phi})=\exp\left\{ \dfrac{y'\theta -b(\theta )}{a(\phi)}+c(y,\phi ) \right\}
\end{equation}


    where $ \vec{\theta } $ is the canonical parameter for location and $ \vec{\phi } $ for scale(usually we take $ a(\phi )\propto \phi  $).

    Properties of $ f(y;\theta ,\phi ) $:
\begin{itemize}[topsep=2pt,itemsep=0pt]
    \item Expectation
\begin{equation}\label{EqaScaledExponentialFamilyExpectation}
    \mu \equiv \mathbb{E}(Y)=\int yf(y) \,\mathrm{d}y=\int \left(a(\phi )\dfrac{\partial^{} }{\partial \theta ^{}}+\dfrac{\mathrm{d}^{} b(\theta )}{\mathrm{d}\theta ^{}}\right)f(y) \,\mathrm{d}y= b'(\vec{\theta })
\end{equation}
      
    
    \item Variance
    \begin{align}\label{EqaScaledExponentialFamilyVariance}
        \sigma ^2\equiv var(Y)=& \int yy^Tf(y) \,\mathrm{d}y- \mathbb{E}(Y)\mathbb{E}(Y)^T\\
        =&\int \left( \dfrac{\partial^{2} }{\partial \theta\partial\theta^T }+(b'(\theta )y+yb'(\theta ))-b'(\theta )b'(\theta )^T+a(\phi )\dfrac{\mathrm{d}^{2}b(\theta ) }{\mathrm{d}\theta  \mathrm{d}\theta ^T } \right)f(y) \,\mathrm{d}y -\mathbb{E}(Y)\mathbb{E}(Y)^T\\
        =&a(\phi )\dfrac{\mathrm{d}^{2} b(\vec{\theta })}{\mathrm{d}\theta \mathrm{d}\theta ^T}=a(\phi )b''(\vec{\theta })
    \end{align}
\item Examples: Normal, Binomial, Poisson
\begin{itemize}[topsep=2pt,itemsep=0pt]
    \item Normal $ f(y)=\dfrac{1}{\sqrt{2\pi}|\Sigma | }\exp\left( -\dfrac{1}{2}(y-\mu )'\Sigma ^{-1}(y-\mu ) \right)$ with $ \Sigma =\sigma ^2I $:
    \begin{equation}
        f(y)=\exp\left( \dfrac{y'\mu -\frac{1}{2}\mu'\mu }{\sigma ^2}-\dfrac{y'y}{2\sigma ^2}-\dfrac{1}{2}\ln(2\pi\sigma ^2) \right) 
    \end{equation}
    
    Compare with \autoref{EqaScaledExponentialFamily}, $ \theta =\mu$: $ b(\theta  )=\dfrac{1}{2}\mu '\mu  $, $ a(\phi )=\sigma ^2$
    \begin{itemize}[topsep=2pt,itemsep=0pt]
        \item $ \mathbb{E}(Y)=b'(\theta  )=\mu  $
        \item $ var(Y)=a(\phi )b''(\theta  )=\sigma ^2 $
    \end{itemize}

    \item Binomial $ \mathbb{P}(y)=\binom{n}{y}\pi^y(1-\pi)^{n-y} \sim B(n,\pi) $:
    \begin{equation}
        f(y)=\exp\left( y\ln(\dfrac{\pi}{1-\pi})+n\ln(1-\pi)+\ln\binom{n}{y} \right)
    \end{equation}
    
    Compare with \autoref{EqaScaledExponentialFamily}, $ \theta =\ln(\dfrac{\pi}{1-\pi})\Leftrightarrow \pi=\dfrac{1}{1+e^{-\theta }}$: $ b(\theta )=-n\ln(1-\pi)=-n\ln\dfrac{1}{1+e^\theta } $, $ a(\phi )=1 $
    \begin{itemize}[topsep=2pt,itemsep=0pt]
        \item $ \mathbb{E}(Y)=b'(\theta )=n\ln\dfrac{1}{1+e^{-\theta }}=n\pi $
        \item $ var(Y)=a(\phi )b''(\theta )=n\pi(1-\pi) $
    \end{itemize}
    \item Poisson $ \mathbb{P} (y)=\dfrac{\lambda^y }{y!}e^{-\lambda } \sim P(\lambda ) $:
    \begin{equation}
        f(y)=\exp\left( y\ln\lambda -\lambda-\ln y!  \right) 
    \end{equation}
    
    Compare with \autoref{EqaScaledExponentialFamily}, $ \theta =\ln\lambda \Leftrightarrow \lambda =e^\theta $: $ v(\theta )=\lambda =e^\theta  $, $ a(\phi )=1 $
    \begin{itemize}[topsep=2pt,itemsep=0pt]
        \item $ \mathbb{E}(Y)=b'(\theta )=\lambda  $
        \item $ var(Y)=a(\phi )b''(\theta )=\lambda  $
    \end{itemize}

\end{itemize}
\end{itemize}

\begin{point}
    Dependent Mode Generalize: Link Function
\end{point}

    Note that $ Y_i\sim N(\mu_i,\sigma _i^2)=N(x_i'\beta ,\sigma _i^2) $ contains the dependency of $ \mu_i  $ on  $ x_i\beta  $ thus we can further generalize the regression model as $ \mu _i=x'\beta   $, here $ \mu _i $ stands for $ \mathbb{E}(Y) $ as in \autoref{EqaScaledExponentialFamilyExpectation}. However for different distributions, $ \mu=\mathbb{E}(Y) $ have specific range, e.g. $ \mu\in [0,n] $ for $ B(n,p) $, while $ x'\beta \in \mathbb{R} $, thus use a \textbf{link function}$ g $: $ I_{\mu}\to I_{x'\beta } $ to adjust the range:
    \begin{equation}
         x_i'\beta = g(\mu_i )\Leftrightarrow \mu _i=g^{-1}(x'\beta )
    \end{equation}
    
    Note: Link function should be monodrome \& differentiable such that $ g^{-1} $ exists. And here $ x'\beta  $ term still exist (because it's still generalized \uline{linear} model), thus we denote $ \eta:=x'\beta  $ as a linear predictor/classifier
    \begin{equation}
        \eta:= x'\beta  
    \end{equation}
    
    Regression Model:
    \begin{equation}
        \eta_i=g(\mu_i)\Leftrightarrow \mu _i=g^{-1}(\eta_i) 
    \end{equation}

\begin{point}
    Useful Generalized Linear Model:
\end{point}

    Important Question: how to choose proper generalization `pair' : Distribution \& Link Function pair?

    Idea: Use the expectation transform:
\begin{align*}
    \text{Distribution: }&\mu =\mathbb{E}(Y)=b'(\theta )\\
    \text{Link Function: }& \mu=g^{-1}(x'\beta ) 
\end{align*}
    
    Thus 
    \begin{equation}
        g^{-1}(x'\beta )=b'(\theta )\Rightarrow  \eta=x'\beta = g(b'(\theta ))
    \end{equation}
    
    For model simplification, we can choose $ g(\,\cdot\,), b(\,\cdot\,) $ such that 
\begin{equation}\label{EqaGLMModelFunctionSelection}
    g(b'(\,\cdot\,))=\mathrm{Id}(\,\cdot\,)\Leftrightarrow g^{-1}(\,\cdot\,)=b'(\,\cdot\,)    
\end{equation}

    such condition is called \textbf{Canonical Link} of generalized linear model, such choise of link function makes $ x'\beta  $ the canonical parameter in model.
    \begin{equation}
        \theta=\eta=x'\beta =g(\mu )\leftrightsquigarrow g^{-1}(\theta )=g^{-1}(\eta)=g^{-1}(x'\beta )=\mu =\mathbb{E}(Y)
    \end{equation}
    
    
    
    
    
    


\begin{itemize}[topsep=2pt,itemsep=0pt]
    \item Simle linear model: $ N(\mu ,\sigma ^2) $, $ g(\,\cdot\,)=\mathrm{Id}(\,\cdot\,) $
    \begin{equation}
        \mu _i=\eta_i
    \end{equation}
    \item Logistic Model: $ B(n,\pi) $, $ g(x)=\mathrm{logit}(x)=\ln\dfrac{x}{1-x}\Leftarrow   $$ g^{-1}(y)=\mathrm{logistic}(y)=\dfrac{1}{1+e^{-y}}  $
    
    \begin{equation}
        n\pi_i=\mu _i=g^{-1}(\eta_i) 
    \end{equation}
    
    \item Poisson Model: $ P(\lambda ) $, $ g(\,\cdot\,)=\ln(\,\cdot\,)\Leftrightarrow g^{-1}(\,\cdot\,)=\exp(\,\cdot\,)  $
    \begin{equation}
        \lambda _i=\mu_i=g^{-1}(\eta_i) 
    \end{equation}
    
    
\end{itemize}

    
\begin{point}
    Solution of Generalized Linear Model
\end{point}

    Using the distribution of $ Y_i $ dependent on $ x_i'\beta  $, we can use MLE maximizing to solve $ \beta  $. Algorithm for such maximizing task is called Iteratice Re-weighted Least Squares, more specifically when using Newton-Raphson Method, this method is called Fisher's Scoring Method. Detail see \autoref{SubSubSectionFisherScoringMethod}.



    


















% \begin{point}
%     Logistic Regression
% \end{point}

%     Naturally we hope distribution parameter $ \mu _i $/$ \pi_i $/$ \lambda _i $ can be function $ g $ of $ x_i $, or more specially function of $ \eta_i\equiv x_i'\beta  $(linear combination). For simple linear model, $ \mu_i=\mathrm{Id}(x_i'\beta )=i'\beta=\eta_i  $, but for e.g. linear probability model where $ \pi_i\in [0,1] $, we need such \textbf{link function}$ g $: $ I_{\pi}\to I_{\eta} $ to adjust the range:
%     \begin{equation}
%         \pi_i= g^{-1}(\eta_i)=\dfrac{1}{1+e^{-\eta_i}}\Leftrightarrow \eta_i=g(\pi_i)=\log\dfrac{\pi_i}{1-\pi_i}
%     \end{equation}
    
%     where $ g(\pi)=\dfrac{\pi_i}{1-\pi_i}\equiv \mathrm{logit}(\pi) $, $ g^{-1}(\eta)=\dfrac{1}{1+e^{-\eta}}\equiv \mathrm{logistic}(\eta)  $

%     Then usually MLE method is used to estimate $ \beta  $:
%     \begin{align*}
%         \log L(\beta ;x)=&\sum_{x_i}\left\{ y_i\log(\pi_i)+(1-y_i)\log(1-\pi_i) \right\}\\
%         \pi_i=&\dfrac{1}{1+e^{-x_i'\beta }}
%     \end{align*}
   
    
    
    
    
    


