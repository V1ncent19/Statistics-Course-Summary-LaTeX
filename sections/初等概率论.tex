\chapter{概率论部分}\label{Section1Probability}
\begin{center}
    Instructor: Wanlu Deng
\end{center}

%     Chapter Overview
% \begin{itemize}[topsep=2pt,itemsep=2pt]
%     \item \hyperlink{Basic axioms}{Basic axioms}
% \end{itemize}

    









%     Cover：Basic axioms, random events, $\sigma$-field; random variable/vector and their properties, some special distributions; $E$\,\&\,$\sigma^2$\,\&\,$cov$ and their properties; probability-generating/moment-generating/characteristic function; weak/strong law of large number, central limit thm.; intro. to multivariate normal distribution.



\section{Some Important Distributions}\label{SectionImportantDistributions}

\begin{table}[htbp]
    \centering
    \begin{tabular}{c|cccc}
        \hline
        $X$&$p_X(k)\big/f_X(x)$&$\quad \mathbb{E}\quad$&$var$&MGF\\
        \hline
        $\mathrm{Bern} (p)$& &$p$&$pq$&$q+pe^s$\\
        $B (n,p)$&$C_n^k p^k(1-p)^{n-k}$&$np$&$npq$&$(q+pe^s)^n$\\
        $\mathrm{Geo} (p)$&$(1-p)^{k-1}p$&$\dfrac{1}{p}$&$\dfrac{q}{p^2}$&$\dfrac{pe^s}{1-qe^s}$\\
        $H(n,M,N)$&$\dfrac{C_M^kC_{N-M}^{n-k}}{C_N^n}$&$n\dfrac{M}{N}$&$\dfrac{nM(N-n)(N-M)}{N^2(n-1)}$&\\
        $P(\lambda)$&$\dfrac{\lambda^k}{k!}e^{-\lambda}$&$\lambda$&$\lambda$&$e^{\lambda(e^s-1)}$\\
        $U(a,b)$&$\dfrac{1}{b-a}$&$\dfrac{a+b}{2}$&$\dfrac{(b-a)^2}{12}$&$\dfrac{e^{sb}-e^{sa}}{(b-a)^s}$\\
        $N(\mu,\sigma^2)$&$\dfrac{1}{\sigma \sqrt{2\pi}}e^{-\frac{(x-\mu)^2}{2\sigma^2}}$&$\mu$&$\sigma^2$&$e^{\frac{\sigma^2s^2}{2}+\mu s}$\\
        $\epsilon(\lambda)$&$\lambda e^{-\lambda x}$&$\dfrac{1}{\lambda}$&$\dfrac{1}{\lambda^2}$&$\frac{\lambda}{\lambda-s}$\\
        $\Gamma(\alpha,\lambda)$&$\dfrac{\lambda^\alpha}{\Gamma(\alpha)}x^{\alpha-1}e^{-\lambda x}$&$\dfrac{\alpha}{\lambda}$&$\dfrac{\alpha}{\lambda^2}$&$\left(\frac{\lambda}{\lambda-s}\right)^\alpha $\\
        $B(\alpha,\beta)$&$\dfrac{1}{B(\alpha,\beta)}x^{\alpha-1}(1-x)^{\beta-1}$&$\dfrac{\alpha}{\alpha+\beta}$&$\dfrac{\alpha\beta}{(\alpha+\beta)^2(\alpha+\beta+1)}$&\\
        $\chi^2_n$&$\dfrac{1}{2^{\frac{n}{2}}\Gamma(\frac{n}{2})}x^{\frac{n}{2}-1}e^{-\frac{x}{2}}$&$n$&$2n$&$ (1-2s)^{-n/2} $\\
        $t_\nu$&$\dfrac{\Gamma(\frac{\nu+1}{2})}{\sqrt{\nu\pi}\Gamma(\frac{\nu}{2})}(1+\frac{x^2}{\nu})^{-\frac{\nu+1}{2}}$&$0$&$\dfrac{\nu}{\nu-2}$&\\
        $F_{m,n}$&$\dfrac{\Gamma(\frac{m+n}{2})}{\Gamma(\frac{m}{2})\Gamma(\frac{n}{2})}\dfrac{m^\frac{m}{2}n^\frac{n}{2}x^{\frac{m}{2}-1}}{(mx+n)^{\frac{m+n}{2}}}$&$\dfrac{n}{n-2}$&$\dfrac{2n^2(m+n-2)}{m(n-2)^2(n-4)}$&\\
        \hline
    \end{tabular}
\end{table}

    Definition of PGF, MGF, CF see \autoref{SectionPGFMGFCF}.

    More Properties of $\chi^2,t,F$ see {\autoref{chi2_t_F_properties}}.

    Relation between distributions and more properties see \url{http://www.math.wm.edu/~leemis/chart/UDR/UDR.html}. Distribution support in \lstinline|R.| see \url{https://CRAN.R-project.org/view=Distributions}

Use the following command for all distributions supported in \lstinline|R. stats::|.
\begin{lstlisting}[language=R]
?Distributions
\end{lstlisting}


\section{Probability and Probability Model}

    What is \textbf{Probability}? A `belief' in `what would happen'.


\subsection{Sample Space and $\sigma$-Field}

\begin{point}
    Experiment and Sample Space
\end{point}

    Def. sample space $\Omega$: The set of \text{all} possible outcomes of one particular \textbf{experiment} . Conducting the experiment would result in a result/sample point $ \omega  $ in sample space $ \Omega  $. These results should be mutually exclusive, e.g. Tossing two coins simultaneously, the sample space is the set of all possible results
    \begin{align}
        \Omega = \{(0,0),(0,1),(1,0),(1,1)\}, \quad \omega \in \Omega 
    \end{align}

    On the sample space, the `belief' in results happening is measured by probability $ \mathbb{P}\left( \omega  \right),\,\omega \in\Omega   $

    \textbf{Note}: Randomness comes from the random result $ \omega  $ that an experiment generates.
    
\begin{point}
    Event 
\end{point}

    We may care about a conbination of some results, say `at least one of the coin lands tails-up'. It's like a kind of `structure' on sample space describing how we put results together to form \textbf{Events}. The definition is \index{Sigma-field@$ \sigma $-Field} a $\sigma$-field(or a $\sigma$-algebra) $\mathscr{F}$ as a collection of some subsets of $\Omega$, with properties:
    \begin{itemize}[topsep=2pt,itemsep=0pt]
        \item $\Omega\in\mathscr{F}$
        \item if $A\in\mathscr{F}$,then $A^\complement \in\mathscr{F}$
        \item if $A_n\in\mathscr{F}$, then ${\displaystyle\bigcup_{n=1}^\infty} A_n\in\mathscr{F}$
    \end{itemize}

    And $(\Omega,\mathscr{F})$ is a measurable space, on which we can select the events that we care about.

    Events (and their properties) can be described in the language of set, e.g. for events $ A $, $ B\in\mathscr{F} $
\begin{itemize}[topsep=2pt,itemsep=0pt]
    \item $ A=B $ means they are the same event
    \item $ A\cup B $ means one of them happens
    \item $ A\cap B $ or $ AB $ means both happen 
\end{itemize}

And some more complex ones
\begin{itemize}[topsep=2pt,itemsep=0pt]
    \item $ A\cup B=B\cup A $, $ A\cap B=B\cap A $
    \item $ A\cup (B\cup C)=A\cup B\cup C $, $ A\cap (B\cap C)=A\cap B\cap C $
    \item $ A\cap (B\cup C)=(A\cap B)\cup (B\cap C) $, $ A\cup (B \cap C)=(A\cup B)\cap (A\cup C) $
    \item $ A\cup B=A+ A^\complement\cap B $, $ A=A\cap B+A\cap B^\complement $
    \item[$ \Delta  $] $ (A\cup B)^\complement =A^\complement\cap B^\complement $, $ (A\cap B)^\complement = A^\complement \cup B^\complement  $
    \item $ (\bigcup_{j=1}^\infty A_j)^\complement =\bigcap_{j=1}^\infty A_j^\complement $
    \item $ (\bigcap_{j=1}^\infty A_j)^\complement =\bigcup_{j=1}^\infty A_j^\complement $
\end{itemize}
    
\subsection{Axioms of Probability}

    $\mathbb{P}(\,\cdot\,):\,\mathscr{F}\mapsto [0,1]$ is the probability measure (or probability function) defined on $(\Omega,\mathscr{F})$ describing the possibility that some event $ A\in\mathscr{F}  $ happens. Definition of probability $ \mathbb{P}(A) $ in useful models:
    \begin{align}
       \mathbb{P}\left( A \right) :=\begin{cases}
            \dfrac{\#A}{\#\Omega }&\text{Classical Model}\\
            \dfrac{m(A)}{m(\Omega )}&\text{Geometric Model}
        \end{cases}   
    \end{align}
    
    Where $ m(\, \cdot \, ) $ is some measure of events in continuous space, say integral in Euclidean Space $ \mathbb{R}^r $
    \begin{align}
        m_\mathrm{\mathbb{R}^r}(A)=\int_A \,\mathrm{d}x_1\,\mathrm{d}x_2\ldots\,\mathrm{d}x_r  
    \end{align}
    
    
    
    

    
    
    
\begin{point}
    Basic Axioms of Probability Mearure $ \mathbb{P}(\,\cdot\,) $
\end{point}

\begin{itemize}[itemsep=2pt,topsep=-2pt]
\item Non-negativity
\begin{equation}    \mathbb{P}(A)\geq 0\qquad \forall A\in\Omega    
\end{equation}
\item Normalization\footnote{Note: In other sections when dealing with not-yet-normalized distribution (say in Bayesian statistics), I usually use $ Z $ as the normalize constant, following the tradition in statistical physics where $ Z $ is the partition function.
\begin{align}
    \mathbb{P}=\dfrac{1}{Z}\tilde{\mathbb{P}},\quad Z=\int \mathbb{\tilde{P}} 
\end{align}}
\begin{equation}    \mathbb{P}(\Omega)=1    
\end{equation}




\item Countable Subadditivity\index{Countable Additivity}
\begin{equation}    \mathbb{P}(A_1\cup A_2\cup\cdots)=\mathbb{P}(A_1)+\mathbb{P}(A_2)+\cdots\quad ,\, (A_i\bot\!\!\!\bot  A_j\quad \forall i\neq j)
\end{equation}

where `countable subadditivity' means the events can be sequentially listed. e.g. $ [0,1]=\bigcup _{x\in [0,1]}\{x\} $ is not countable, intuition:
\begin{align}
    1=\mathbb{P}\left( [0,1] \right)=\mathbb{P}\left( \bigcup _{x\in [0,1]}\{x\} \right)   {\color{red}\neq} \sum_{x\in[0,1]}\mathbb{P}\left( x \right) =0
\end{align}


\end{itemize}

    Then $(\Omega,\mathscr{F},\mathbb{P})$ is probability space\index{Probability Space}, where $ \Omega  $ for experiment outcomes and randomness, $ \mathscr{F} $ for events and their algebra, $ \mathbb{P} $ for probability measure.

\begin{point}
        Properties of Probability:
\end{point}

    \begin{itemize}
        \item Addition Formula
        \begin{align}
            \mathbb{P}\left( A\cup B \right) =\mathbb{P}\left( A \right) +\mathbb{P}\left( B \right) -\mathbb{P}\left( A\cap B \right)  
        \end{align}
        \item Monotonicity
        \begin{equation}    
            \mathbb{P}(A)\leq \mathbb{P}(B)\quad \text{for}\, A\subset B
        \end{equation}
        \item Finite Subadditivity (Boole Inequality)\index{Inequality!Boole Inequality}
        \begin{equation}    
            \mathbb{P}(\bigcup_{i=1}^nA_i)\leq\sum_{i=1}^n \mathbb{P}(A_i)    
        \end{equation}
        \item Countable Subadditivity ($ \sigma  $-Subadditivity)\index{Sigma-Subadditivity@$ \sigma  $-Subadditivity}
        \begin{align}
            \mathbb{P}(\bigcup_{i=1}^\infty A_i)\leq\sum_{i=1}^\infty \mathbb{P}(A_i)  
        \end{align}
        
        
        \item Inclusion-Exclusion Formula (Jordan Formula)\index{Inclusion-Exclusion Formula}\index{Jordan Formula}
        \begin{align}
            \mathbb{P}(\bigcup_{i=1}^nA_i)=&\sum_{1\leq i\leq n}\mathbb{P}(A_i)-\sum_{1\leq i<j\leq n}\mathbb{P}(A_i\cap A_j)\\
            &+\sum_{1\leq i<j<k\leq n}\mathbb{P}(A_i\cap A_j\cap A_k)-\cdots\\
            &+(-1)^{n-1}\mathbb{P}(A_1 \cap A_2\cap\cdots \cap A_n)
        \end{align}

        Or in condensed notation:
        \begin{align}
            \mathbb{P}( \bigcup_{i=1}^n A_i)=&\sum_{k=1}^n (-1)^{k-1}\sum_{1\leq j_1<j_2<\ldots<j_k\leq n}\mathbb{P}\left( A_{j_1}\cap A_{j_2}\cap\ldots\cap A_{j_k} \right)   
        \end{align}
        
        
        \item Borel-Cantelli Lemma\index{Borel-Cantelli Lemma}
        \begin{align}
            &\sum_{n=1}^\infty \mathbb{P}(A_n)<\infty\Rightarrow \mathbb{P}(\lim_{n\to\infty}\sup A_n)=0\\
            &\sum_{n=1}^\infty \mathbb{P}(A_n)=\infty\Rightarrow \mathbb{P}(\lim_{n\to\infty}\sup A_n)=1\quad \text{if }A_i\text{ independent}
        \end{align}
            
    \end{itemize}


\begin{point}
    An Example 
\end{point}

    We have $ n $ different balls. Draw $ m $ times with replacement. What is the number of results regardless of order the balls drawn (e.g. $ \{\mathrm{red,red,black} \} $ is the same as $ \{\mathrm{red,black,red} \} $)? 

    The model is the same as we are `voting' for $ n $ different balls, with total ballot ticket $ m $. The $ m $ tickets are divided by $ n-1 $ plates (making them similar to ballot boxes), e.g. here's a $ n=4,m=6 $ vote corresponding to a result $ \omega \in\Omega  $:
    \begin{align}
         \bullet  \big|  \big| \bullet \bullet \bullet \big|  \bullet \bullet 
    \end{align}

    which the same as inserting plates sequentially and then cancel the order of plates:
    \begin{align}
        \# \Omega  = (m+1)*(m+2)\ldots (m+n-1)\bigg/ (n-1)! = \dfrac{(n+m-1)!}{m!(n-1)!}=\binom{n+m-1}{m}
    \end{align}

    (The idea of spacer plate is quite useful in dealing with some troublesome discrete cases, I think.)
    
    \begin{table}[H]
        \centering
        \renewcommand\arraystretch{1}
        \caption{$ \# \Omega  $ of Sampling $ n $ balls $ m $ draw}
        \begin{tabular}{ccc}
            \hline
            \hline
            &\multicolumn{2}{c}{Replacement}\\
            \cline{2-3}
            &With&Without\\
            \hline
            Ordered&$ n^m $&$ A_{n}^m $\\
            Unordered&$ \binom{n+m-1}{m} $&$ \binom{n}{m} $\\
            \hline
            \hline
        \end{tabular}
        \label{}
    \end{table}


    
    
    


\subsection{Conditional Probability}
    Motivation: To update the knowledge of probability measure.

    Def. \textbf{Conditional Probability} of $B$ given $A$:
    \begin{equation}    
        \mathbb{P}(B|A)=\frac{\mathbb{P}(A\cap B)}{\mathbb{P}(A)}    
    \end{equation}

    Actually it's a change of $\sigma$-field: $\Omega$ $ \to $ $B$
    \begin{align}
        \mathbb{P}\left( B|A \right) = \dfrac{m(B)}{m(A)} 
    \end{align}


\begin{point}
    Application of conditional probability:
\end{point}

        \begin{itemize}
        \item Multiplication Formula
        \begin{equation}    
            \mathbb{P}(\bigcap_{i=1}^n A_i)=\mathbb{P}(A_1)\prod_{i=2}^n \mathbb{P}(A_i|A_1\cap A_2\cap \cdots\cap A_{i-1})    
        \end{equation}
        \item Total Probability Thm.\index{Total Probability Thm.}
        \begin{equation}    
            \mathbb{P}(B)=\sum_{i=1}^n \mathbb{P}(A_i)\mathbb{P}(B|A_i)  
        \end{equation}
        where $\{A_i\}$ is a partition of $\Omega$: $ \Omega =\bigcup_{i}A_i ,\, A_i\cap A_j=\delta _{ij}\emptyset$

        (Actually just $ B\subset \bigcup_{i}A_i $ is enough, similar for Bayes's rule)
        \item Bayes's Rule\index{Bayes's Rule}
        \begin{equation}    
            \mathbb{P}(A_i|B)=\dfrac{\mathbb{P}(A_i)\mathbb{P}(B|A_i)}{\sum_{j=1}^n\mathbb{P}(A_j)\mathbb{P}(B|A_j)}    ,\quad 1\leq i\leq n
        \end{equation}
        where $\{A_i\}$ is a partition of $\Omega$: $ \Omega =\bigcup_{i}A_i,\, A_i\cap A_j=\delta _{ij}\emptyset $
    \end{itemize}

\subsection{Independency}
    Statistical Independency is defined as:
    \begin{equation}    
        A\independent B:\,\mathbb{P}(A\cap B) =\mathbb{P}(A)\mathbb{P}(B)
    \end{equation}

    Properties
    \begin{itemize}[topsep=2pt,itemsep=0pt]
        \item Complement set and indepency
        \begin{align}
            A\independent B\Leftrightarrow A^\complement \independent B 
        \end{align}
        \item Independency of multiple events
        \begin{align}
            A_1\independent A_2\independent\ldots \independent A_n \Leftrightarrow &\mathbb{P}\left( A_{j_1}\cap A_{j_2}\cap\ldots\cap A_{j_k} \right)=\mathbb{P}\left( A_{j_1} \right) \mathbb{P}\left( A_{j_2} \right) \ldots \mathbb{P}\left( A_{j_k} \right) \\
            &  \,\forall 1\leq j_1\leq j_2\leq \ldots \leq j_k\leq n\quad \forall k\leq n,\quad n<\infty
        \end{align}
    \end{itemize}
    
        

\section{Random Variable and Distribution}\label{SectionPropertiesOfRandomVariableAndVector}
\index{r.v. (Random Variable or Random Vector)}
Motivation: defining events is troublesome, and unhelpful to extract the key feature of events. A wise approach is to map samples \& events to numbers $ \Omega \mapsto \mathbb{R}^r $.

\subsection{Random Variable}
    Def. \text{Random Variable}: a \textbf{function}/mapping $X$ defined on sample space $\Omega$,  from $\Omega$ to some $\mathscr{X}\in\mathbb{R} $.
    \begin{align}
        X(\omega ):\, \Omega \mapsto \mathscr{X}\in\mathbb{R} 
    \end{align}

    \textbf{Note}: The mapping itself is non-random, the heart of randomness is still sample $ \omega  $ experimented. 

    Naturally $ X $ induces a mapping of probability measure
    \begin{align}
        F_X: \mathscr{X} \mapsto \Omega \mapsto \mathbb{P} 
    \end{align}

    To describe the mapping of probability, def. Cumulative Distribution Function (CDF)\index{CDF (Cumulative Distribution Function)}. (Here $ X(\omega ) $ is still used to remind the origin of randomness, in most case we simply use $ X $. )
    \begin{equation}
        F_X(x)=\mathbb{P} (X(\omega )\leq x)
    \end{equation}



    \begin{itemize}
        \item
        \begin{center}
            \parbox[t]{8.65cm}{PMF:\index{PMF (Probability Mass Function)}\begin{equation}        p_X(x)=F_X(x^+)-F_X(x^-)\end{equation}}
            \parbox[t]{8.65cm}{PDF:\index{PDF (Probability Density Function)}
            \begin{equation}        
                f_X(x)=\frac{\mathrm{d}F_X(x)}{\mathrm{d}x}
            \end{equation}}
        \end{center}
        \item Right-Continuity of CDF: A physical perspective is that PMF could be written as\footnote{Definition of Dirac $ \delta  $ function see \autoref{SubSubSectionFourierAndConvolution}.} 
        \begin{align}
            p_X(x)=\sum_{\tilde{x}\in\mathcal{X}}\mathbb{P}\left( X=\tilde{x} \right) \delta (x-\tilde{x})
        \end{align}
        where discrete $ X $ take values in $ \mathcal{X} $. In this way for any infinitesimal interval containing $ x $: $ \mathbb{I}_{x}\ni x$, we have 
        \begin{align}
            F_X(x^+)-F_X(x^-)=\int_{\mathbb{I}_{x}} p_X(x)\,\mathrm{d}x= \int_{\mathbb{I}_{x}}\sum_{\tilde{x}\in\mathcal{X}}\mathbb{P}\left( X=\tilde{x} \right) \delta (x-\tilde{x})\,\mathrm{d}x=\begin{cases}
                F_X(x^+)-F_X(x^-),&x\in \mathcal{X}\\
                0,&\text{others}
            \end{cases}
        \end{align}

        With such notation, in this note I sometimes ignore the difference between discrete cases / continuous cases.
        \item Representation of events: We could use random variable to express, say event $ A $ defined as        
        \begin{align}
            A:=\{\omega :X(\omega )\leq x\} 
        \end{align}
        
        
        \item Indicator function:\index{Indicator Function}
        \begin{equation}    
            \mathbb{I}_{x\in A}(x)=\begin{cases}
                1& x\in  A\\
                0& x\notin A
            \end{cases}
        \end{equation}
        \item Convolution\index{Convolution}
        \begin{itemize}
            \item $W=X+Y$
            \begin{equation}        
                f_W(w)=\int_{-\infty}^\infty f_X(x)f_Y(w-x)\mathrm{d}x    
            \end{equation}
            \item $V=X-Y$
            \begin{equation}        
                f_V(v)=\int_{-\infty}^\infty f_X(x)f_Y(x-v)\mathrm{d}x    
            \end{equation}
            \item $Z=XY$
            \begin{equation}        
                f_Z(z)=\int_{-\infty}^\infty \frac{1}{|x|}f_X(x)f_Y(\frac{z}{x})\mathrm{d}x
            \end{equation}
        \end{itemize}

            Examples:        
        \begin{itemize}[topsep=2pt,itemsep=0pt]
            \item Poisson\footnote{More about Poisson Distribution / Poisson Process see \autoref{SubSubSectionIndepedentProcess}}
            \begin{align}
                P(\lambda _1)+P(\lambda _2)\sim P(\lambda _1+\lambda _2) 
            \end{align}
            \item Binomial
            \begin{align}
                B(n_1,p)+B(n_2,p)\sim B(n_1+n_2,p) 
            \end{align}
            \item Gamma / Exponential
            \begin{align}
                \Gamma (\alpha _1,\lambda )+\Gamma (\alpha _2,\lambda )\sim \Gamma (\alpha _1+\alpha _2,\lambda ) 
            \end{align}
            with 
            \begin{align}
                \varepsilon (\lambda )=\Gamma (1,\lambda ) 
            \end{align}
            
            
            
            
            \item More relations of distributions see \url{http://www.math.wm.edu/~leemis/chart/UDR/UDR.html}
            \item Relation between Poisson Process and Exponential and Uniform distribution see \autoref{SubSubSectionPoissonProcess}.
            
            
        \end{itemize}
        
        \item Order Statistics\index{Order Statistics}\footnote{A relative object is Rank statistics, see \autoref{SubSectionIntroToNonParametricHypothesisTesting}.}
        
        Def $X_{(1)},X_{(2)},\cdots,X_{(n)}$ as order statistics of $\vec{X}$
        \begin{equation}    
            g_{X_{(i)}}=n!\prod_i f(x_i)\qquad \mathrm{for}\, x_1<x_2\cdots <x_n    
        \end{equation}
        PDF of $X_{(k)}$
        \begin{equation}\label{EqaDistributionOfOrderStatistics} 
            g_k(x_k)=nC_{n-1}^{k-1}[F(x_k)]^{k-1}[1-F(x_k)]^{n-k}f(x_k)
        \end{equation}
        \item $p$-fractile\index{Fractile!$ p $-fractile}
        \begin{equation}    \xi_p=F^{-1}(p)=\inf\{x|F(x)\geq p\}\end{equation}
    \end{itemize}






\subsection{Random Vector}
    A general case of random variable. Its definition is similar    
    \begin{align}
        \vec{X}(\omega ):\, \Omega \mapsto \mathscr{X}\in\mathbb{R}^n 
    \end{align}
    a $n$-dimension Random Vector $\vec{X}=(X_1,X_2,\ldots,X_n)$ defined on $(\Omega,\mathscr{F},\mathbb{P})$.

    CDF $F(x_1,\ldots,x_n)$ defined on $\mathbb{R}^n$:
    \begin{equation}F(x_1,\ldots,x_n)=\mathbb{P}(X_1\leq x_1,\ldots,X_n\leq x_n)\end{equation}

    Joint PDF of random vector: 
    \begin{equation}
        f(x_1,\ldots,x_n)=\dfrac{\partial^n F(x_1,\ldots,x_n)}{\partial x_1\ldots\partial x_n}
    \end{equation}

    $k$-dimensional Marginal Distribution: For $1\leq k<n$ and index set $S_k=\{i_1.\ldots,i_k\}$, distribution of $\vec{X}=(X_{i_1},X_{i_2},\ldots,X_{i_k})$
    \begin{equation}F_{S_k}(X_{i_1}\leq x_{i_1},X_{i_2}\leq x_{i_2}\ldots,X_{i_k}\leq x_{i_k})=\mathbb{P}(X_{i_1}\leq x_{i_1},\ldots,X_{i_k}\leq x_{i_k};X_{i_{k+1}},\ldots,X_{i_n}\leq\infty)\end{equation}

    Marginal distribution: 
    \begin{equation}
        g_{S_k}(x_{i_1},\ldots,x_{i_k})=\int_{\mathbb{R}^{n-k}}f(x_1,\ldots,x_n)\mathrm{d}x_{i_{k+1}}\ldots\mathrm{d}x_{j_n}=\dfrac{\partial^{n-k}F(x_1,\ldots,x_n)}{\partial x_{i_{k+1}}\ldots\partial x_{i_n}}
    \end{equation}


    \begin{itemize}
        \item[$\Delta$] \textbf{Function of r.v.}
        
        For $\vec{X}=(X_1,X_2,\cdots,X_n)$ with PDF $f(\vec{X})$ and define 
        \begin{equation}    
            \vec{Y}=(Y_1,Y_2,\cdots,Y_n)=\left(y_1(\vec{X}),y_2(\vec{X}),\cdots,y_n(\vec{X})\right)
        \end{equation}
        with inverse mapping
        \begin{equation}    
            \vec{X}=(X_1,X_2,\cdots,X_n)=\left(x_1(\vec{Y}),x_2(\vec{Y}),\cdots,x_n(\vec{Y})\right)
        \end{equation}
        then
        \begin{equation}    
            g(\vec{Y})= f\left(x_1(\vec{Y}),x_2(\vec{Y}),\cdots,x_n(\vec{Y})\right)\left|\frac{\partial \vec{X}}{\partial\vec{Y}}\right|\mathbb{I}_{D_Y}
        \end{equation}

        (Intuitively: $g(\vec{Y} )\mathrm{d}\vec{Y}=\mathrm{d}\mathbb{P}=f(\vec{X})\mathrm{d}\vec{X}$)
    \end{itemize}



    




\section{Expectation $\mathbb{E}$, Variance $var$ and Covariance $cov$}
Motivation: what would happen `on average'?

Expectation and Variance of common distributions see \autoref{SectionImportantDistributions}.

\subsection{Expection $ \mathbb{E}(\,\cdot\,) $}
    Expectation of r.v. $g(X)$ def.:
    \begin{equation}
    \mathbb{E} [g(X)]=\begin{cases}
        {\displaystyle\int_\Omega g(x) f_X(x)\mathrm{d}x=\int_\Omega g(x)\mathrm{d}F(x)}\\
        {\displaystyle\sum_{\Omega}g(x)f_X(x)}
    \end{cases}
\end{equation}

    Sometimes when there are more than 1 variables, say $ x,y $, we would use notation $ \mathbb{E}_X\left( g(X,Y) \right)  $ to specify the variable to avoid confusion.

    \textbf{Note}: For discrete r.v. the expectation always exists, but for continuous \& unbounded r.v. the expectation might diverge, rigorously speaking:
    \begin{align}
        \mathbb{E}\left[ X \right]\exists:\, \int_{\mathbb{R}}|x|f(x)\,\mathrm{d}x<\infty  
    \end{align}
    
    
 
\begin{point}
    Properties of Expectation $E(\,\cdot\,)$:
\end{point}

\begin{itemize}
    \item Linearity of Expectation\begin{equation}
        \mathbb{E}(aX+bY)=a \mathbb{E}(X)+b\mathbb{E}(Y)
    \end{equation}
    \item Conditional Expectation\begin{equation}
        \mathbb{E}(X|A)=\frac{\mathbb{E}(X\mathbb{I}_A)}{\mathbb{P}(A)}
    \end{equation}
    
    Note: if take $A$ as $Y$ is also a r.v. then conditional expectation is actually a function of $Y$
    \begin{equation}\xi (Y)=\mathbb{E}(X|Y)=\int xf_{X|Y}(x)\mathrm{d}x\end{equation}

    

    \item Law of Total Expectation\begin{equation}
        \mathbb{E}_Y\big\{\mathbb{E}_X[g(X)|Y]\big\}=\mathbb{E}_X[g(X)]
    \end{equation}
    \item r.v.\& Event
    \begin{equation}
        \mathbb{P}(A|X)=\mathbb{E}(\mathbb{I}_A|X)\Rightarrow \mathbb{E}[P(A|X)]=\mathbb{E}(\mathbb{I}_A)=\mathbb{P}(A)
    \end{equation}
    \item Conditional Expectation
    \begin{equation}
        \mathbb{E}\big[h(Y)g(X)|Y\big]=h(Y)\mathbb{E}[g(X)|Y]
    \end{equation}
\end{itemize}


\subsection{Variance $ var(\, \cdot \, ) $}
    Variance of r.v. $X$: 
    \begin{equation}
        var(X)=\mathbb{E}\big[(X-\mathbb{E}(X))^2\big]=\mathbb{E}(X^2)-(\mathbb{E}(X))^2
    \end{equation}
    (sometimes denoted as $\sigma^2_X$.)

    Another definition comes from the MMSE estimation, 
    \begin{align}
        var(X)=\mathop{\min}\limits_{c}\mathbb{E}\left[ (X-c)^2 \right]   
    \end{align}
    
    its solution is $ c=\mathbb{E}\left[ X \right]  $. See \autoref{SubSecMMSE} for more.

\begin{point}
    Properties:
\end{point}

\begin{itemize} 
    \item Linear combination of Variance\begin{equation}
        var(aX+b)=a^2var(X)
    \end{equation}
    \item Conditional Variance
    \begin{equation}
        var(X|Y)=\mathbb{E}{[X-\mathbb{E}(X|Y)]^2|Y}
    \end{equation}
    \item Law of Total Variance\begin{equation}
        var(X)=\mathbb{E}[var(X|Y)]+var[\mathbb{E}(X|Y)]
    \end{equation}
\end{itemize}

    Standard Deviation def. as :
    \begin{equation}\sigma_X=\sqrt{var(X)}\end{equation}

    Then can construct \textbf{Standardization}\index{Standardization} of r.v.
    \begin{equation}X_\mathrm{sd} =\frac{X-\mathbb{E}(X)}{\sqrt{var(X)}}\end{equation}


\subsection{Covariance $ cov(\, \cdot \, ) $ and Correlation $ corr(\, \cdot \, ) $}\label{SubSubSectionCovarianceAndCorrelation}
    Covariance of r.v. $X$ and $Y$:\begin{equation}
        cov(X,Y)=\mathbb{E}\big[(X-\mu_X)(Y-\mu_Y)\big]=\mathbb{E}(XY)-\mathbb{E}(X)\mathbb{E}(Y)
    \end{equation}

    And Correlation Coefficient\index{Correlation Coefficient}
    \begin{equation}
        \rho_{X,Y}=corr(X,Y)=\frac{cov(X,Y)}{\sqrt{var(X)var(Y)}}
    \end{equation}

    Remark: correlation $\nRightarrow$ cause and effect. 
    Detail on causal effect topic see \autoref{SecCausalInference}.

    Properties:
\begin{itemize}
\item Bilinear of Covariance\begin{align}
    cov(X+Y,Z)&=cov(X,Z)+cov(Y,Z)\\
    cov(X,Y+Z)&=cov(X,Y)+cov(X,Z)
\end{align}
    
\item Variance and Covariance
\begin{equation}\label{EqaVarOfSumOfRV}
    var(X+Y)=var(X)+var(Y)+2cov(X,Y)
\end{equation}
\item Covariance Matrix\index{Covariance Matrix}

    Def $\Sigma=\mathbb{E}\big[(X-\mu)(X-\mu)^T\big]=\{\sigma_{ij}\}$ (where $X$ should be considered as a column vector)
\begin{equation}\label{covariancematrix}
    \Sigma=
        \begin{pmatrix}
        var(X_1) & cov(X_1,X_2) & \ldots & cov(X_1,X_n)\\
        cov(X_2,X_1) & var(X_2) & \ldots & cov(X_2,X_n)\\
        \vdots & \vdots & \ddots & \vdots\\
        cov(X_n,X_1) & cov(X_n,X_2) & \ldots & var(X_n)\\
        \end{pmatrix}    
    \end{equation}
\end{itemize}

Attachment: Independence:\begin{equation}    X_i \independent X_j\Rightarrow \begin{cases}
        f(x_1,x_2,\cdots,x_n)=\prod f(x_i)\\
        F(x_1,x_2,\cdots,x_n)=\prod F(x_i)\\
        E(\prod X_i)=\prod E(X_i)\\
        var(\sum X_i)=\sum var(X_i)
    \end{cases},\qquad n<\infty
\end{equation}


\section{PGF, MGF and C.F}\label{SectionPGFMGFCF}

    Generating Function: Representation of $\mathbb{P}$ in function space. $\mathbb{P}\Leftrightarrow$ Generating Function.

\subsection{Probability Generating Function}
    PGF\index{PGF (Probability Generating Function)}: used for non-negative, integer $X$, which is the $ z $-transform of $ p_X $
    \begin{equation}
        g(s)=\mathbb{E}(s^X)=\sum_{j=0}^\infty s^j\mathbb{P}(X=j)    ,s\in[-1,1]
    \end{equation}

\begin{point}
        Properties
\end{point}
    \begin{itemize}[topsep=2pt,itemsep=0pt]
        \item $\mathbb{P}(X=k)=\dfrac{g^{(k)}(0)}{k!}$
        \item $\mathbb{E}(X)=g^{(1)}(1)$
        \item $var(X)=g^{(2)}(1)+g^{(1)}(1)-[g^{(1)}(1)]^2 $
        \item For $X_1,X_2,\cdots,X_n$ independent with $g_i(s)=\mathbb{E}(s^{X_i})$, $Y={\displaystyle \sum_{i=1}^n} X_i$, then
        \begin{equation}    
            g_Y(s)=\prod_{i=1}^n g_i(s),s\in[-1,1]
        \end{equation}
        \item For ${X_i}$ i.i.d with $\psi_i(s)=\psi(s)\equiv \mathbb{E}(s^{X_i})$, $Y$ with $G(s)\equiv\mathbb{E}(s^{Y})$, $W=X_1+X_2+\cdots +X_Y$,then
        \begin{equation}    
            g_W(s)=G[\psi(s)]    
        \end{equation}
        \item 2-Dimensional PGF of $(X,Y)$
        \begin{equation}    
            g(s,t)=\mathbb{E}(s^Xt^Y)=\sum_{i=o}^\infty\sum_{j=0}^\infty \mathbb{P}_{(X,Y)}(X=i,Y=j)s^it^j,\quad s,t\in[-1,1]
        \end{equation}
    \end{itemize}
\subsection{Moment Generating Function}
    MGF\index{MGF (Moment Generating Function)}: used for non-negative $ X $, which is the Laplacian transformation of $ f_X $.
    \begin{equation}
        M_X(s)=\mathbb{E}(e^{sX})=\begin{cases}
            \sum_je^{sx}\mathbb{P}(X=x_j)\\
            \int_{-\infty}^\infty e^{sx}f_X(x)\mathrm{d}x
        \end{cases}
    \end{equation}

    Properties
    \begin{itemize}
        \item MGF of $Y=aX+b$: $
            M_Y(s)=e^{sb}M(sa)    $
        \item $\mathbb{E}(X^k)=M^{(k)}(0)$
        \item $\mathbb{P}(X=0)={\displaystyle\lim_{s\to -\infty}}M(s)$
        \item For $X_1,X_2,\cdots,X_n$ independent with $M_{X_i}(s)=\mathbb{E}(e^{sX_i})$, $Y={\displaystyle \sum_{i=1}^n} X_i$, then
        \begin{equation}    
            M_Y(s)=\prod_{i=1}^n M_{X_i}(s)
        \end{equation}
    \end{itemize}
\subsection{Characteristic Function}
    C.F \index{C.F. (Characteristic Function)}is actually the Fourier Transform of $f_X$.
    \begin{equation}
        \phi(t)=\mathbb{E}(e^{itX}) = \int_{-\infty}^\infty e^{itx}f_X(x)\mathrm{d}x
    \end{equation}

    Properties
    \begin{itemize}
    \item if $E(|X|^k)<\infty$,then
    \begin{equation}
        \phi^{(k)}(t)=i^k\mathbb{E}(X^ke^{itX})\qquad \phi^{(k)}(0)=i^k\mathbb{E}(X^k)    
    \end{equation}
    \item For $X_1,X_2,\cdots,X_n$ independent with $\phi_{X_i}(t)=\mathbb{E}(e^{itX_i})$, $Y={\displaystyle \sum_{i=1}^n} X_i$, then
    \begin{equation}
        \phi_Y(t)=\prod_{i=1}^n \phi_{X_i}(t)
    \end{equation}
    \item Inverse (Fourier) Transform
    \begin{equation}
        f(x)=\frac{1}{2\pi}\int_{-\infty}^\infty e^{-itx}\phi(t)\mathrm{d}t    
    \end{equation}
\end{itemize}



\section{Convergence and Limit Distribution}
\subsection{Convergence Mode}
    \index{Convergence}
    \begin{equation}
        \begin{cases}
            \text{Convergence in Distribution }&{\displaystyle X_n\xrightarrow[]{\mathrm{d}}X:\lim_{n\to\infty}F_n(x)=F(x)}\\
            \text{Convergence in Probability }&{\displaystyle X_n\xrightarrow[]{\mathrm{p}}X:\lim_{n\to\infty}\mathbb{P}(|X_n-X)\geq\varepsilon)=0\, ,\forall\varepsilon>0}\\
            \text{Almost Sure Convergence }&{\displaystyle X_n\xrightarrow[]{\text{a.s.}}X:\mathbb{P}(\lim_{n\to\infty}X_n=X)=1}\\
            L_p\text{ Convergence }&{\displaystyle X_n\xrightarrow[]{L_p}X:\lim_{n\to\infty}\mathbb{E}(|X_n-X|^p)=0}
        \end{cases}
    \end{equation}

        Relations between convergence:
        \begin{center}
            \begin{tikzpicture}
                \draw(-1,-1)rectangle(1,1);
                \draw(-3,-0.5)rectangle(-5,-2.5);
                \draw(-3,0.5)rectangle(-5,2.5);
                \draw(3,-1)rectangle(5,1);
                \draw[-latex](-3,-1.5)--(-1,-0.5);
                \draw[-latex](-1,-0.75)--(-3,-1.75);
                \draw[-latex](-3,1.5)--(-1,0.5);
                \draw[-latex](1,0)--(3,0);
                \node at (0,0){$X_n\xrightarrow[]{\mathrm{p}}X$};
                \node at (-4,-1.5){$X_n\xrightarrow[]{L_p}X$};
                \node at (-4,1.5){$X_n\xrightarrow[]{\text{a.s.}}X$};
                \node at (4,0){$X_n\xrightarrow[]{\mathrm{d}}X$};
                \node at (-1.5,-1.5){$L_p<\infty $};
            \end{tikzpicture}
        \end{center}

        Note: $ L_2 $ convergence is also denoted m.s. (mean squared) convergence $ \xrightarrow[]{\mathrm{m.s.}}  $.

        Useful Thm.:
        \begin{itemize}
            \item Continuous Mapping Thm.: For continuous function $g(\cdot)$\index{Continuous Mapping Thm.}\hypertarget{ContinuousMapping}{}
            \begin{enumerate}
                \item $X_n\xrightarrow[]{\text{a.s.}}X\Rightarrow g(X_n)\xrightarrow[]{\text{a.s.}}g(X)$
                \item $X_n\xrightarrow[]{\mathrm{p}}X\Rightarrow g(X_n)\xrightarrow[]{\mathrm{p} }g(X)$
                \item $X_n\xrightarrow[]{\mathrm{d}}X\Rightarrow g(X_n)\xrightarrow[]{\mathrm{d}}g(X)$
            \end{enumerate}
            \item Slutsky's Thm.\index{Slutsky's Thm.}: For $X_n\xrightarrow[]{\mathrm{d}}X,Y_n\xrightarrow[]{\mathrm{p}}c$
            \begin{enumerate}
                \item $X_n+Y_n\xrightarrow[]{\mathrm{d}}X+c$
                \item $X_nY_n\xrightarrow[]{\mathrm{d}}cX$
                \item $X_n/Y_n\xrightarrow[]{\mathrm{d}}X/c$
            \end{enumerate}
            \item Continuity Thm. for characteristic function:
            \begin{equation}        \lim_{n\to\infty}\phi_n(t)=\varphi(t)\Leftrightarrow X_n\xrightarrow[]{\mathrm{d}}X\end{equation}
        \end{itemize} 


\subsection{Law of Large Number \& Central Limit Theorem}

\begin{itemize}
\item m.s. LLN\index{m.s. LLN (Mean-Squared Law of Large Number)}: For $ X_i $ with $ cov(X_i,X_j)=0$, if $ i\neq j $, and $ \mathbb{E}\left[ X_i \right] =\mu <\infty $
\begin{align}
    \frac{1}{n}\sum X_i\xrightarrow[]{L_2} \mathbb{E}\left[ X_1 \right]
\end{align}


\item WLLN\index{LLN (Law of Large Number)}\index{WLLN (Weak Law of Large Number)}: For $ X_i $ i.i.d. $ \sim f_X $, with $ \mathbb{E}\left[ X_i \right]=\mu <\infty $
\begin{equation}\label{EqaWLLN}    \frac{1}{n}\sum X_i\xrightarrow[]{\mathrm{p}}\mu 
\end{equation}
\item SLLN\index{SLLN (Strong Law of Large Number)}: For $ X_i $ i.i.d. $ \sim f_X $, with $ \mathbb{E}\left[ X_i \right] =\mu <\infty $
\begin{equation}    \frac{1}{n}\sum X_i\xrightarrow[]{\text{a.s.}}  \mu 
\end{equation}
\item CLT\index{CLT (Central Limit Thm.)}: For $ X_i $ i.i.d. $ \sim f_X $, with $ \mathbb{E}\left[ X_i \right] =\mu <\infty $, $ var(X_i)=\sigma ^2<\infty $
\begin{align}
     \dfrac{\sqrt{n}\left(\bar{X}-\mu \right)}{\sigma }\xrightarrow[]{\mathrm{d}} N(0,1)
\end{align}
or in equivalent form
\begin{align}    
    &\frac{1}{\sigma\sqrt{n}}\sum(X_k-\mu)\xrightarrow[]{\mathrm{d}} N(0,1)\\
    &\bar{X}\xrightarrow[]{\mathrm{d}} N(\mu ,\dfrac{\sigma ^2}{n})
\end{align}

\begin{proof}
    Denote the characteristic function of $ X\sim f_X(x) $ as $ \phi _X(t):=\mathbb{E}\left[ e^{itX} \right] $, with expectation $ \mu :=\mathbb{E}\left[ X \right]  $ and variance $ \sigma^2:=var(X)=\mathbb{E}\left[ X^2\right] -\mu  ^2 $.
    
    Define $ Z=\dfrac{X-\mu }{\sigma } $ The taylor series of $ \phi _Z(t) $ at $ t=0 $ yields:
    \begin{align*}
        \phi _Z(t)=1-\dfrac{t^2}{2}+o(t^2) 
    \end{align*}
    The characteristic function of mean $ \displaystyle\bar{Z}:=\dfrac{1}{n}\sum_{i=1}^nZ_i=\dfrac{1}{n}\sum_{i=1}^n\dfrac{X_i-\mu }{\sigma } $ w.r.t. $ X_i $ i.i.d. $ \sim f_X(x) $
    \begin{align*}
        \phi _{\bar{Z}}(t)=\mathbb{E}\left[ e^{it\bar{Z}} \right]=&\left[\phi _{Z}(\dfrac{t}{n})\right]^n=\left[1-\dfrac{t^2}{2n^2}\right] ^n 
    \end{align*}
    with $ n\to\infty $ limit:\footnote{Note: if use characteristic function of $ X_i $ directly, notice that
    \begin{align*}
        n\log\left(1+ \dfrac{at}{n}-\dfrac{bt^2}{2n^2}\right)= at -\left(b+a^2\right)\dfrac{t^2}{2n}+\mathcal{O}(\dfrac{1}{n^2})
    \end{align*}
    using the taylor series of $ \log(1+\xi ) $ at $ \xi =0 $.
    }
    \begin{align*}
        \lim_{n\to\infty}\phi _{\bar{Z}}(t)=&\lim_{n\to\infty} \left[1-\dfrac{1}{n}\dfrac{t^2}{2n}\right] ^n=e^{-\frac{t^2}{2n}} \Rightarrow \bar{Z}=\dfrac{\bar{X}-\mu }{\sigma }\xrightarrow[]{\mathrm{d}} N(0,\dfrac{1}{n})
    \end{align*}

\end{proof}

\item de Moivre-Laplace Thm.\index{de Moivre-Laplace Thm.} is a special case of CLT at $ S_n\sim B (n,p) $
\begin{equation}    \mathbb{P}(k\leq S_n\leq m)\approx \Phi(\frac{m+0.5-np}{\sqrt{npq}})-\Phi(\frac{k-0.5-np}{\sqrt{npq}})
\end{equation}
\item Stirling Eqa. derived from CLT
\begin{equation}    \frac{\lambda^k}{k!}e^{-\lambda}\approx \frac{1}{\sqrt{\lambda}\sqrt{2\pi}}e^{-\frac{(k-\lambda)^2}{2\lambda}}\xrightarrow[\lambda=n]{k=n}n!\approx\sqrt{2\pi n}(\frac{n}{e})^n\sim O\left((\dfrac{n}{e})^n\right)
\end{equation}

\end{itemize}


\section{Inequalities}\label{SubSectionUsefulInequality}
    
\begin{itemize}
    \item Cauchy-Schwarz Inequality\index{Inequality!Cauchy-Schwarz Inequality}
    \begin{equation}
        \left\vert \mathbb{E}(XY) \right\vert\leq \sqrt{\mathbb{E}(X^2)\mathbb{E}(Y^2)}
    \end{equation}

    \item Bonferroni Inequality\index{Inequality!Bonferroni Inequality}
\begin{equation}    \mathbb{P}(\bigcup_{i=1}^n A_i)\geq \sum_{1\leq i\leq n}  \mathbb{P}(A_i)+\sum_{1\leq i <j\leq n}  \mathbb{P}(A_i\cap A_j)
\end{equation}
    \item Markov Inequality\index{Inequality!Markov Inequality}
\begin{equation}     \mathbb{P}(|X|\geq \epsilon)\leq\frac{\mathbb{E}(|X|^\alpha)}{\epsilon^\alpha}
\end{equation}

    with $ \alpha =1 $, and $ \varepsilon  $ selected as a multiple of $ \mathbb{E}\left[ |X| \right]  $:
    \begin{align}
        \mathbb{P}\left( |X|\geq m\mathbb{E}\left[ |X| \right]  \right) \leq \dfrac{1}{m} 
    \end{align}
    
    
    \item Chebyshev Inequality\index{Inequality!Chebyshev Inequality}
\begin{equation}     \mathbb{P}(|X-\mathbb{E}(X)|\geq\epsilon)\leq\frac{var(X)}{\epsilon^2}
\end{equation}

    Chebyshev inequality is used to proof WLLN \autoref{EqaWLLN}
    \item Jensen Inequality\index{Jensen Inequality}: For convex function $h(x)$:\footnote{Or equivalently for concave function $ \tilde{h}(x) $:
    \begin{align}
         \mathbb{E}[\tilde{h}(X)]\leq \tilde{h}(\mathbb{E}(X))
    \end{align}
    }
\begin{equation}    \mathbb{E}[h(X)]\geq h(\mathbb{E}(X))
\end{equation}
    
    Example of using Jensen Eqa. to proof some other inequalities:
    \begin{itemize}[topsep=2pt,itemsep=0pt]
        \item Non-negativity of Kullback-Leibler Divergence: For two distributions $ f(\, \cdot \, ) $ and $ g(\, \cdot \, ) $, the K-L Divergence is defined as 
        \begin{align}
            \mathrm{KL}(f\Vert g):=-\int f(x)\log \dfrac{g(x)}{f(x)}  \,\mathrm{d}x
        \end{align}
        
        Take $ h(\xi ):=\log \xi  $ a concave function for $ \xi \in(0,\infty) $ and $ Z:=\dfrac{g(X)}{f(X)} $ with $ X\sim f(x) $, then
        \begin{align}
            \mathbb{E}\left( h(Z) \right) =&\int _A \left(\log z\right) f_Z(z) \,\mathrm{d}z=\int _A \left(\log \dfrac{g(x)}{f(x)}\right) f(x) \,\mathrm{d}x\\
            \leq &h(\mathbb{E}\left( Z \right) )=\log \int _A zf_Z(z) \,\mathrm{d}z=\log \int _A \dfrac{g(x)}{f(x)}f(x) \,\mathrm{d}x=0\\
            \Rightarrow& -\int _A \log f(x)\dfrac{g(x)}{f(x)} \,\mathrm{d}x\geq 0
        \end{align}
        % \item Non-negativity of \hyperlink{NormDefinition}{$ \ell_p $ Norm}:
        % \begin{align}
        %     \left\Vert x \right\Vert _p= \left(\sum_{i=1}^n |x_i|^p \right)^{1/p},\quad p\geq 1
        % \end{align}
        
        % Take $ h(\xi ):=|\xi |^p  $ a convex function for $ p\geq 1 $ and r.v. $ Z_n $ is defined a discrete one with distribution
        % \begin{align}
        %     Z_n=x_1,x_2\,\ldots, x_n ,\,\mathrm{w.p.} \dfrac{1}{n}
        % \end{align}
        
        % i.e. $ Z_n $ has equal probability to be assigned value in $ \{x_1,x_2\,\ldots, x_n\} $. Then
        % \begin{align}
        %     \mathbb{E}\left( h(Z) \right) =&\sum_{i=1}^n\dfrac{1}{n}|x_i|^p\\
        %     \geq&h(\mathbb{E}\left( Z \right) )=\left|\sum_{i=1}^n\dfrac{1}{n}x_i\right|^p\\
        %     \Rightarrow & \left|\sum_{i=1}^nx_i\right|^p\leq n^{p-1}\sum_{i=1}^n\dfrac{1}{n}|x_i|^p,\quad p\geq 1
        % \end{align}
    \end{itemize}
    \item Cantelli Inequality\index{Cantelli Inequality}
    \begin{align}
        \mathbb{P}\left( X-\mathbb{E}\left[ X \right] \geq \lambda  \right) \leq \dfrac{var(X)}{var(X)+\lambda ^2} 
    \end{align}
    with $ \lambda =\sqrt{var(X)}:=\sigma  $, we have
    \begin{align}
        \begin{cases}
            \mathbb{P}\left( X\geq \mathbb{E}\left[ X \right] + \sigma \right) \leq \dfrac{1}{2}\\
            \mathbb{P}\left( X\leq \mathbb{E}\left[ X \right] - \sigma \right) \leq \dfrac{1}{2}
        \end{cases}
    \end{align}
    i.e. difference between mean and median is upperly bounded by standard deviation
    \begin{align}
        |\mathbb{E}\left[ X \right] - \mathrm{med}(X ) |\leq \sigma  
    \end{align}
    \item Hoeffding Inequality\index{Hoeffding Inequality}: with independent r.v. sequence $ X_i\in [a_i,b_i] $, and $ S_n:=\sum_{i=1}^nX_i $
    \begin{align}
        \mathbb{P}\left( \left\vert S_n-\mathbb{E}\left[ S_n \right]  \right\vert \geq \varepsilon   \right)  \leq 2\exp\left[ -\dfrac{2\varepsilon ^2}{\sum_{i=1}^n (b_i-a_i)^2} \right]
    \end{align}

    Or in equivalent form $ \varepsilon =nt $
    \begin{align*}
        \mathbb{P}\left( \dfrac{1}{n}\left\vert \sum_{i=1}^n\left(X_i-\mathbb{E}\left[ X_i \right] \right) \right\vert \geq t   \right)  \leq 2\exp\left[ -\dfrac{2n^2t ^2}{\sum_{i=1}^n (b_i-a_i)^2} \right]
    \end{align*}
    
    For special case of $ [a_i,b_i]=[a,b] $, $ \forall i $, $ |[a,b]|:=L $,
    \begin{align*}
        \mathbb{P}\left( \dfrac{1}{n}\left\vert \sum_{i=1}^n\left(X_i-\mathbb{E}\left[ X_i \right] \right)  \right\vert \geq t   \right)  \leq 2\exp\left[ -\dfrac{2nt^2}{L^2} \right]
    \end{align*}
    
    

    The proof needs the Hoeffding Lemma: for $ \mathbb{E}\left[ Z \right]=0  $ and $ Z\in[a,b] $
    \begin{align}
        \mathbb{E}\left[ e^{tZ} \right]\leq \exp\left[ \dfrac{t^2(b-a)^2}{8} \right],\quad \forall t  
    \end{align}
    
    \item McDiarmid Inequality\index{McDiarmid Inequality}: with independent r.v. sequence $ X_i $, and a function $ f(\, \cdot \, ) $ with bounded difference $ c_i $:
    \begin{align*}
        \left| f\left(X_1,X_2,\ldots,X_{n+1}\right)- f\left(X_1,X_2,\ldots,X_{n}\right) \right| \leq c_i
    \end{align*}

    we have McDiarmid inequality
    \begin{align*}
        \mathbb{P}\left( \left| f(X_1,\ldots,X_n)-\mathbb{E}\left[ f(X_1,\ldots,X_n) \right]  \right| \geq nt\right) \leq 2\exp\left[ -\dfrac{2n^2t^2}{\sum_{i=1}^nc_i^2} \right] 
    \end{align*}
    
\end{itemize}


\section{Multivariate Normal Distribution}\label{SubsectionDerivationMultivariateNormal}
    General Case and more discussion see \autoref{SubSubSectionMultivariateNormalDistribution}.

    Distribution of Normal $ X\sim N(\mu ,\sigma ^2) $:\index{Distribution!Normal Distribution}
    \[
        f(x)=\dfrac{1}{\sqrt{2\pi}\sigma }e^{-\frac{(x-\mu )^2}{2\sigma ^2}} 
    \]
    
    

    For $X_1,X_2,\cdots,X_n$ independent and $X_k\sim N(\mu_k,\sigma^2_k),\, k=1,\cdots,n$, $T={\displaystyle\sum_{k=1}^n c_kX_k}, (c_k$ const), then
    \begin{equation}
        T\sim N(\sum_{k=1}^nc_k\mu_k,\sum_{k=1}^n c_k^2\sigma^2_k)    
    \end{equation}

    Deduction in some special cases:
    \begin{itemize}
        \item Given $\mu_1=\mu_2=\cdots=\mu_n=\mu,\, \sigma^2_1=\sigma^2_2=\cdots=\sigma^2_n=\sigma^2$, i.e. $X_k$ i.i.d., then
        \begin{equation}\label{EqaDistributionOfSumOfiidNormal}
            T\sim   N(\mu\sum_{k=1}^n c_k,\sigma^2\sum_{k=1}^n c_k^2) 
        \end{equation}
        \item Further take $c_1=c_2=\cdots=c_n=\dfrac{1}{n}$, i.e. $T={\displaystyle \sum_{k=1}^n X_k /n}=\bar{X}$, then
        \begin{equation}    
            T=\bar{X}\sim N(\mu,\frac{\sigma^2}{n})    
        \end{equation}
    \end{itemize}





\subsection{Linear Transform}
    First consider $\epsilon_1,\epsilon_2,\cdots,\epsilon_m$ i.i.d. $\sim N(0,1)$, $n\times 1$ const column vector $\vec{\mu}$, $n\times m$ const matrix $\bm{B}=\{b_{ij}\}$, def.$X_i={\displaystyle\sum_{j=1}^m b_{ij}\epsilon_j}$, i.e.
    \begin{equation}
        \vec{X}=
        \begin{pmatrix}
            X_1\\X_2\\ \vdots\\X_n
        \end{pmatrix}
        =
        \begin{pmatrix}
            b_{11}&b_{12}&\ldots&b_{1m}\\
            b_{21}&b_{22}&\ldots&b_{2m}\\
            \vdots&\vdots&\ddots&\vdots\\
            b_{n1}&b_{n2}&\ldots&b_{nm}
        \end{pmatrix}
        \begin{pmatrix}
            \epsilon_1\\
            \epsilon_2\\
            \vdots\\
            \epsilon_m
        \end{pmatrix}
        +\begin{pmatrix}
            \mu _1\\\mu _2\\ \vdots\\\mu _n
        \end{pmatrix}=\bm{B}\vec{\varepsilon }+\vec{\mu }
    \end{equation}

    
    We have: $\vec{X}\sim N(\vec{\mu},\Sigma)$, where $\Sigma$, as defined in \autoref{covariancematrix} is
    \begin{equation}
        \Sigma=\mathbb{E}[(\vec{X}-\vec{\mu})(\vec{X}-\vec{\mu})^T]=\bm{BB}^T=
        \begin{pmatrix}
        var(X_1) & cov(X_1,X_2) & \ldots & cov(X_1,X_n)\\
        cov(X_2,X_1) & var(X_2) & \ldots & cov(X_2,X_n)\\
        \vdots & \vdots & \ddots & \vdots\\
        cov(X_n,X_1) & cov(X_n,X_2) & \ldots & var(X_n)\\
        \end{pmatrix}  
        =\{\sigma_{ij}\}  
    \end{equation}

    Furthur Consider $\vec{Y}=(Y_1,\cdots,Y_n)^T$, $n\times n$ const square matrix $\bf{A}=\{a_{ij}\}$ and def. $\vec{Y}=\bf{A}\vec{X}$ i.e.
    \begin{equation}
        \begin{pmatrix}
            Y_1\\
            Y_2\\
            \vdots\\
            Y_n
        \end{pmatrix}
        =
        \begin{pmatrix}
            a_{11}&a_{12}&\ldots&a_{1n}\\
            a_{21}&a_{22}&\ldots&a_{2n}\\
            \vdots&\vdots&\ddots&\vdots\\
            a_{n1}&a_{n2}&\ldots&a_{nn}
        \end{pmatrix}
        \begin{pmatrix}
            X_1\\
            X_2\\
            \vdots\\
            X_n
        \end{pmatrix}
    \end{equation}

    Then $\vec{Y}\sim N(\bm{A}\vec{\mu},\bm{A}\Sigma\bm{A}^T)$
%    Then $Y_1,\cdots,Y_n\sim N$ 

    Special case: $X_1,\cdots,X_n$ i.i.d. $\sim N(\mu,\sigma^2)$, $\vec{X}=(X_1,\cdots,X_n)^T$, 
    \begin{align}
        \mathbb{E}(Y_i)=&\mu\sum_{k=1}^n a_{ik}\\
        var(Y_i)=&\sigma^2\sum_{k=1}^n a_{ik}^2\\
        cov(Y_i,Y_j)=&\sigma^2\sum_{k=1}^n a_{ik} a_{jk}
    \end{align}

    Specially when 
    $\bm{A}=\{a_{ij}\}$ orthonormal, we have $Y_1,\cdots,Y_n$ independent
    \begin{equation}
        Y_i\sim N(\mu\sum_{k=1}^n a_{ik},\sigma^2)    
    \end{equation}

    \begin{point}
        Definition of Jointly Gaussian/Normal\index{Jointly Gaussian Variable}
    \end{point}

    A random vector $ \vec{X} $ is called jointly Gaussian if and only if any (finite) linear combination of $ \vec{X} $ is still Gaussian (Normal)
    \begin{align}
        \sum_{k=1}^m\alpha _{k}X_{i_k}\sim N(\, \cdot \, , \, \cdot \, ),\,\forall \{\alpha _k\}_{k=1}^m,\,\forall \{i_k\}_{k=1}^m,\,\forall m\leq n
    \end{align}

    Counter Example: $ [X,Y] $ in which $ X\sim N(0,1) $, $ Y=-X $ is not jointly Gaussian.
    
    
    

    \subsection{Distributions of Function of Normal Variable: $\chi^2,$ $t\,\& \,F$}\label{chi2_t_F_properties}
        Consider $X_1,X_2,\ldots,X_n$ i.i.d. $\sim N(0,1)$; $Y,Y_1,Y_2,\ldots,Y_m$ i.i.d. $\sim N(0,1)$
        \begin{itemize}
            \item $\chi^2$ Distribution: Def. $\chi^2$ distribution with degree of freedom $n$:\index{Distribution!$ \chi^2 $ Distribution}
            \begin{equation}        
                \xi =\sum_{i=1}^n X_i^2\sim \chi^2_n
            \end{equation}

            PDF of $\chi^2_n$:
            \begin{equation}        
                g_n(x)=\dfrac{1}{2^{n/2}\Gamma(n/2)}x^{\frac{n}{2}-1}e^{-x/2}\mathbb{I}_{x>0}  
            \end{equation}

            Properties
            \begin{itemize}
                \item $\mathbb{E}$ and $var$ of $\xi\sim\chi^2_n$
                \begin{equation}            \mathbb{E}(\xi)=n\qquad var(\xi)=2n\end{equation}
                \item For independent $\xi_i\sim\chi^2_{n_i},\, i=1,2,\ldots,k$:\begin{equation}            
                    \xi_0=\sum_{i=1}^k\xi_i\sim\chi^2_{n_1+\ldots+n_k}\end{equation}
                \item Denoted as $\Gamma(\alpha,\lambda)$: \begin{equation}            \xi=\sum_{i=1}^nX_i^2\sim\Gamma(\frac{n}{2},\frac{1}{2})=\chi^2_n\end{equation}
            \end{itemize}
            \item $t$ Distribution: Def. $t$ distribution with degree of freedom $n$:\index{Distribution!$ t $ Distribution}
            \begin{equation}        
                T=\frac{Y}{\sqrt{\dfrac{\sum_{i=1}^nX_i^2}{n}}}=\frac{Y}{\sqrt{\xi \big/ n}}\sim t_n
            \end{equation}

            (Usually take $\nu$ instead of $n$ as degree of freedom for $ t $ distribution)

            PDF of $t_\nu$:
            \begin{equation}        
                t_\nu(x)=\dfrac{\Gamma(\frac{\nu+1}{2})}{\Gamma(\frac{\nu}{2})\sqrt{\nu\pi}}\left(1+\frac{x^2}{\nu}\right)^{-\frac{\nu+1}{2}}
            \end{equation}

            Denote: Upper $\alpha$-fractile\index{Fractile!Upper $ \alpha $-fractile} of $t_\nu$, satisfies $\mathbb{P}(T\geq c)=\alpha$:
            \begin{equation}        
                t_{\nu,\alpha}=\mathop{\arg}\limits_{c}\mathbb{P}(T\geq c)=\alpha,\quad T\sim t_\nu 
            \end{equation}
            
            (Similar for $ N $, $\chi^2_n$ and $F_{m,n}$ etc.)
            \item $F$ Distribution: Def. $F$ distribution with degree of freedom $m$ and $n$:\index{Distribution!$ F $ Distribution}
            \begin{equation}        
                F=\frac{\sum_{i=1}^mY_i^2\big/ m}{\sum_{i=1}^nX_i^2\big/ n}\sim F_{m,n}
            \end{equation}

            PDF of $F_{m,n}$:
            \begin{equation}        
                f_{m,n}(x)=\frac{\Gamma(\frac{m+n}{2})m^\frac{m}{2}n^{\frac{n}{2}}}{\Gamma(\frac{m}{2})\Gamma(\frac{n}{2})}x^{\frac{m}{2}-1}(mx+n)^{-\frac{m+n}{2}} \mathbb{I}_{x>0}
            \end{equation}

            Properties
            \begin{itemize}
                \item If $Z\sim F_{m,n}$, then $\dfrac{1}{Z}\sim F_{n,m}$.
                \item If $T\sim t_n$, then $T^2\sim F_{1,n}$
                \item $F_{m,n,1-\alpha}=\dfrac{1}{F_{n,m,\alpha}}$
            \end{itemize}
        \end{itemize}

        \begin{point}
            Some useful Lemma (uesd in statistic inference, see \autoref{SubSectionConfidenceIntervalForDistributions}):
        \end{point}
        
            
        \begin{itemize}
            \item For $X_1,X_2,\ldots,X_n$ independent with $X_i\sim N(\mu_i,\sigma^2_i)$, then
            \begin{equation}        
                \sum_{i=1}^n\left(\frac{X_i-\mu_i}{\sigma_i}\right)^2\sim \chi^2_n
            \end{equation}  
            \item For $X_1,X_2,\ldots,X_n$ i.i.d.$\sim N(\mu,\sigma^2)$, then
            \begin{equation}        
                T=\frac{\sqrt{n}(\bar{X}-\mu)}{S}\sim t_{n-1}   
            \end{equation}
            
            For $X_1,X_2,\ldots,X_m$ i.i.d.$\sim N(\mu_1,\sigma^2)$, $Y_1,Y_2,\ldots,Y_n$ i.i.d.$\sim N(\mu_2,\sigma^2)$, \\ denote sample pooled variance $S_{\omega}^2=\dfrac{(m-1)S^2_1+(n-1)S^2_2}{m+n-2}$, then
            \begin{equation}        
                T=\frac{(\bar{X}-\bar{Y})-(\mu_1-\mu_2)}{S_{\omega}}\cdot \sqrt{\frac{mn}{m+n}}\sim t_{m+n-2}
            \end{equation}
            \item For $X_1,X_2,\ldots,X_m$ i.i.d.$\sim N(\mu,\sigma^2)$, $Y_1,Y_2,\ldots,Y_n$ i.i.d.$\sim N(\mu_2,\sigma^2)$, then
            \begin{equation}        
                T=\frac{S_1^2}{S_2^2}\frac{\sigma^2_2}{\sigma^2_1}\sim F_{m-1,n-1}   
            \end{equation}
            \item For $X_1,X_2,\ldots,X_n$ i.i.d. $\sim \epsilon(\lambda)$, then
            \begin{equation}        
                2\lambda n\bar{X}=2\lambda\sum_{i=1}^nX_i \sim\chi^2_{2n} 
            \end{equation}

            Remark: for $X_i\sim\epsilon(\lambda)=\Gamma(1,\lambda)\Rightarrow 2\lambda\sum_{i=1}^nX_i\sim\Gamma(n,1/2)=\chi^2_{2n}$. 
        \end{itemize}

